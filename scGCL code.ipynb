{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5z3M58Ir1BD"
      },
      "source": [
        "Github Repository: https://github.com/juehoujhou4/CS598DLH-FinalProject\n",
        "\n",
        "Video Presentation: https://drive.google.com/file/d/1NWSuSIKHH33VoLWO0wG8AwtK-E-SEnj4/view?usp=drive_link"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgIJyogv6pcv"
      },
      "source": [
        "## **scGCL: an imputation method for scRNA-seq data based on Graph Contrastive Learning**\n",
        "\n",
        "### **Background**\n",
        "The sparse nature of single-cell RNA sequencing (scRNA-seq) data arises from the biological and technical challenges encountered when measuring gene expression at the individual cell level. This sparsity indicates that numerous genes go undetected in the examined cells within the dataset. Several factors contribute to this phenomenon, including the complexities of capturing and analyzing the minute amounts of RNA present in single cells, such as:\n",
        "\n",
        "1. Low RNA content- Low content makes it challenging to detect all mRNA molecules present; some cells naturally contain more mRNA molecules than others, which have more chance to detect the genes.\n",
        "2. Low gene expression or gene expression variance- Natural differences in gene expression levels between cells, even within the same cell type, can contribute to the observed sparsity. Some genes are only expressed in specific cell states or conditions, leading to a large number of zeros in the data matrix where a particular gene is not active in most cells.\n",
        "3. Technical variance- The process of isolating single cells, reverse transcribing RNA into cDNA, and amplifying the cDNA before sequencing introduces technical variability. Some mRNA molecules may be lost or degraded during these steps, resulting in incomplete detection of the transcriptome.\n",
        "4. Dropout events- The most important factor contributing to sparsity, dropout events occur when mRNA molecules present in the cell are not detected, leading to a zero count for genes that are actually expressed but missed during sequencing. This is often due to inefficiencies in reverse transcription, amplification, or sequencing steps. Addressing dropout events through imputation in scRNA-seq data can significantly streamline the analysis process by filling in gaps where gene expression values are missing or undetected.\n",
        "\n",
        "The importance of addressing this sparsity problem lies in several key areas:\n",
        "1. Enhanced Data Quality: Sparsity in scRNA-seq data means that many gene expression readings are missing or near zero, leading to a dataset filled with a lot of noise and few signals. By effectively imputing these missing values, the quality of the data can be significantly enhanced, leading to more reliable and interpretable results.\n",
        "2. Improved Biological Insights: The main goal of scRNA-seq analysis is to uncover the complex mechanisms of cellular processes and heterogeneity among cells in different states or environments. Addressing the sparsity issue allows for a more accurate identification of gene expression patterns, cell types, and developmental states, thereby facilitating deeper biological insights and discoveries.\n",
        "3. Enabling Advanced Analysis: Many downstream analyses, such as clustering, trajectory inference, and differential expression analysis, require robust datasets without extensive missing values. By solving the sparsity problem, these analyses can be performed more effectively, leading to more nuanced understanding of cellular behavior and interactions.\n",
        "4. Increased Comparability and Integration: As the field moves towards large-scale studies involving multiple datasets, the ability to accurately impute missing data becomes crucial for integrating and comparing datasets from different sources. This comparability is essential for drawing broader conclusions and for the reproducibility of findings across studies.\n",
        "\n",
        "The difficulty of addressing the sparsity in scRNA-seq data include:\n",
        "1. Lack of Ground Truth: In many cases, there is no \"ground truth\" for what the imputed values should be. This makes it difficult to train models and evaluate their performance objectively. The best that can be done is to rely on biological validation or downstream analysis outcomes, which can be time-consuming and not always definitive.\n",
        "2. Data complexity: The high dimensionality of the data, coupled with the sparsity of gene expression (many genes are not expressed in many cells), makes it difficult to accurately impute missing values without introducing bias or losing critical information.\n",
        "3. Noise: scRNA-seq data is also plagued by technical noise and dropout events, where genes are expressed but not detected due to limitations in sequencing depth or efficiency. Differentiating between true biological zeros and technical dropouts is a significant challenge.\n",
        "\n",
        "The ideal approach we desired:\n",
        "1. The approach not only diminishes noise in the data but also enhances the precision of clustering and classification efforts.\n",
        "2. It bolsters the accuracy of differential expression analyses, and aids in the integration of data from various sources, thereby simplifying the overall complexity of handling scRNA-seq datasets.\n",
        "\n",
        "### **Paper proposed**\n",
        "The paper proposes a novel method, scGCL (single-cell Graph Contrastive Learning), specifically designed for imputing missing values in single-cell RNA sequencing (scRNA-seq) data. This method innovatively combines graph contrastive learning with the Zero-inflated Negative Binomial (ZINB) distribution model to estimate dropout events accurately. By employing contrastive learning within a graph theory framework, scGCL is adept at capturing the complex relationships between cells, thereby enhancing the prediction and reconstruction of missing gene expression values.\n",
        "\n",
        "\n",
        "The innovation of the scGCL method lies in its unique approach to leveraging the strengths of graph contrastive learning, tailored specifically to the graph domain of scRNA-seq data. This allows for a sophisticated encapsulation of both global and local semantic information within the data. Furthermore, scGCL introduces a strategic selection of positive samples that significantly improve the representation of target nodes. This is complemented by the utilization of an autoencoder framework based on the ZINB distribution, specifically designed to model the global probability distribution of gene expression data effectively. This nuanced approach provides a robust solution to dropout imputation challenges, setting it apart from existing methods.\n",
        "\n",
        "\n",
        "The contribution of scGCL to the research regime is substantial, addressing critical challenges outlined in the background of sparse data, technical noise, and the need for advanced computational strategies in scRNA-seq data analysis. By providing a more accurate and efficient method for imputing missing values, scGCL enhances the quality of scRNA-seq data analysis, leading to deeper biological insights and facilitating advancements in personalized medicine and genomics. This approach not only solves a significant problem in the field but also pushes the boundaries of what is possible with self-supervised learning in genomics, highlighting the importance of this paper to the ongoing research and development within the domain.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1bd4LYsKIcrAg7bcmHICGLksy7oFEuXQQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOvDrGaofN5T"
      },
      "source": [
        "Hypotheses to be tested:\n",
        "\n",
        "**Hypothesis 1**: We hypothesize that scGCL will exhibit better clustering performance and imputation performance than other existing methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Z6I7smTuL_"
      },
      "source": [
        "## (1) Environment\n",
        "\n",
        "The below two cells download and import the necessary packages for the model. The following is a list of versions for each necessary package that we used to recreate the original code. This code has only been tested in the Google colab environment.\n",
        "\n",
        "\n",
        "*   Python: 3.10\n",
        "*   numpy:  1.25.2\n",
        "*   pandas:  2.0.3\n",
        "*   sklearn:  1.2.2\n",
        "*   h5py 3.9.0\n",
        "*   scipy:  1.11.4\n",
        "*   torch:  2.2.1+cu121\n",
        "*   faiss 1.8.0\n",
        "*   argparse:  1.1\n",
        "*   anndata: 0.10.7\n",
        "*   scanpy: 1.10.1\n",
        "*   torch_geometric: 2.5.2\n",
        "*   tensorboardX: 2.6.2.2\n",
        "*   matplotlib: 3.7.1\n",
        "\n",
        "We use the auto_time module to time each cell. This package is not necessary to run the model.\n",
        "\n",
        "Please add the following folder as a shortcut on your 'My Drive' to access the project files: https://drive.google.com/drive/folders/1jRK-MyK9mvC5eoPjtSZ8qIbYdjCT6Lwl?usp=sharing\n",
        "\n",
        "**This is necessary to run the notebook as it needs access to the data files and the pre-trained model.**\n",
        "\n",
        "Please run the below cells to download and import the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "outputs": [],
      "source": [
        "#import drive module from google drive package\n",
        "# from google.colab import drive\n",
        "\n",
        "#mount notebook to google drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdA8lWvyPCkS",
        "outputId": "c77a5894-3c87-4613-b48e-34b87109f849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.10.7-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.0)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.3)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.58.1)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.25.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.0)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.0.3)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.11.4)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.2)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8026 sha256=cb4ff597aab014c37c29af320b62f7d3cdab14739164c41ffc3355c241ea012a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: array-api-compat, stdlib_list, legacy-api-wrap, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.10.7 array-api-compat-1.6 legacy-api-wrap-1.4 pynndescent-0.5.12 scanpy-1.10.1 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n",
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1\n",
            "time: 346 µs (started: 2024-05-01 19:58:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#install necessary packages\n",
        "!pip install scanpy\n",
        "!pip install torch_geometric\n",
        "!pip install tensorboardX\n",
        "!pip install faiss-cpu\n",
        "\n",
        "#install and load python autotime module to time each cell\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yu61Jp1xrnKk",
        "outputId": "120c2649-e968-41cd-f349-22d9e3369d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.7 s (started: 2024-05-01 19:58:17 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# import packages you need\n",
        "import sys\n",
        "from tensorboardX import SummaryWriter\n",
        "import os\n",
        "import os.path as osp\n",
        "import copy\n",
        "import faiss\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "\n",
        "import csv\n",
        "import cmath as cm\n",
        "import h5py\n",
        "from matplotlib import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "from anndata import AnnData\n",
        "\n",
        "import sklearn\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import normalized_mutual_info_score, pairwise, adjusted_rand_score,silhouette_score\n",
        "\n",
        "import scanpy as sc\n",
        "import scipy\n",
        "import scipy.stats\n",
        "from scipy import sparse as sp\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch_geometric.data import InMemoryDataset, download_url, Data\n",
        "from torch_geometric.io import read_npz\n",
        "from torch_geometric.utils import remove_self_loops, to_undirected, dropout_adj\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils.loop import add_self_loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZtoSbHqWkfx"
      },
      "source": [
        "## (2) Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "source": [
        "\n",
        "*   Source of the data: The paper actually runs the experiment on 14 different datasets. However, we will run our project on a single dataset which the paper has called \"Adam\" because it comes from this paper ([Adam et al. (2017)](https://journals.biologists.com/dev/article/144/19/3625/48196/Psychrophilic-proteases-dramatically-reduce-single)). The dataset is a set of RNA sequences that is taken from kidney cells of mice. This dataset contains data for 3,660 cells, 23,797 genes and 8 cell types. The dataset is available to download [here](https://github.com/zehaoxiong123/scGCL/tree/main/test_csv/Adam) from the original github repository.\n",
        "\n",
        "*  A gene expression matrix is a matrix that contains cell samples and a gene. In our case, each cell is a row in the matrix and each gene is is a column. Each entry of the matrix is a numerical value that is >= 0 that represents the read counts of each gene in a particular cell. For example, the entry in row 2; col 6 represents the read count of gene 6 in cell sample 2.\n",
        "\n",
        "*   Data Process: The expression matrix of the scRNA-seq data is taken as the input raw data. To reduce noise in the scRNA-seq data, the paper pre-process the raw gene expression profiles using the following pre-processing methods:\n",
        "\n",
        "  1.   Data filtering and quality control are the first steps in scRNA-seq data pre-processing. Therefore, we only keep genes with non-zero expression in more than 1% of cells and cells with non-zero expression in more than 1% of genes.\n",
        "  2.   Since the data in the count matrix are discrete and affected by the size factor, we normalize it by the size factor then transform discrete values through the log function. Finally, we select the top $t$ highly variable genes based on the normalized discrete values computed by the scanpy package.\n",
        "  \n",
        "    Generally, we select 2048 highly variable genes for training and use a consistent pre-processing method before running all baseline methods.\n",
        "\n",
        "\n",
        "*   Illustration: ![picture](https://drive.google.com/uc?id=1ECCyhcVFhvKKIU3q-MM1EbTwvnjDlEJq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaPncTenyBEF"
      },
      "source": [
        "### (2.1) Dataset Information\n",
        "\n",
        "Run the cells below to download the dataset and print out basic statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My6VPlxcOAKm",
        "outputId": "69713a4b-5237-4970-9f01-236b923eddaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.51 ms (started: 2024-05-01 19:58:36 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Paths for the new directories\n",
        "results_path = '/content/results'\n",
        "images_path = '/content/images'\n",
        "\n",
        "# Create \"results\" directory if it doesn't exist\n",
        "if not os.path.exists(results_path):\n",
        "    os.makedirs(results_path)\n",
        "\n",
        "# Create \"images\" directory if it doesn't exist\n",
        "if not os.path.exists(images_path):\n",
        "    os.makedirs(images_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byTL7JAGGfw3",
        "outputId": "9e780dbb-0ee9-4342-ff1d-4eb098fc27ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16OVBx4kZnYfrWnkc6gLWI9FZ1hd1b_Sf\n",
            "To: /content/data.tgz\n",
            "100%|██████████| 18.5M/18.5M [00:00<00:00, 94.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.macl'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1iWA-wRjqR_cN8GeuFZAg9XyJ1NQglGpj\n",
            "From (redirected): https://drive.google.com/uc?id=1iWA-wRjqR_cN8GeuFZAg9XyJ1NQglGpj&confirm=t&uuid=22e93ca8-490b-4232-9298-93fdf25a0733\n",
            "To: /content/model_checkpoints.tgz\n",
            "100%|██████████| 58.7M/58.7M [00:00<00:00, 67.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.macl'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n",
            "time: 10.9 s (started: 2024-05-01 19:58:36 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# download data required for subsequential analysis\n",
        "import gdown\n",
        "\n",
        "# raw data downloading\n",
        "url_data = \"16OVBx4kZnYfrWnkc6gLWI9FZ1hd1b_Sf\"\n",
        "\n",
        "gdown.download(id=url_data, output=\"data.tgz\", quiet=False)\n",
        "!tar -xzf data.tgz\n",
        "\n",
        "# model checkpoint downloading\n",
        "url_model = \"1iWA-wRjqR_cN8GeuFZAg9XyJ1NQglGpj\"\n",
        "\n",
        "gdown.download(id=url_model, output=\"model_checkpoints.tgz\", quiet=False)\n",
        "!tar -xzf model_checkpoints.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZScZNbROw-N",
        "outputId": "103be6a6-7f70-4348-b24f-31583e5a4d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Organism that provided cell samples:  Mus musculus\n",
            "Lifestage of when cell sample was taken:  post natal day 1\n",
            "Organ that cell sample was taken from:  Kidney\n",
            "Num of unique cell types:  8\n",
            "List of unique cell types:  ['LH', 'Podocytes', 'Stromal', 'Ureteric bud', 'Distal tubule', 'CM', 'ED', 'PT']\n",
            "Num of cell samples (Rows):  3660\n",
            "Num of genes (Columns):  23797\n",
            "This is an example row from the expression matrix:\n",
            "[0. 1. 1. ... 0. 0. 0.]\n",
            "time: 6.09 s (started: 2024-05-01 19:58:47 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# dir and function to load raw data\n",
        "raw_data_dir = 'data_testing/test_csv/data.h5'\n",
        "\n",
        "# load raw file\n",
        "f = h5py.File(raw_data_dir, 'r')\n",
        "\n",
        "data = np.array(f[\"exprs/data\"])\n",
        "indices = np.array(f[\"exprs/indices\"])\n",
        "indptr = np.array(f[\"exprs/indptr\"])\n",
        "\n",
        "matrix = csr_matrix((data, indices, indptr), shape=list(f[\"exprs/shape\"]))\n",
        "\n",
        "# list out organism\n",
        "organism = str(list(f[\"obs/organism\"])[0])[2:-1] # string slicing just to get rid of extra characters\n",
        "print(\"Organism that provided cell samples: \", organism)\n",
        "\n",
        "# list out lifestage it was taken\n",
        "lifestage = str(list(f[\"obs/lifestage\"])[0])[2:-1] # string slicing just to get rid of extra characters\n",
        "print(\"Lifestage of when cell sample was taken: \", lifestage)\n",
        "\n",
        "# list out organ taken sample from\n",
        "organ = str(list(f[\"obs/organ\"])[0])[2:-1] # string slicing just to get rid of extra characters\n",
        "print(\"Organ that cell sample was taken from: \", organ)\n",
        "\n",
        "# num of unique cell types\n",
        "cell_types = list(set(list(f[\"obs/cell_type1\"])))\n",
        "cell_types = [str(cell)[2:-1] for cell in cell_types]\n",
        "\n",
        "print(\"Num of unique cell types: \", len(cell_types))\n",
        "# list out unique cell types\n",
        "print(\"List of unique cell types: \", cell_types)\n",
        "\n",
        "# num of cell samples\n",
        "print(\"Num of cell samples (Rows): \", matrix.shape[0])\n",
        "# num of genes\n",
        "print(\"Num of genes (Columns): \", matrix.shape[1])\n",
        "\n",
        "# print out sample row (A single cell sample)\n",
        "print(\"This is an example row from the expression matrix:\")\n",
        "print(matrix.toarray()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA_1dMTB4Y30"
      },
      "source": [
        "### (2.2) Data preprocess functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGzYFVPoMtb6",
        "outputId": "d501b5f5-98b9-46e3-d59c-cfdfc83707a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.08 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#Data preprocess functions:\n",
        "#This cell defines functions that are used to preprocess the data\n",
        "#This process can be seen in an illustration that is shown at the beginning of the data section\n",
        "\n",
        "\n",
        "'''\n",
        "  Preprocesses the raw data.\n",
        "  Takes in raw datafile to filter and normalize data. Calls the later defined normalize function\n",
        "  The actual filtering and normalization takes place in the called normalize funcion.\n",
        "  Most of the function is reading the file and formatting into correct shape for the normalize function.\n",
        "  Requires the anndata package and the h5py package to read in the h5 file\n",
        "  # Arguments\n",
        "      filename: filename of the raw datafile (in .h5 format)\n",
        "      gene_num: number of genes that you want to process. Comes from an hyper-parameter (default for our model is 2048)\n",
        "      raw: bool. Used to determine if the the input into the normalize function needs to normalized or not.\n",
        "           Used only if the dataset is already normalized\n",
        "  # Return\n",
        "      count, adata.obs (anndata object that contains normalized matrix and annotations)\n",
        "'''\n",
        "def normalize_for_AF(filename,gene_num,raw, sparsify = False, skip_exprs = False):\n",
        "    with h5py.File(filename, \"r\") as f:\n",
        "        obs = np.array(f[\"obs_names\"][...])\n",
        "        var = np.array(f[\"var_names\"][...])\n",
        "        cell_name = np.array(f[\"obs\"][\"cell_type1\"])\n",
        "        cell_type, cell_label = np.unique(cell_name, return_inverse=True)\n",
        "        class_num = np.array(cell_label).max() + 1\n",
        "        data_label = []\n",
        "        data_array = []\n",
        "        for i in range(cell_label.shape[0]):\n",
        "            x = np.zeros(class_num)\n",
        "            x[cell_label[i]] = 1\n",
        "            data_label.append(x)\n",
        "        data_label = np.array(data_label)\n",
        "        cell_type = np.array(cell_type)\n",
        "        if not skip_exprs:\n",
        "            exprs_handle = f[\"exprs\"]\n",
        "            if isinstance(exprs_handle, h5py.Group):\n",
        "                mat = sp.csr_matrix((exprs_handle[\"data\"][...], exprs_handle[\"indices\"][...],\n",
        "                                         exprs_handle[\"indptr\"][...]), shape=exprs_handle[\"shape\"][...])\n",
        "            else:\n",
        "                mat = exprs_handle[...].astype(np.float32)\n",
        "                if sparsify:\n",
        "                    mat = sp.csr_matrix(mat)\n",
        "        else:\n",
        "            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))\n",
        "        X = np.array(mat.toarray())\n",
        "        np.int = int\n",
        "        X = np.ceil(X).astype(np.int)\n",
        "        adata = sc.AnnData(X)\n",
        "        adata.obs['Group'] = cell_label\n",
        "        adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=raw, logtrans_input=True)\n",
        "        count = adata.X\n",
        "        return count,adata.obs['Group']\n",
        "\n",
        "def nomalize_for_AD(file_name,label_path,gene_num):\n",
        "    data = pd.read_csv(file_name, header=None, sep=\",\")\n",
        "    label = pd.read_csv(label_path, header=0, sep=\",\")\n",
        "    # data_label = []\n",
        "    label = np.array(label)[:, 2]\n",
        "    cell_type, cell_label = np.unique(label, return_inverse=True)\n",
        "    data_label = []\n",
        "    for i in range(len(cell_label)):\n",
        "        data_label.append(cell_type[cell_label[i]])\n",
        "    data_label = np.array(data_label)\n",
        "    print(data_label)\n",
        "    arr1 = np.array(data)\n",
        "    gene_name = np.array(arr1[1:, 0])\n",
        "    cell_name = np.array(arr1[0, 1:])\n",
        "    X = arr1[1:, 1:].T\n",
        "    adata = sc.AnnData(X)\n",
        "    adata.obs['Group'] = data_label\n",
        "    adata.obs['cell_name'] = cell_name\n",
        "    adata.var['Gene'] = gene_name\n",
        "    adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=False,\n",
        "                      logtrans_input=True)\n",
        "    count = adata.X\n",
        "    return count, cell_label,adata.obs['size_factors'],adata.var['Gene']\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Calculate clustering accuracy. Require scikit-learn installed\n",
        "  # Arguments\n",
        "      y: true labels, numpy.array with shape `(n_samples,)`\n",
        "      y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
        "  # Return\n",
        "      accuracy (in [0,1])\n",
        "\"\"\"\n",
        "def acc(y_true, y_pred):\n",
        "\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    ind = linear_sum_assignment(w.max() - w)\n",
        "    ind = np.array(ind).T\n",
        "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "  Filters and Normalizes a given dataset and returns the matrix\n",
        "  Requires the anndata package\n",
        "  # Arguments\n",
        "      adata: filename of the raw datafile (in .h5 format)\n",
        "      rest of the arguments are options for the normalization\n",
        "  # Return\n",
        "      adata (anndata object that contains normalized matrix and annotations)\n",
        "'''\n",
        "def normalize(adata, copy=True, highly_genes = None, filter_min_counts=True, size_factors=True, normalize_input=True, logtrans_input=True):\n",
        "    if isinstance(adata, sc.AnnData):\n",
        "        if copy:\n",
        "            adata = adata.copy()\n",
        "    elif isinstance(adata, str):\n",
        "        adata = sc.read(adata)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    norm_error = 'Make sure that the dataset (adata.X) contains unnormalized count data.'\n",
        "    assert 'n_count' not in adata.obs, norm_error\n",
        "\n",
        "    if adata.X.size < 50e6: # check if adata.X is integer only if array is small\n",
        "        if sp.issparse(adata.X):\n",
        "            assert (adata.X.astype(int) != adata.X).nnz == 0, norm_error\n",
        "        else:\n",
        "            assert np.all(adata.X.astype(int) == adata.X), norm_error\n",
        "\n",
        "    if filter_min_counts:\n",
        "        sc.pp.filter_genes(adata, min_counts=1)\n",
        "        sc.pp.filter_cells(adata, min_counts=1)\n",
        "\n",
        "    if size_factors or normalize_input or logtrans_input:\n",
        "        adata.raw = adata.copy()\n",
        "    else:\n",
        "        adata.raw = adata\n",
        "\n",
        "    if size_factors:\n",
        "        # sc.pp.normalize_per_cell(adata)\n",
        "        sc.pp.normalize_total(adata)\n",
        "        adata.obs['size_factors'] = adata.obs.n_counts / np.median(adata.obs.n_counts)\n",
        "    else:\n",
        "        adata.obs['size_factors'] = 1.0\n",
        "\n",
        "    if logtrans_input:\n",
        "        sc.pp.log1p(adata)\n",
        "\n",
        "    if highly_genes is not None:\n",
        "        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, n_top_genes = highly_genes, subset=True)\n",
        "\n",
        "    if normalize_input:\n",
        "        sc.pp.scale(adata)\n",
        "\n",
        "    return adata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwJLGGS-31Yy"
      },
      "source": [
        "## (3) Helper functions\n",
        "\n",
        "The below functions are helper and utility functions that make generating the graph, data processing, and imputation easier. The last helper function defines the single cell class. These cells need to be run before training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojFgpkeP4RpW"
      },
      "source": [
        "### (3.1) Graph based functions\n",
        "\n",
        "Please see comments below for explanation on each function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P1vo3v0MVHM",
        "outputId": "091b6c99-2764-4837-c8ed-c2a9f9c51a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.71 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# preprocessing pipeline for graph-based data analysis\n",
        "\n",
        "# this function constructs an adjacency matrix and its normalized count data\n",
        "# Optionally applies PCA to reduce dimensionality before computing\n",
        "# the k-nearest neighbors graph.\n",
        "def get_adj(count, k=15, pca=50, mode=\"connectivity\"):\n",
        "    if pca:\n",
        "        countp = dopca(count, dim=pca)\n",
        "    else:\n",
        "        countp = count\n",
        "    A = kneighbors_graph(countp, k, mode=mode, metric=\"cosine\", include_self=True)\n",
        "    adj = A.toarray()\n",
        "    adj_n = norm_adj(adj)\n",
        "    return adj, adj_n\n",
        "\n",
        "# This function computes a diagonal matrix where each diagonal element\n",
        "# is the degree of the corresponding node in the graph\n",
        "def degree_power(A, k):\n",
        "    degrees = np.power(np.array(A.sum(1)), k).flatten()\n",
        "    degrees[np.isinf(degrees)] = 0.\n",
        "    if sp.issparse(A):\n",
        "        D = sp.diags(degrees)\n",
        "    else:\n",
        "        D = np.diag(degrees)\n",
        "    return D\n",
        "\n",
        "# This function normalizes the adjacency matrix using the degree matrix.\n",
        "def norm_adj(A):\n",
        "    normalized_D = degree_power(A, -0.5)\n",
        "    output = normalized_D.dot(A).dot(normalized_D)\n",
        "    return output\n",
        "\n",
        "# This function applies PCA to the data to reduce its dimensionality\n",
        "def dopca(X, dim=10):\n",
        "    pcaten = PCA(n_components=dim)\n",
        "    X_10 = pcaten.fit_transform(X)\n",
        "    return X_10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ZpXd2j471b"
      },
      "source": [
        "### (3.2) Imputation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hV3XDMVq5Biw",
        "outputId": "ef14226e-b0a9-4796-e425-36808663d444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.75 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "# Revised from Original version in scVI\n",
        "# Ref:\n",
        "# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py\n",
        "\n",
        "\"\"\"\n",
        "  X: original testing set\n",
        "  ========\n",
        "  returns:\n",
        "  X_zero: copy of X with zeros\n",
        "  i, j, ix: indices of where dropout is applied\n",
        "\"\"\"\n",
        "def impute_dropout(X, seed=1, rate=0.1):\n",
        "\n",
        "\n",
        "    # If the input is a dense matrix\n",
        "    if isinstance(X, np.ndarray):\n",
        "        X_zero = np.copy(X)\n",
        "        # select non-zero subset\n",
        "        i, j = np.nonzero(X_zero)\n",
        "    # If the input is a sparse matrix\n",
        "    else:\n",
        "        X_zero = scipy.sparse.lil_matrix.copy(X)\n",
        "        # select non-zero subset\n",
        "        i, j = X_zero.nonzero()\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    # changes here:\n",
        "    # choice number 1 : select 10 percent of the non zero values (so that distributions overlap enough)\n",
        "    ix = np.random.choice(range(len(i)), int(\n",
        "        np.floor(rate * len(i))), replace=False)\n",
        "    # X_zero[i[ix], j[ix]] *= np.random.binomial(1, rate)\n",
        "    X_zero[i[ix], j[ix]] = 0.0\n",
        "\n",
        "    # choice number 2, focus on a few but corrupt binomially\n",
        "    #ix = np.random.choice(range(len(i)), int(slice_prop * np.floor(len(i))), replace=False)\n",
        "    #X_zero[i[ix], j[ix]] = np.random.binomial(X_zero[i[ix], j[ix]].astype(np.int), rate)\n",
        "    return X_zero, i, j, ix\n",
        "\n",
        "# IMPUTATION METRICS\n",
        "# Revised freom Original version in scVI\n",
        "# Ref:\n",
        "# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  X_mean: imputed dataset\n",
        "  X: original dataset\n",
        "  X_zero: zeros dataset, does not need\n",
        "  i, j, ix: indices of where dropout was applied\n",
        "  ========\n",
        "  returns:\n",
        "  median L1 distance between datasets at indices given\n",
        "\"\"\"\n",
        "def imputation_error(X_mean, X, X_zero, i, j, ix):\n",
        "\n",
        "    # If the input is a dense matrix\n",
        "    if isinstance(X, np.ndarray):\n",
        "        all_index = i[ix], j[ix]\n",
        "        x, y = X_mean[all_index], X[all_index]\n",
        "        result = np.abs(x - y)\n",
        "    # If the input is a sparse matrix\n",
        "    else:\n",
        "        all_index = i[ix], j[ix]\n",
        "        x = X_mean[all_index[0], all_index[1]]\n",
        "        y = X[all_index[0], all_index[1]]\n",
        "        yuse = scipy.sparse.lil_matrix.todense(y)\n",
        "        yuse = np.asarray(yuse).reshape(-1)\n",
        "        result = np.abs(x - yuse)\n",
        "    # return np.median(np.abs(x - yuse))\n",
        "    return np.mean(result), np.median(result), np.min(result), np.max(result)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  X_mean: imputed dataset\n",
        "  X: original dataset\n",
        "  X_zero: zeros dataset, does not need\n",
        "  i, j, ix: indices of where dropout was applied\n",
        "  ========\n",
        "  returns:\n",
        "  cosine similarity between datasets at indices given\n",
        "\"\"\"\n",
        "def imputation_cosine(X_mean, X, X_zero, i, j, ix):\n",
        "\n",
        "    # If the input is a dense matrix\n",
        "    if isinstance(X, np.ndarray):\n",
        "        all_index = i[ix], j[ix]\n",
        "        x, y = X_mean[all_index], X[all_index]\n",
        "        x = x.reshape(1, -1)\n",
        "        y = y.reshape(1, -1)\n",
        "\n",
        "        print(x)\n",
        "        print(y)\n",
        "        result = cosine_similarity(x, y)\n",
        "    # If the input is a sparse matrix\n",
        "    else:\n",
        "        all_index = i[ix], j[ix]\n",
        "        x = X_mean[all_index[0], all_index[1]]\n",
        "        y = X[all_index[0], all_index[1]]\n",
        "        yuse = scipy.sparse.lil_matrix.todense(y)\n",
        "        yuse = np.asarray(yuse).reshape(-1)\n",
        "        x = x.reshape(1, -1)\n",
        "        yuse = yuse.reshape(1, -1)\n",
        "        result = cosine_similarity(x, yuse)\n",
        "    # return np.median(np.abs(x - yuse))\n",
        "    return result[0][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAmmbiNB5H4o"
      },
      "source": [
        "### (3.3) Define Single Cell class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE6NZLDIMPhH",
        "outputId": "799820b0-dd9a-4cda-c783-48c28f8da3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.04 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#This class defines a class for a single cell\n",
        "#it initializes with a filepath to the raw dataset\n",
        "#It then runs the process method and then uses that to create nodes and edges based on the data\n",
        "class Singlecell(InMemoryDataset):\n",
        "\n",
        "    def __init__(self, root, name, filepath, transform=None, pre_transform=None):\n",
        "        self.name = name.lower()\n",
        "        self.filepath = filepath\n",
        "        super(Singlecell, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "        return osp.join(self.root, self.name.capitalize(), 'raw')\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.root, self.name.capitalize(), 'processed')\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return 'amazon_electronics_{}.npz'.format(self.name.lower())\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def download(self):\n",
        "\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "\n",
        "        raw = False\n",
        "        data, data_label = normalize_for_AF(self.filepath, 2048,raw);\n",
        "\n",
        "        x = torch.tensor(np.array(data),dtype=torch.float32)\n",
        "        y = torch.tensor(data_label, dtype=torch.long)\n",
        "\n",
        "        adj, adj_n = get_adj(data)\n",
        "        adj = sp.coo_matrix(adj_n)\n",
        "        edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)\n",
        "        edge_index, _ = remove_self_loops(edge_index)\n",
        "        edge_index = to_undirected(edge_index, x.size(0))  # Internal coalesce.\n",
        "        data = Data(x=x, edge_index=edge_index, y=y)\n",
        "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
        "        data, slices = self.collate([data])\n",
        "        print(self.processed_paths[0])\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}{}()'.format(self.__class__.__name__, self.name.capitalize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtFeVsVqxyrU"
      },
      "source": [
        "### (3.4) Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi_REaQFNTGw",
        "outputId": "5e197b5f-dc9a-47e9-8fea-8785ad1914fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.8 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#set seed for random function\n",
        "#Even though the seed is the same as the paper, the random number generator is different\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "#Returns the current time using datetime\n",
        "def currentTime():\n",
        "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "#This function parses the command line arguments\n",
        "#In a notebook, there are no command line arguments but this function still returns the default values\n",
        "#You can see all the options and their defaults to run the model\n",
        "#Change the default values to vary the hyperparameters of the model\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--embedder\", type=str, default=\"AFGRL\")\n",
        "    parser.add_argument(\"--dataset\", type=str, default=\"adam\", help=\"Name of the dataset. Supported names are: wikics, cs, computers, photo, and physics\")\n",
        "\n",
        "    parser.add_argument('--checkpoint_dir', type=str, default = '/content/model_checkpoints', help='directory to save checkpoint')\n",
        "    parser.add_argument(\"--root\", type=str, default=\"data\")\n",
        "    parser.add_argument(\"--task\", type=str, default=\"clustering\", help=\"Downstream task. Supported tasks are: node, clustering, similarity\")\n",
        "\n",
        "    parser.add_argument(\"--layers\", nargs='?', default='[2048]', help=\"The number of units of each layer of the GNN. Default is [256]\")\n",
        "    parser.add_argument(\"--pred_hid\", type=int, default=2048, help=\"The number of hidden units of layer of the predictor. Default is 512\")\n",
        "\n",
        "    parser.add_argument(\"--topk\", type=int, default=4, help=\"The number of neighbors to search\")\n",
        "    parser.add_argument(\"--clus_num_iters\", type=int, default=20)\n",
        "    parser.add_argument(\"--num_centroids\", type=int, default=9, help=\"The number of centroids for K-means Clustering\")\n",
        "    parser.add_argument(\"--num_kmeans\", type=int, default=5, help=\"The number of K-means Clustering for being robust to randomness\")\n",
        "    parser.add_argument(\"--eval_freq\", type=float, default=5, help=\"The frequency of model evaluation\")\n",
        "    parser.add_argument(\"--mad\", type=float, default=0.9, help=\"Moving Average Decay for Teacher Network\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
        "    parser.add_argument(\"--es\", type=int, default=300, help=\"Early Stopping Criterion\")\n",
        "    parser.add_argument(\"--device\", type=int, default=0)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1) #For demonstration purposes, set to 1. Model has been pre-trained with 300 epochs.\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "    parser.add_argument(\"--aug_params\", \"-p\", nargs=\"+\", default=[0.3, 0.4, 0.3, 0.2],help=\"Hyperparameters for augmentation (p_f1, p_f2, p_e1, p_e2). Default is [0.2, 0.1, 0.2, 0.3]\")\n",
        "    return parser.parse_known_args()\n",
        "\n",
        "class Augmentation:\n",
        "\n",
        "    def __init__(self, p_f1=0.2, p_f2=0.1, p_e1=0.2, p_e2=0.3):\n",
        "        \"\"\"\n",
        "        two simple graph augmentation functions --> \"Node feature masking\" and \"Edge masking\"\n",
        "        Random binary node feature mask following Bernoulli distribution with parameter p_f\n",
        "        Random binary edge mask following Bernoulli distribution with parameter p_e\n",
        "        \"\"\"\n",
        "        self.p_f1 = p_f1\n",
        "        self.p_f2 = p_f2\n",
        "        self.p_e1 = p_e1\n",
        "        self.p_e2 = p_e2\n",
        "        self.method = \"BGRL\"\n",
        "\n",
        "    def _feature_masking(self, data, device):\n",
        "        feat_mask1 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f1\n",
        "        feat_mask2 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f2\n",
        "        feat_mask1, feat_mask2 = feat_mask1.to(device), feat_mask2.to(device)\n",
        "        x1, x2 = data.x.clone(), data.x.clone()\n",
        "        x1, x2 = x1 * feat_mask1, x2 * feat_mask2\n",
        "\n",
        "        edge_index1, edge_attr1 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e1)\n",
        "        edge_index2, edge_attr2 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e2)\n",
        "\n",
        "        new_data1, new_data2 = data.clone(), data.clone()\n",
        "        new_data1.x, new_data2.x = x1, x2\n",
        "        new_data1.edge_index, new_data2.edge_index = edge_index1, edge_index2\n",
        "        new_data1.edge_attr, new_data2.edge_attr = edge_attr1, edge_attr2\n",
        "\n",
        "        return new_data1, new_data2\n",
        "\n",
        "    def __call__(self, data):\n",
        "        return self._feature_masking(data)\n",
        "\n",
        "def decide_config(root, dataset):\n",
        "    \"\"\"\n",
        "    Create a configuration to download datasets\n",
        "    :param root: A path to a root directory where data will be stored\n",
        "    :param dataset: The name of the dataset to be downloaded\n",
        "    :return: A modified root dir, the name of the dataset class, and parameters associated to the class\n",
        "    \"\"\"\n",
        "    dataset = dataset.lower()\n",
        "    if dataset == \"adam\":\n",
        "        dataset = \"Adam\"\n",
        "        root = osp.join(root, \"pyg\")\n",
        "        filepath = '/content/data_testing/test_csv/data.h5'\n",
        "        params = {\"kwargs\": {\"root\": root, \"name\": dataset, \"filepath\": filepath},\n",
        "                  \"name\": dataset, \"class\": Singlecell, \"src\": \"pyg\"}\n",
        "    else:\n",
        "        raise Exception(\n",
        "            f\"Unknown dataset name {dataset}, name has to be one of the following 'cora', 'citeseer', 'pubmed', 'photo', 'computers', 'cs', 'physics'\")\n",
        "    return params\n",
        "\n",
        "\n",
        "def create_dirs(dirs):\n",
        "    \"\"\"\n",
        "    Create directories based on the directory structure provided\n",
        "    This function takes a list of directory paths and creates the corresponding directory structure\n",
        "    param: dirs (list): A list of directory paths where each path represents a directory structure\n",
        "\n",
        "    \"\"\"\n",
        "    for dir_tree in dirs:\n",
        "        sub_dirs = dir_tree.split(\"/\")\n",
        "        path = \"\"\n",
        "        for sub_dir in sub_dirs:\n",
        "            path = osp.join(path, sub_dir)\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def tensor2var(x, grad=False):\n",
        "    \"\"\"\n",
        "    Convert a PyTorch tensor to a PyTorch Variable, optionally specifying gradient computation\n",
        "    param: x (torch.Tensor): The input PyTorch tensor to convert\n",
        "    param: grad (bool, optional): Whether to compute gradients for the variable. Defaults to False\n",
        "    return: torch.autograd.Variable: The converted PyTorch Variable\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x, requires_grad=grad)\n",
        "\n",
        "def create_masks(data):\n",
        "    \"\"\"\n",
        "    Splits data into training, validation, and test splits in a stratified manner if\n",
        "    it is not already splitted. Each split is associated with a mask vector, which\n",
        "    specifies the indices for that split. The data will be modified in-place\n",
        "    :param data: Data object\n",
        "    :return: The modified data\n",
        "    \"\"\"\n",
        "    if not hasattr(data, \"val_mask\"):\n",
        "\n",
        "        data.train_mask = data.dev_mask = data.test_mask = None\n",
        "\n",
        "        for i in range(20):\n",
        "            labels = data.y.numpy()\n",
        "            dev_size = int(labels.shape[0] * 0.1)\n",
        "            test_size = int(labels.shape[0] * 0.8)\n",
        "\n",
        "            perm = np.random.permutation(labels.shape[0])\n",
        "            test_index = perm[:test_size]\n",
        "            dev_index = perm[test_size:test_size + dev_size]\n",
        "\n",
        "            data_index = np.arange(labels.shape[0])\n",
        "            test_mask = torch.tensor(np.in1d(data_index, test_index), dtype=torch.bool)\n",
        "            dev_mask = torch.tensor(np.in1d(data_index, dev_index), dtype=torch.bool)\n",
        "            train_mask = ~(dev_mask + test_mask)\n",
        "            test_mask = test_mask.reshape(1, -1)\n",
        "            dev_mask = dev_mask.reshape(1, -1)\n",
        "            train_mask = train_mask.reshape(1, -1)\n",
        "\n",
        "            if 'train_mask' not in data:\n",
        "                data.train_mask = train_mask\n",
        "                data.val_mask = dev_mask\n",
        "                data.test_mask = test_mask\n",
        "            else:\n",
        "                data.train_mask = torch.cat((data.train_mask, train_mask), dim=0)\n",
        "                data.val_mask = torch.cat((data.val_mask, dev_mask), dim=0)\n",
        "                data.test_mask = torch.cat((data.test_mask, test_mask), dim=0)\n",
        "\n",
        "    else:  # in the case of WikiCS\n",
        "        data.train_mask = data.train_mask.T\n",
        "        data.val_mask = data.val_mask.T\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, beta, epochs):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        self.step = 0\n",
        "        self.total_steps = epochs\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "\n",
        "        beta = 1 - (1 - self.beta) * (np.cos(np.pi * self.step / self.total_steps) + 1) / 2.0\n",
        "        self.step += 1\n",
        "        return old * beta + (1 - beta) * new\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Initialize weights of a neural network module using Xavier uniform initialization\n",
        "    Initialization and sets the bias to a value of 0.01\n",
        "    param: m (nn.Module): The neural network module\n",
        "    \"\"\"\n",
        "\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "def loss_fn(x, y):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity loss between two vectors\n",
        "    param: x (torch.Tensor): The first input tensor\n",
        "    param: y (torch.Tensor): The second input tensor\n",
        "    return: torch.Tensor: The cosine similarity loss\n",
        "    \"\"\"\n",
        "    x = F.normalize(x, dim=-1, p=2)\n",
        "    y = F.normalize(y, dim=-1, p=2)\n",
        "\n",
        "    return 2 - 2 * (x * y).sum(dim=-1)\n",
        "\n",
        "\n",
        "def l2_normalize(x):\n",
        "    \"\"\"\n",
        "    Perform L2 normalization on a tensor\n",
        "    param: x (torch.Tensor): The input tensor\n",
        "    returns: torch.Tensor: The L2 normalized tensor\n",
        "    \"\"\"\n",
        "    return x / torch.sqrt(torch.sum(x**2, dim=1).unsqueeze(1))\n",
        "\n",
        "\n",
        "def update_moving_average(ema_updater, ma_model, current_model):\n",
        "    \"\"\"\n",
        "    Update the moving average of a model's parameters\n",
        "    This function updates the moving average parameters of 'ma_model' based on the current parameters\n",
        "    of 'current_model' using the provided 'ema_updater' object\n",
        "    param: ema_updater: The object responsible for updating the moving average\n",
        "    param: ma_model (nn.Module): The model with the moving average parameters\n",
        "    param: current_model (nn.Module): The model with the current parameters\n",
        "    \"\"\"\n",
        "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "        old_weight, up_weight = ma_params.data, current_params.data\n",
        "        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n",
        "\n",
        "\n",
        "def set_requires_grad(model, val):\n",
        "    \"\"\"\n",
        "    Set the requires_grad attribute of all parameters in a given model\n",
        "    param: model (nn.Module): The neural network model\n",
        "    param: val (bool): The value to set for requires_grad\n",
        "    \"\"\"\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = val\n",
        "\n",
        "\n",
        "def enumerateConfig(args):\n",
        "    \"\"\"\n",
        "    Enumerate and return the names and values of attributes in an object\n",
        "    param: args: An object containing attributes to enumerate\n",
        "    return: tuple: A tuple containing lists of attribute names and values\n",
        "\n",
        "    This function extracts the names and corresponding values of attributes from the input object\n",
        "    \"\"\"\n",
        "    args_names = []\n",
        "    args_vals = []\n",
        "    for arg in vars(args):\n",
        "        args_names.append(arg)\n",
        "        args_vals.append(getattr(args, arg))\n",
        "\n",
        "    return args_names, args_vals\n",
        "\n",
        "\n",
        "def config2string(args):\n",
        "    \"\"\"\n",
        "    This function converts specified attributes of the input object to a formatted string\n",
        "    param: args: An object containing attributes\n",
        "    return: str: A formatted string representing the attributes\n",
        "    \"\"\"\n",
        "    args_names, args_vals = enumerateConfig(args)\n",
        "    st = ''\n",
        "    for name, val in zip(args_names, args_vals):\n",
        "        if val == False:\n",
        "            continue\n",
        "        if name in ['dataset']:\n",
        "            st_ = \"{}_{}_\".format(name, val)\n",
        "            st += st_\n",
        "\n",
        "    return st[:-1]\n",
        "\n",
        "\n",
        "def printConfig(args):\n",
        "    \"\"\"\n",
        "    Print the names and values of the givne args\n",
        "    param: args: An object containing attributes\n",
        "    \"\"\"\n",
        "    args_names, args_vals = enumerateConfig(args)\n",
        "    print(args_names)\n",
        "    print(args_vals)\n",
        "\n",
        "\n",
        "def repeat_1d_tensor(t, num_reps):\n",
        "    \"\"\"\n",
        "    This function repeats the input 1D tensor 'num_reps' times along a new dimension\n",
        "    param: t (torch.Tensor): The input 1D tensor\n",
        "    param: num_reps (int): The number of times to repeat the tensor\n",
        "    return: torch.Tensor: The repeated tensor.\n",
        "    \"\"\"\n",
        "    return t.unsqueeze(1).expand(-1, num_reps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2Bgey9kLm9n",
        "outputId": "836d94c1-9212-4e99-ff54-341f6ee03b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.96 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "def download_pyg_data(config):\n",
        "    \"\"\"\n",
        "    Downloads a dataset from the PyTorch Geometric library\n",
        "\n",
        "    :param config: A dict containing info on the dataset to be downloaded\n",
        "    :return: A tuple containing (root directory, dataset name, data directory)\n",
        "    \"\"\"\n",
        "    leaf_dir = config[\"kwargs\"][\"root\"].split(\"/\")[-1].strip()\n",
        "    data_dir = osp.join(config[\"kwargs\"][\"root\"], \"\" if config[\"name\"] == leaf_dir else config[\"name\"])\n",
        "    dst_path = osp.join(data_dir, \"raw\", \"data.pt\")\n",
        "    if not osp.exists(dst_path):\n",
        "        DatasetClass = config[\"class\"]\n",
        "        if config[\"name\"] == \"WikiCS\":\n",
        "            dataset = DatasetClass(data_dir, transform=T.NormalizeFeatures())\n",
        "            size_factor = dataset.size_factors\n",
        "        else :\n",
        "            dataset = DatasetClass(**config[\"kwargs\"])\n",
        "        create_masks(data=dataset.data)\n",
        "        torch.save((dataset.data, dataset.slices), dst_path)\n",
        "    return config[\"kwargs\"][\"root\"], config[\"name\"], data_dir\n",
        "\n",
        "\n",
        "def download_data(root, dataset):\n",
        "    \"\"\"\n",
        "    Download data from different repositories. Currently only PyTorch Geometric is supported\n",
        "\n",
        "    :param root: The root directory of the dataset\n",
        "    :param name: The name of the dataset\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    config = decide_config(root=root, dataset=dataset)\n",
        "    if config[\"src\"] == \"pyg\":\n",
        "        return download_pyg_data(config)\n",
        "\n",
        "\n",
        "class Dataset(InMemoryDataset):\n",
        "\n",
        "    \"\"\"\n",
        "    A PyTorch InMemoryDataset to build multi-view dataset through graph data augmentation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root=\"data\", dataset='Adam', transform=None, pre_transform=None):\n",
        "        self.root, self.dataset, self.data_dir = download_data(root=root, dataset=dataset)\n",
        "        create_dirs(self.dirs)\n",
        "        super().__init__(root=self.data_dir, transform=transform, pre_transform=pre_transform)\n",
        "        self.process()\n",
        "        path = osp.join(self.data_dir, \"processed\", self.processed_file_names[0])\n",
        "        self.data, self.slices = torch.load(path)\n",
        "\n",
        "    def process_full_batch_data(self, data):\n",
        "\n",
        "        print(\"Processing full batch data\")\n",
        "        nodes = torch.tensor(np.arange(data.num_nodes), dtype=torch.long)\n",
        "\n",
        "        edge_index, edge_attr = add_self_loops(data.edge_index, data.edge_attr)\n",
        "\n",
        "        data = Data(nodes=nodes, edge_index=data.edge_index, edge_attr=data.edge_attr, x=data.x, y=data.y,\n",
        "                    train_mask=data.train_mask, val_mask=data.val_mask, test_mask=data.test_mask,\n",
        "                    num_nodes=data.num_nodes, neighbor_index=edge_index, neighbor_attr=edge_attr)\n",
        "\n",
        "        return [data]\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"\n",
        "        Process either a full batch or cluster data.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        processed_path = osp.join(self.processed_dir, self.processed_file_names[0])\n",
        "        if not osp.exists(processed_path):\n",
        "            path = osp.join(self.raw_dir, self.raw_file_names[0])\n",
        "            data, _ = torch.load(path)\n",
        "            edge_attr = data.edge_attr\n",
        "            edge_attr = torch.ones(data.edge_index.shape[1]) if edge_attr is None else edge_attr\n",
        "            data.edge_attr = edge_attr\n",
        "\n",
        "\n",
        "            data_list = self.process_full_batch_data(data)\n",
        "\n",
        "            data, slices = self.collate(data_list)\n",
        "            torch.save((data, slices), processed_path)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [\"data.pt\"]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return [f'byg.data.pt']\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "        return osp.join(self.data_dir, \"raw\")\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "        return osp.join(self.data_dir, \"processed\")\n",
        "\n",
        "    @property\n",
        "    def model_dir(self):\n",
        "        return osp.join(self.data_dir, \"model\")\n",
        "\n",
        "    @property\n",
        "    def result_dir(self):\n",
        "        return osp.join(self.data_dir, \"results\")\n",
        "\n",
        "    @property\n",
        "    def dirs(self):\n",
        "        return [self.raw_dir, self.processed_dir, self.model_dir, self.result_dir]\n",
        "\n",
        "    def download(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##  (4) Model\n",
        "\n",
        "**References:**\n",
        "\n",
        "1.  Zehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, Bo Wang, scGCL: an imputation method for scRNA-seq data based on graph contrastive learning, Bioinformatics, Volume 39, Issue 3, March 2023, btad098, https://doi.org/10.1093/bioinformatics/btad098\n",
        "\n",
        "\n",
        "2. Link to the original github repository: https://github.com/zehaoxiong123/scGCL/tree/main\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Model architectures:** There are two different models that make up the overall scGCL algorithm\n",
        "  1. The Augment-Free Graph Representation Learning (AFGRL):\n",
        "  This GNN model is used to capture the topographical information of the cell graph\n",
        "    *  Layers: 2048\n",
        "  \n",
        "  2. The ZINB Autoencoder:\n",
        "  This model is used to capture the characteristics of scRNA sequence data.\n",
        "    *  Layers: 2048\n",
        "\n",
        "Both of these models combined are used to update the scGCL method that aims to reduce the similarity between two nodes on the cell graph.\n",
        "\n",
        "\n",
        "\n",
        "**Training objectives:**\n",
        "  1. Optimizer: We use the AdamW optimizer method\n",
        "  2. Loss function: The scGCL loss value is calculated from combining the contrast loss, ZINB loss and the reconstruction loss of scRNA-seq profiles. You can see this loss calculation in the formula below:\n",
        "\n",
        "  $L = γ_1L_{Contrast} + γ_2L_{ZINB} + γ_3L_{Reconstruct}$\n",
        "\n",
        "  where $γ_1$, $γ_2$ and $γ_3$ are hyperparameters that are used to control the loss.\n",
        "\n",
        "\n",
        "\n",
        "**Others:**\n",
        "\n",
        "There is a pretrained model saved but you can train another model with 1 epoch by running the training cells.\n",
        "\n",
        "The pre-trained model is stored in a file in the 'model_checkpoints' folder. The evaluate funtion is set to use this pre-trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmZ5fq8tQDK5",
        "outputId": "52871832-a5bf-4c6c-9094-8450f629665d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 13.2 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, num_dim, num_class):\n",
        "        \"\"\"\n",
        "        This class defines a logistic regression model with a linear layer and cross-entropy loss.\n",
        "        param: num_dim (int): Number of input dimensions.\n",
        "        param: num_class (int): Number of classes.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(num_dim, num_class)\n",
        "        torch.nn.init.xavier_uniform_(self.linear.weight.data)\n",
        "        self.linear.bias.data.fill_(0.0)\n",
        "        self.cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        logits = self.linear(x)\n",
        "        loss = self.cross_entropy(logits, y)\n",
        "        return logits, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQFonakSKhzg",
        "outputId": "ef653348-3960-4278-e4c6-eeb9a51a54ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.55 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# scGCL Model\n",
        "# Revised freom Original version in AFGRL\n",
        "# Ref:\n",
        "# https://github.com/Namkyeong/AFGRL/tree/master/embedder.py\n",
        "\n",
        "class embedder:\n",
        "    \"\"\"\n",
        "    This class implements methods for inference and evaluation of embeddings\n",
        "    This model defined here is a revised version of the AFGRL model\n",
        "    params: args: args object that defines the parameters of the embeddings model\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.hidden_layers = eval(args.layers)\n",
        "        printConfig(args)\n",
        "\n",
        "    def infer_embeddings(self, epoch):\n",
        "        self._model.train(False)\n",
        "        self._embeddings = self._labels = None\n",
        "        self._train_mask = self._dev_mask = self._test_mask = None\n",
        "\n",
        "        for bc, batch_data in enumerate(self._loader):\n",
        "            # augmentation = utils.Augmentation(float(self._args.aug_params[0]), float(self._args.aug_params[1]),\n",
        "            #                                   float(self._args.aug_params[2]), float(self._args.aug_params[3]))\n",
        "\n",
        "            batch_data.to(self._device)\n",
        "            # view1, view2 = augmentation._feature_masking(batch_data, self._device)\n",
        "\n",
        "            emb, loss = self._model(x = batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,\n",
        "                                                                           neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],\n",
        "                                                                           edge_weight=batch_data.edge_attr, epoch=epoch)\n",
        "            emb = emb.detach()\n",
        "            y = batch_data.y.detach()\n",
        "            if self._embeddings is None:\n",
        "                self._embeddings, self._labels = emb, y\n",
        "            else:\n",
        "                self._embeddings = torch.cat([self._embeddings, emb])\n",
        "                self._labels = torch.cat([self._labels, y])\n",
        "\n",
        "\n",
        "    def evaluate(self, task, epoch, sillog):\n",
        "        if task == \"node\":\n",
        "            self.evaluate_node(epoch)\n",
        "        elif task == \"clustering\":\n",
        "            self.evaluate_clustering(epoch,sillog)\n",
        "        elif task == \"similarity\":\n",
        "            self.run_similarity_search(epoch)\n",
        "\n",
        "\n",
        "    def evaluate_node(self, epoch):\n",
        "        emb_dim, num_class = self._embeddings.shape[1], self._labels.unique().shape[0]\n",
        "\n",
        "        dev_accs, test_accs = [], []\n",
        "\n",
        "        for i in range(20):\n",
        "\n",
        "            self._train_mask = self._dataset[0].train_mask[i]\n",
        "            self._dev_mask = self._dataset[0].val_mask[i]\n",
        "            if self._args.dataset == \"wikics\":\n",
        "                self._test_mask = self._dataset[0].test_mask\n",
        "            else:\n",
        "                self._test_mask = self._dataset[0].test_mask[i]\n",
        "\n",
        "            classifier = LogisticRegression(emb_dim, num_class).to(self._device)\n",
        "            optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "\n",
        "            for _ in range(100):\n",
        "                classifier.train()\n",
        "                logits, loss = classifier(self._embeddings[self._train_mask], self._labels[self._train_mask])\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            dev_logits, _ = classifier(self._embeddings[self._dev_mask], self._labels[self._dev_mask])\n",
        "            test_logits, _ = classifier(self._embeddings[self._test_mask], self._labels[self._test_mask])\n",
        "            dev_preds = torch.argmax(dev_logits, dim=1)\n",
        "            test_preds = torch.argmax(test_logits, dim=1)\n",
        "\n",
        "            dev_acc = (torch.sum(dev_preds == self._labels[self._dev_mask]).float() /\n",
        "                       self._labels[self._dev_mask].shape[0]).detach().cpu().numpy()\n",
        "            test_acc = (torch.sum(test_preds == self._labels[self._test_mask]).float() /\n",
        "                        self._labels[self._test_mask].shape[0]).detach().cpu().numpy()\n",
        "\n",
        "            dev_accs.append(dev_acc * 100)\n",
        "            test_accs.append(test_acc * 100)\n",
        "\n",
        "        dev_accs = np.stack(dev_accs)\n",
        "        test_accs = np.stack(test_accs)\n",
        "\n",
        "        dev_acc, dev_std = dev_accs.mean(), dev_accs.std()\n",
        "        test_acc, test_std = test_accs.mean(), test_accs.std()\n",
        "\n",
        "        print('** [{}] [Epoch: {}] Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(self.args.embedder, epoch, dev_acc, dev_std, test_acc, test_std))\n",
        "\n",
        "        if dev_acc > self.best_dev_acc:\n",
        "            self.best_dev_acc = dev_acc\n",
        "            self.best_test_acc = test_acc\n",
        "            self.best_dev_std = dev_std\n",
        "            self.best_test_std = test_std\n",
        "            self.best_epoch = epoch\n",
        "\n",
        "        self.best_dev_accs.append(self.best_dev_acc)\n",
        "        self.st_best = '** [Best epoch: {}] Best val | Best test: {:.4f} ({:.4f}) / {:.4f} ({:.4f})**\\n'.format(\n",
        "            self.best_epoch, self.best_dev_acc, self.best_dev_std, self.best_test_acc, self.best_test_std)\n",
        "        print(self.st_best)\n",
        "\n",
        "\n",
        "    def evaluate_clustering(self, epoch,sillog):\n",
        "        embeddings = F.normalize(self._embeddings, dim = -1, p = 2).detach().cpu().numpy()\n",
        "\n",
        "        nb_class = len(self._dataset[0].y.unique())\n",
        "        true_y = self._dataset[0].y.detach().cpu().numpy()\n",
        "\n",
        "        estimator = KMeans(n_clusters = nb_class)\n",
        "\n",
        "        NMI_list = []\n",
        "\n",
        "        for i in range(10):\n",
        "            estimator.fit(embeddings)\n",
        "            y_pred = estimator.predict(embeddings)\n",
        "            s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')\n",
        "            NMI_list.append(s1)\n",
        "        estimator.fit(embeddings)\n",
        "        y_pred = estimator.predict(embeddings)\n",
        "        silhid = metrics.silhouette_score(self._embeddings.detach().cpu().numpy(), y_pred, metric='euclidean')\n",
        "        s1 = sum(NMI_list) / len(NMI_list)\n",
        "        sillog.append(silhid)\n",
        "        arr_sil = np.array(sillog)\n",
        "\n",
        "        print('** [{}] [Current Epoch {}] this epoch NMI values: {:.4f} ** and this epoch sil values: {}'.format(self.args.embedder, epoch, s1,silhid))\n",
        "\n",
        "        if math.floor(silhid*100) >= math.floor(self.best_dev_acc*100):\n",
        "\n",
        "            self.best_dev_acc = round(silhid, 2)\n",
        "            self.best_embeddings = embeddings\n",
        "            self.best_test_acc = s1\n",
        "            print(\"~~~~~~~~~~~~~~~~~~\")\n",
        "            print(silhid)\n",
        "            if self._args.checkpoint_dir != '':\n",
        "                print('Saving checkpoint...')\n",
        "                torch.save(embeddings, os.path.join(self._args.checkpoint_dir,\n",
        "                                                    'embeddings_{}_{}.pt'.format(self._args.dataset, self._args.task)))\n",
        "                # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)\n",
        "                a = pd.DataFrame(self.best_embeddings).T\n",
        "                a.to_csv(\"/content/results/scGCL-Tosches_turtle-euclidean.csv\")\n",
        "            print(\"save\")\n",
        "            print(\"~~~~~~~~~~~~~~~~~~\")\n",
        "\n",
        "\n",
        "    def run_similarity_search(self, epoch):\n",
        "        test_embs = self._embeddings.detach().cpu().numpy()\n",
        "        test_lbls = self._dataset[0].y.detach().cpu().numpy()\n",
        "        numRows = test_embs.shape[0]\n",
        "\n",
        "        cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)\n",
        "        st = []\n",
        "        for N in [5, 10]:\n",
        "            indices = np.argsort(cos_sim_array, axis=1)[:, -N:]\n",
        "            tmp = np.tile(test_lbls, (numRows, 1))\n",
        "            selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)\n",
        "            original_label = np.repeat(test_lbls, N).reshape(numRows,N)\n",
        "            st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))\n",
        "\n",
        "        print(\"** [{}] [Current Epoch {}] sim@5 : {} | sim@10 : {} **\".format(self.args.embedder, epoch, st[0], st[1]))\n",
        "\n",
        "        if st[0] > self.best_dev_acc:\n",
        "            self.best_dev_acc = st[0]\n",
        "            self.best_test_acc = st[1]\n",
        "            self.best_epoch = epoch\n",
        "\n",
        "        self.best_dev_accs.append(self.best_dev_acc)\n",
        "        self.st_best = '** [Best epoch: {}] Best @5 : {} | Best @10: {} **\\n'.format(self.best_epoch, self.best_dev_acc, self.best_test_acc)\n",
        "        print(self.st_best)\n",
        "\n",
        "        return st\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines an encoder module for the graph neural network\n",
        "    param: layer_config (list): Configuration of layers.\n",
        "    param: dropout (float): Dropout rate.\n",
        "    param: project (bool): Whether to project.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_config, dropout=None, project=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.stacked_gnn = nn.ModuleList([GCNConv(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])\n",
        "        # self.stacked_en = nn.ModuleList(\n",
        "        #     [nn.Linear(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])\n",
        "        self.stacked_bns = nn.ModuleList([nn.BatchNorm1d(layer_config[i], momentum=0.01) for i in range(1, len(layer_config))])\n",
        "        self.stacked_prelus = nn.ModuleList([nn.ReLU() for _ in range(1, len(layer_config))])\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        for i, gnn in enumerate(self.stacked_gnn):\n",
        "            x = gnn(x, edge_index, edge_weight=edge_weight)\n",
        "            #x = gnn(x)\n",
        "            x = self.stacked_bns[i](x)\n",
        "            x = self.stacked_prelus[i](x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI8a96_bORbk",
        "outputId": "887e7e8a-f7ac-49be-d669-58075c1a4baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 15.6 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# scGCL Model\n",
        "# Revised freom Original version in AFGRL\n",
        "# Ref:\n",
        "# https://github.com/Namkyeong/AFGRL/tree/master/models/AFGRL.py\n",
        "\n",
        "class AFGRL_ModelTrainer(embedder):\n",
        "    \"\"\"\n",
        "    This class initializes an Embedder object and implements training functionality for AFGRL\n",
        "    param: args: args object that defines the parameters of the embeddings model\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        embedder.__init__(self, args)\n",
        "        self._args = args\n",
        "        self._init()\n",
        "        self.config_str = config2string(args)\n",
        "        print(\"\\n[Config] {}\\n\".format(self.config_str))\n",
        "        self.writer = SummaryWriter(log_dir=\"runs/{}\".format(self.config_str))\n",
        "\n",
        "    def _init(self):\n",
        "        args = self._args\n",
        "        self._task = args.task\n",
        "        print(\"Downstream Task : {}\".format(self._task))\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)\n",
        "        # self._device = f'cuda:{args.device}' if torch.cuda.is_available() else \"cpu\"\n",
        "        self._device = \"cpu\"\n",
        "        # torch.cuda.set_device(self._device)\n",
        "        self._dataset = Dataset(root=args.root, dataset=args.dataset)\n",
        "        self._loader = DataLoader(dataset=self._dataset)\n",
        "        layers = [self._dataset.data.x.shape[1]] + self.hidden_layers\n",
        "        self._model = AFGRL(layers, args).to(self._device)\n",
        "        self._optimizer = optim.AdamW(params=self._model.parameters(), lr=args.lr, weight_decay= 1e-5)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.best_test_acc, self.best_dev_acc, self.best_test_std, self.best_dev_std, self.best_epoch = 0, 0, 0, 0, 0\n",
        "        self.best_dev_accs = []\n",
        "        self.best_embeddings = None\n",
        "        sillog = []\n",
        "        # get Random Initial accuracy\n",
        "        self.infer_embeddings(0)\n",
        "        print(\"initial accuracy \")\n",
        "        self.evaluate(self._task, 0, sillog)\n",
        "\n",
        "        f_final = open(\"/content/results/{}.txt\".format(self._args.embedder), \"a\")\n",
        "\n",
        "        # Start Model Training\n",
        "        print(\"Training Start!\")\n",
        "        self._model.train()\n",
        "        for epoch in range(self._args.epochs):\n",
        "            for bc, batch_data in enumerate(self._loader):\n",
        "                batch_data.to(self._device)\n",
        "                # view1, view2 = augmentation._feature_masking(batch_data, self._device)\n",
        "\n",
        "                emb, loss = self._model(x=batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,\n",
        "                                     neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],\n",
        "                                     edge_weight=batch_data.edge_attr, epoch=epoch)\n",
        "\n",
        "                self._optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self._optimizer.step()\n",
        "                self._model.update_moving_average()\n",
        "\n",
        "                st = '[{}][Epoch {}/{}] Loss: {:.4f}'.format(currentTime(), epoch, self._args.epochs, loss.item())\n",
        "                print(st)\n",
        "\n",
        "            if (epoch) % 5 == 0:\n",
        "                self.infer_embeddings(epoch)\n",
        "                # self.evaluate(self._task, epoch)\n",
        "                self.evaluate(self._task, epoch, sillog)\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\nTraining Done!\")\n",
        "        self.st_best = '** [last epoch: {}] last NMI: {:.4f} **\\n'.format(self._args.epochs, self.best_test_acc)\n",
        "        print(\"[Final] {}\".format(self.st_best))\n",
        "        print('Saving checkpoint...')\n",
        "        torch.save(self.best_embeddings, os.path.join(self._args.checkpoint_dir,\n",
        "                                            'embeddings_{}_{}.pt'.format(self._args.dataset,\n",
        "                                                                         self._args.task)))\n",
        "        # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)\n",
        "        a = pd.DataFrame(self.best_embeddings).T\n",
        "        a.to_csv(\"/content/results/scGCL-Tosches_turtle-euclidean.csv\")\n",
        "        f_final.write(\"{} -> {}\\n\".format(self.config_str, self.st_best))\n",
        "\n",
        "class AFGRL(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the AFGRL model architecture and operations\n",
        "    param: layer_config (list): Configuration of layers\n",
        "    param: args: args object that defines the parameters of the embeddings model\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_config, args, **kwargs):\n",
        "        super().__init__()\n",
        "        dec_dim = [512, 256]\n",
        "        self.student_encoder = Encoder(layer_config=layer_config, dropout=args.dropout, **kwargs)\n",
        "        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n",
        "        set_requires_grad(self.teacher_encoder, False)\n",
        "        self.teacher_ema_updater = EMA(args.mad, args.epochs)\n",
        "        self.neighbor = Neighbor(args)\n",
        "        rep_dim = layer_config[-1]\n",
        "        rep_dim_o = layer_config[0]\n",
        "        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, args.pred_hid), nn.BatchNorm1d(args.pred_hid), nn.ReLU(), nn.Linear(args.pred_hid, rep_dim), nn.ReLU())\n",
        "        self.ZINB_Encoder = nn.Sequential(nn.Linear(rep_dim, dec_dim[0]), nn.ReLU(),\n",
        "                                          nn.Linear(dec_dim[0], dec_dim[1]), nn.ReLU())\n",
        "        self.pi_Encoder =  nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o),nn.Sigmoid())\n",
        "        self.disp_Encoder = nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o), nn.Softplus())\n",
        "        self.mean_Encoder = nn.Linear(dec_dim[1], rep_dim_o)\n",
        "        self.student_predictor.apply(init_weights)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.topk = args.topk\n",
        "        # self._device = args.device\n",
        "        self._device = \"cpu\"\n",
        "    def clip_by_tensor(self,t, t_min, t_max):\n",
        "        \"\"\"\n",
        "        clip_by_tensor\n",
        "        :param t: tensor\n",
        "        :param t_min: min\n",
        "        :param t_max: max\n",
        "        :return: cliped tensor\n",
        "        \"\"\"\n",
        "        t = torch.tensor(t,dtype = torch.float32)\n",
        "        t_min = torch.tensor(t_min,dtype = torch.float32)\n",
        "        t_max = torch.tensor(t_max,dtype = torch.float32)\n",
        "\n",
        "        result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n",
        "        result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n",
        "        return result\n",
        "\n",
        "    def reset_moving_average(self):\n",
        "        del self.teacher_encoder\n",
        "        self.teacher_encoder = None\n",
        "\n",
        "    def update_moving_average(self):\n",
        "        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n",
        "        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n",
        "\n",
        "    def forward(self, x, y, edge_index, neighbor, edge_weight=None, epoch=None):\n",
        "        student = self.student_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
        "        # student_ = self.student_encoder(x=x2, edge_index=edge_index_2, edge_weight=edge_weight_2)\n",
        "        pred = self.student_predictor(student)\n",
        "        # pred_ = self.student_predictor(student_)\n",
        "        z = self.ZINB_Encoder(student)\n",
        "        pi = self.pi_Encoder(z)\n",
        "        disp = self.disp_Encoder(z)\n",
        "        disp = self.clip_by_tensor(disp,1e-4,1e4)\n",
        "        mean = self.mean_Encoder(z)\n",
        "        mean = self.clip_by_tensor(torch.exp(mean),1e-5,1e6)\n",
        "        modify = 0\n",
        "        with torch.no_grad():\n",
        "            teacher = self.teacher_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
        "        if edge_weight == None:\n",
        "            adj = torch.sparse.FloatTensor(neighbor[0], torch.ones_like(neighbor[0][0]), [x.shape[0], x.shape[0]])\n",
        "        else:\n",
        "            adj = torch.sparse.FloatTensor(neighbor[0], neighbor[1], [x.shape[0], x.shape[0]])\n",
        "        #\n",
        "        ind, k = self.neighbor(adj, F.normalize(student, dim=-1, p=2), F.normalize(teacher, dim=-1, p=2), self.topk, epoch)\n",
        "        zinb = ZINB(pi, theta=disp, ridge_lambda=0, debug=False)\n",
        "        zinb_loss = zinb.loss(x, mean, mean=True)\n",
        "\n",
        "        loss1 = loss_fn(pred[ind[0]], teacher[ind[1]].detach())\n",
        "        loss2 = loss_fn(pred[ind[1]], teacher[ind[0]].detach())\n",
        "        recon_loss = torch.nn.MSELoss(reduction='mean')\n",
        "        recon_loss_ = recon_loss(x,student)\n",
        "        # adj_recon_ = recon_loss(adj.to_dense(),adj_recon)\n",
        "        loss_reforce = (loss1 + loss2)\n",
        "        if modify == 0:\n",
        "            loss = zinb_loss + loss_reforce + recon_loss_\n",
        "        elif modify == 1:\n",
        "            loss = loss_reforce + recon_loss_\n",
        "        elif modify == 2:\n",
        "            loss = zinb_loss\n",
        "        return student, loss.mean()\n",
        "\n",
        "class Neighbor(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the Neighbor module for AFGRL\n",
        "    param: args: args object that defines the parameters of the embeddings model\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        super(Neighbor, self).__init__()\n",
        "        # self.device = args.device\n",
        "        self.device = \"cpu\"\n",
        "        self.num_centroids = args.num_centroids\n",
        "        self.num_kmeans = args.num_kmeans\n",
        "        self.clus_num_iters = args.clus_num_iters\n",
        "\n",
        "    def __get_close_nei_in_back(self, indices, each_k_idx, cluster_labels, back_nei_idxs, k):\n",
        "        # get which neighbors are close in the background set\n",
        "        batch_labels = cluster_labels[each_k_idx][indices]\n",
        "        top_cluster_labels = cluster_labels[each_k_idx][back_nei_idxs]\n",
        "        batch_labels = repeat_1d_tensor(batch_labels, k)\n",
        "\n",
        "        curr_close_nei = torch.eq(batch_labels, top_cluster_labels)\n",
        "        return curr_close_nei\n",
        "\n",
        "    def forward(self, adj, student, teacher, top_k, epoch):\n",
        "        n_data, d = student.shape\n",
        "        similarity = torch.matmul(student, torch.transpose(teacher, 1, 0).detach())\n",
        "        similarity += torch.eye(n_data, device=self.device) * 10\n",
        "\n",
        "        _, I_knn = similarity.topk(k=top_k, dim=1, largest=True, sorted=True)\n",
        "        tmp = torch.LongTensor(np.arange(n_data)).unsqueeze(-1).to(self.device)\n",
        "\n",
        "        knn_neighbor = self.create_sparse(I_knn)\n",
        "        locality = knn_neighbor * adj\n",
        "\n",
        "        ncentroids = self.num_centroids\n",
        "        niter = self.clus_num_iters\n",
        "\n",
        "        pred_labels = []\n",
        "        # d_means = []\n",
        "        for seed in range(self.num_kmeans):\n",
        "            kmeans = faiss.Kmeans(d, ncentroids, niter=niter, gpu=False, seed=seed + 1234)\n",
        "            kmeans.train(teacher.cpu().numpy())\n",
        "            _, I_kmeans = kmeans.index.search(teacher.cpu().numpy(), 1)\n",
        "\n",
        "            clust_labels = I_kmeans[:,0]\n",
        "            # d_means.append(D_kmeans)\n",
        "            pred_labels.append(clust_labels)\n",
        "        pred_labels = np.stack(pred_labels, axis=0)\n",
        "        cluster_labels = torch.from_numpy(pred_labels).float()\n",
        "\n",
        "        all_close_nei_in_back = None\n",
        "        with torch.no_grad():\n",
        "            for each_k_idx in range(self.num_kmeans):\n",
        "                curr_close_nei = self.__get_close_nei_in_back(tmp.squeeze(-1), each_k_idx, cluster_labels, I_knn, I_knn.shape[1])\n",
        "\n",
        "                if all_close_nei_in_back is None:\n",
        "                    all_close_nei_in_back = curr_close_nei\n",
        "                else:\n",
        "                    all_close_nei_in_back = all_close_nei_in_back | curr_close_nei\n",
        "\n",
        "        all_close_nei_in_back = all_close_nei_in_back.to(self.device)\n",
        "\n",
        "        globality = self.create_sparse_revised(I_knn, all_close_nei_in_back)\n",
        "\n",
        "        pos_ = locality + globality\n",
        "\n",
        "        return pos_.coalesce()._indices(), I_knn.shape[1]\n",
        "\n",
        "    def create_sparse(self, I):\n",
        "\n",
        "        similar = I.reshape(-1).tolist()\n",
        "        index = np.repeat(range(I.shape[0]), I.shape[1])\n",
        "\n",
        "        assert len(similar) == len(index)\n",
        "        indices = torch.tensor([index, similar],dtype=torch.int32).to(self.device)\n",
        "        result = torch.sparse_coo_tensor(indices, torch.ones_like(I.reshape(-1)), [I.shape[0], I.shape[0]])\n",
        "\n",
        "        return result\n",
        "\n",
        "    def create_sparse_revised(self, I, all_close_nei_in_back):\n",
        "        n_data, k = I.shape[0], I.shape[1]\n",
        "\n",
        "        index = []\n",
        "        similar = []\n",
        "        for j in range(I.shape[0]):\n",
        "            for i in range(k):\n",
        "                index.append(int(j))\n",
        "                similar.append(I[j][i].item())\n",
        "\n",
        "        index = torch.masked_select(torch.LongTensor(index).to(self.device), all_close_nei_in_back.reshape(-1))\n",
        "        similar = torch.masked_select(torch.LongTensor(similar).to(self.device), all_close_nei_in_back.reshape(-1))\n",
        "\n",
        "        assert len(similar) == len(index)\n",
        "        indices = torch.tensor([index.cpu().numpy().tolist(), similar.cpu().numpy().tolist()]).to(self.device)\n",
        "        result = torch.sparse_coo_tensor(indices, torch.ones(len(index)).to(self.device), [n_data, n_data])\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77f15u4oKZO-",
        "outputId": "a159d606-6459-4f4a-e136-04ff8e9f7139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 17.7 ms (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "#ZINB.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Convert NaN values to zero in a tensor\n",
        "param: x (torch.Tensor): Input tensor\n",
        "return: torch.Tensor: Tensor with NaN values replaced by zeros\n",
        "\"\"\"\n",
        "def _nan2zero(x):\n",
        "    return torch.where(torch.isnan(x), torch.zeros_like(x), x)\n",
        "\n",
        "\"\"\"\n",
        "Convert NaN values to inf in a tensor\n",
        "param: x (torch.Tensor): Input tensor\n",
        "return: torch.Tensor: Tensor with NaN values replaced by inf\n",
        "\"\"\"\n",
        "def _nan2inf(x):\n",
        "    return torch.where(torch.isnan(x), torch.zeros_like(x)+np.inf, x)\n",
        "\n",
        "\"\"\"\n",
        "Count the number of non-NaN elements in a tensor\n",
        "param: x (torch.Tensor): Input tensor\n",
        "return: torch.Tensor: Number of non-NaN elements\n",
        "\"\"\"\n",
        "def _nelem(x):\n",
        "    nelem = torch.sum(torch.tensor(~torch.isnan(x),dtype = torch.float32))\n",
        "    return torch.tensor(torch.where(torch.equal(nelem, 0.), 1., nelem), dtype = x.dtype)\n",
        "\n",
        "\"\"\"\n",
        "Calculate the mean of non-NaN elements in a tensor\n",
        "param: x (torch.Tensor): Input tensor\n",
        "return: torch.Tensor: Mean value of non-NaN elements\n",
        "\"\"\"\n",
        "def _reduce_mean(x):\n",
        "    nelem = _nelem(x)\n",
        "    x = _nan2zero(x)\n",
        "    return torch.divide(torch.sum(x), nelem)\n",
        "\n",
        "\"\"\"\n",
        "Calculate the Mean Squared Error (MSE) loss\n",
        "param: y_true (torch.Tensor): True values tensor\n",
        "param: y_pred (torch.Tensor): Predicted values tensor\n",
        "return: torch.Tensor: MSE loss\n",
        "\"\"\"\n",
        "def mse_loss(y_true, y_pred):\n",
        "    ret = torch.square(y_pred - y_true)\n",
        "\n",
        "    return _reduce_mean(ret)\n",
        "\n",
        "\"\"\"\n",
        "Calculate the Poisson loss\n",
        "param: y_true (torch.Tensor): True values tensor\n",
        "param: y_pred (torch.Tensor): Predicted values tensor\n",
        "return: torch.Tensor: Poisson loss\n",
        "\"\"\"\n",
        "def poisson_loss(y_true, y_pred):\n",
        "    y_pred = torch.tensor(y_pred, dtype = torch.float32)\n",
        "    y_true = torch.tensor(y_true, dtype = torch.float32)\n",
        "    nelem = _nelem(y_true)\n",
        "    y_true = _nan2zero(y_true)\n",
        "    ret = y_pred - y_true*torch.log(y_pred+1e-10) + torch.lgamma(y_true+1.0)\n",
        "\n",
        "    return torch.divide(torch.sum(ret), nelem)\n",
        "\n",
        "\"\"\"\n",
        "Defines the Negative Binomial (NB) loss class\n",
        "param: theta (float): Theta parameter for NB distribution\n",
        "param: masking (bool): Flag for masking NaN values\n",
        "param: scope (str): Scope for loss\n",
        "param: scale_factor (float): Scaling factor for loss\n",
        "param: debug (bool): Debug flag\n",
        "\"\"\"\n",
        "class NB(object):\n",
        "    def __init__(self, theta=None, masking=False, scope='nbinom_loss/',\n",
        "                 scale_factor=1.0, debug=False):\n",
        "\n",
        "        # for numerical stability\n",
        "        self.eps = 1e-10\n",
        "        self.scale_factor = scale_factor\n",
        "        self.debug = debug\n",
        "        self.scope = scope\n",
        "        self.masking = masking\n",
        "        self.theta = theta\n",
        "\n",
        "    def loss(self, y_true, y_pred, mean=True):\n",
        "        scale_factor = self.scale_factor\n",
        "        eps = self.eps\n",
        "\n",
        "        y_true = torch.tensor(y_true, dtype = torch.float32)\n",
        "        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n",
        "\n",
        "        if self.masking:\n",
        "            nelem = _nelem(y_true)\n",
        "            y_true = _nan2zero(y_true)\n",
        "\n",
        "            # Clip theta\n",
        "        theta = torch.minimum(self.theta,torch.tensor(1e6))\n",
        "\n",
        "        t1 = torch.lgamma(theta+eps) + torch.lgamma(y_true+1.0) - torch.lgamma(y_true+theta+eps)\n",
        "        t2 = (theta+y_true) * torch.log(1.0 + (y_pred/(theta+eps))) + (y_true * (torch.log(theta+eps) - torch.log(y_pred+eps)))\n",
        "\n",
        "\n",
        "        final = t1 + t2\n",
        "\n",
        "        final = _nan2inf(final)\n",
        "\n",
        "        if mean:\n",
        "            if self.masking:\n",
        "                final = torch.divide(torch.sum(final), nelem)\n",
        "            else:\n",
        "                final = torch.sum(final)\n",
        "\n",
        "\n",
        "        return final\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Defines the Zero-Inflated Negative Binomial (ZINB) loss class according the formula in the paper\n",
        "param: pi (torch.Tensor): Probability tensor for zero inflation.\n",
        "param: ridge_lambda (float): Ridge lambda parameter.\n",
        "param: scope (str): Scope for loss.\n",
        "\"\"\"\n",
        "class ZINB(NB):\n",
        "    def __init__(self, pi, ridge_lambda=0.0, scope='zinb_loss/', **kwargs):\n",
        "        super().__init__(scope=scope, **kwargs)\n",
        "        self.pi = pi\n",
        "        self.ridge_lambda = ridge_lambda\n",
        "\n",
        "    def loss(self, y_true, y_pred, mean=True):\n",
        "        scale_factor = self.scale_factor\n",
        "        eps = self.eps\n",
        "\n",
        "\n",
        "            # reuse existing NB neg.log.lik.\n",
        "            # mean is always False here, because everything is calculated\n",
        "            # element-wise. we take the mean only in the end\n",
        "        nb_case = super().loss(y_true, y_pred, mean=False) - torch.log(1.0-self.pi+eps)\n",
        "\n",
        "        y_true = torch.tensor(y_true, dtype = torch.float32)\n",
        "        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n",
        "        theta = torch.minimum(self.theta,torch.tensor(1e6))\n",
        "\n",
        "        zero_nb = torch.pow(theta/(theta+y_pred+eps), theta)\n",
        "        zero_case = -torch.log(self.pi + ((1.0-self.pi)*zero_nb)+eps)\n",
        "        result = torch.where(torch.less(y_true, 1e-8), zero_case, nb_case)\n",
        "        ridge = self.ridge_lambda*torch.square(self.pi)\n",
        "        result += ridge\n",
        "\n",
        "        if mean:\n",
        "            if self.masking:\n",
        "                result = _reduce_mean(result)\n",
        "            else:\n",
        "                result = torch.sum(result)\n",
        "\n",
        "        result = _nan2inf(result)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_cGXNgjM6Ie"
      },
      "source": [
        "## (5) Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "### (5.1) Hyperparameter Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GjEyLBIOXZw"
      },
      "source": [
        "Hyper-parameters we used and their values:\n",
        "\n",
        "*   Learning Rate: lr=0.001 (10^-3)\n",
        "*   k-value (For K-NN method): k=15\n",
        "*   Distance measurement algorithm: 'cosine'\n",
        "\n",
        "**Hyper-parameter analysis:**\n",
        "\n",
        "For hyperparameter analysis, the k and distances measures are used as metrics for comparison. In this experiment, only the baseline method scTAG uses the K-NN method to construct cell graphs. The distance measures were set to ‘cityblock’, ‘cosine’, ‘euclidean’, ‘L2’, and ‘manhattan’ while the k values were set to 5, 10, 15, 20, and 25.\n",
        "\n",
        "From the results, the ‘cosine’ distance measure had both the highest ARI and NMI for the Adam dataset. This means it captures the distance information/relationships between cells most effectively, which is explained due to the high sparsity of scRNA-seq data. Out of the k values, the ARI and NMI values are highest when k is set to 15. Additionally, scGCL always performs better than the baseline method scTAG across all distance measures and k values.\n",
        "\n",
        "Table 1. Imputation results by scGCL with different distance measures\n",
        "![picture](https://drive.google.com/uc?id=1Sx2FwHMp2MWKr4YJLoMveGPhmO-9ZcpR)\n",
        "\n",
        "Table 2. Imputation results by scTAG with different distance measures\n",
        "![picture](https://drive.google.com/uc?id=1k-v0YVeIG_slf9Y1YqATMvGxysHomtkK)\n",
        "\n",
        "\n",
        "Table 3. Imputation results by scGCL with different k values\n",
        "![picture](https://drive.google.com/uc?id=1jK3pvQCk_H9HhePlQ9GlaY-m6K7ydUIr)\n",
        "\n",
        "\n",
        "Table 4. Imputation results by scTAG with different k values\n",
        "![picture](https://drive.google.com/uc?id=1gxihue0XLR4e-mBvAoMupg8QSl0QzDni)\n",
        "\n",
        "\n",
        "Average imputation results across all datasets\n",
        "![picture](https://drive.google.com/uc?id=12GAOU96VDpt8Idiz5sSPfgwPP0Zfzcp8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LREeENsZUmjn"
      },
      "source": [
        "### (5.2) Computational requirements\n",
        "\n",
        "\n",
        "\n",
        "1.   Num of epochs used for training: 300\n",
        "2.   Avg. run time of one epoch: ~24 seconds per epoch\n",
        "3.   Total runtime for 300 epochs: 1 hour 59 minutes 46 seconds\n",
        "4.   Type of hardware used: GPU (on google colab)\n",
        "\n",
        "\n",
        "\n",
        "For efficiency purposes, we have pre-trained the model and stored the checkpoint to use in evaluation. Executing the code below will train the model for 1 epoch for demonstration purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BapPqIue-WzR",
        "outputId": "5ec7f26a-85aa-4ff5-8eec-69f0c3339c52"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['embedder', 'dataset', 'checkpoint_dir', 'root', 'task', 'layers', 'pred_hid', 'topk', 'clus_num_iters', 'num_centroids', 'num_kmeans', 'eval_freq', 'mad', 'lr', 'es', 'device', 'epochs', 'dropout', 'aug_params']\n",
            "['AFGRL', 'adam', '/content/model_checkpoints', 'data', 'clustering', '[2048]', 2048, 4, 20, 9, 5, 5, 0.9, 0.001, 300, 0, 1, 0.0, [0.3, 0.4, 0.3, 0.2]]\n",
            "Downstream Task : clustering\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "<ipython-input-10-91fdd227d2b5>:42: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/pyg/Adam/processed/data.pt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing full batch data\n",
            "\n",
            "[Config] dataset_adam\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-dac09814067f>:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t = torch.tensor(t,dtype = torch.float32)\n",
            "<ipython-input-15-dac09814067f>:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n",
            "<ipython-input-15-dac09814067f>:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n",
            "<ipython-input-15-dac09814067f>:159: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
            "  adj = torch.sparse.FloatTensor(neighbor[0], neighbor[1], [x.shape[0], x.shape[0]])\n",
            "<ipython-input-16-77586843ab3e>:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_true = torch.tensor(y_true, dtype = torch.float32)\n",
            "<ipython-input-16-77586843ab3e>:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n",
            "<ipython-input-16-77586843ab3e>:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_true = torch.tensor(y_true, dtype = torch.float32)\n",
            "<ipython-input-16-77586843ab3e>:141: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial accuracy \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "** [AFGRL] [Current Epoch 0] this epoch NMI values: 0.7946 ** and this epoch sil values: 0.26154962182044983\n",
            "~~~~~~~~~~~~~~~~~~\n",
            "0.26154962\n",
            "Saving checkpoint...\n",
            "save\n",
            "~~~~~~~~~~~~~~~~~~\n",
            "Training Start!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-dac09814067f>:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t = torch.tensor(t,dtype = torch.float32)\n",
            "<ipython-input-15-dac09814067f>:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n",
            "<ipython-input-15-dac09814067f>:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n",
            "<ipython-input-16-77586843ab3e>:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_true = torch.tensor(y_true, dtype = torch.float32)\n",
            "<ipython-input-16-77586843ab3e>:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n",
            "<ipython-input-16-77586843ab3e>:140: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_true = torch.tensor(y_true, dtype = torch.float32)\n",
            "<ipython-input-16-77586843ab3e>:141: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-05-01 20:01:09][Epoch 0/1] Loss: 3031691.5000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** [AFGRL] [Current Epoch 0] this epoch NMI values: 0.7982 ** and this epoch sil values: 0.27419140934944153\n",
            "~~~~~~~~~~~~~~~~~~\n",
            "0.2741914\n",
            "Saving checkpoint...\n",
            "save\n",
            "~~~~~~~~~~~~~~~~~~\n",
            "\n",
            "Training Done!\n",
            "[Final] ** [last epoch: 1] last NMI: 0.7982 **\n",
            "\n",
            "Saving checkpoint...\n",
            "time: 3min 41s (started: 2024-05-01 19:58:54 +00:00)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\"\"\"\n",
        "Main function to parse arguments and start model training.\n",
        "\"\"\"\n",
        "def main():\n",
        "    args, unknown = parse_args()\n",
        "\n",
        "    if args.embedder == 'AFGRL':\n",
        "        embedder = AFGRL_ModelTrainer(args)\n",
        "\n",
        "    embedder.train()\n",
        "    embedder.writer.close()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function that performs the imputation and saves the result to a CSV file\n",
        "param: file_path (str): Path to the file containing imputation data\n",
        "return: file stored in the specified path that saves the result of the imputation\n",
        "\"\"\"\n",
        "def imputation(file_path):\n",
        "    args, unknown = utils.parse_args()\n",
        "    imputation_m = torch.load(file_path).detach().data.cpu().numpy()\n",
        "    a = pd.DataFrame(imputation_m).T\n",
        "    a.to_csv(\"/content/results/AFGRL-imputed-\" + args.dataset + \".csv\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIL6kpRQUzO7"
      },
      "source": [
        "## (6) Evaluation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA43LNBkvkGl"
      },
      "source": [
        "The two metrics used to evaluate the performance of scGCL are adjusted rand index (ARI) and normalized mutual information (NMI.) These numbers are compared against four “state-of-the-art” baseline imputation methods: GraphSCI, scTAG, AutoClass, and MAGIC. The clustering results of scGCL on the Adam dataset outperforms all four of these methods, at 0.91 ARI and 0.89 NMI, respectively (from original paper).\n",
        "\n",
        "From the paper itself including metrics from the other 13 datasets, the average ARI and NMI of scGCL across all datasets are 0.82 and 0.80 with the second best values of 0.80 and 0.75.\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1A_NayMELO5dLzEDAFJplTy9GWiwT7pmW)\n",
        "Figure a. scGCL ARI results compared to other baseline methods\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1ObVpExjWopnaCyVsjdI79WyZOdG07GLB)\n",
        "Figure b. scGCLl NMI compared to other baseline methods\n",
        "\n",
        "\n",
        "\n",
        "You can run our evaluation code below. This code uses the pre-trained model stored in the 'model_checkpoints' folder\n",
        "\n",
        "Clustering NMI is the same as the NMI score.\n",
        "Homogeneity score is the same as the ARI score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DI4V0UK-m31",
        "outputId": "345620f6-62a6-4e8c-a34f-8f5152a8f2a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.76 ms (started: 2024-05-01 20:02:35 +00:00)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "random.seed(0)\n",
        "\n",
        "# scGCL Model\n",
        "# Revised freom Original version in AFGRL\n",
        "# Ref:\n",
        "# https://github.com/Namkyeong/AFGRL/tree/master/evaluate.py\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Evaluate node classification based on embeddings\n",
        "prints out node classification results for validation and test sets\n",
        "param: embeddings (tensor): Embeddings to evaluate\n",
        "param: dataset (Dataset): Dataset containing node labels and masks\n",
        "param: name (str): Name of the dataset\n",
        "\"\"\"\n",
        "def evaluate_node(embeddings, dataset, name):\n",
        "\n",
        "    labels = dataset.y\n",
        "    emb_dim, num_class = embeddings.shape[1], dataset.y.unique().shape[0]\n",
        "\n",
        "    dev_accs, test_accs = [], []\n",
        "\n",
        "    for i in range(20):\n",
        "\n",
        "        train_mask = dataset.train_mask[i]\n",
        "        dev_mask = dataset.val_mask[i]\n",
        "        if name == \"wikics\":\n",
        "            test_mask = dataset.test_mask\n",
        "        else:\n",
        "            test_mask = dataset.test_mask[i]\n",
        "\n",
        "        classifier = LogisticRegression(emb_dim, num_class)\n",
        "        optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "\n",
        "        for _ in range(100):\n",
        "            classifier.train()\n",
        "            logits, loss = classifier(embeddings[train_mask], labels[train_mask])\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        dev_logits, _ = classifier(embeddings[dev_mask], labels[dev_mask])\n",
        "        test_logits, _ = classifier(embeddings[test_mask], labels[test_mask])\n",
        "        dev_preds = torch.argmax(dev_logits, dim=1)\n",
        "        test_preds = torch.argmax(test_logits, dim=1)\n",
        "\n",
        "        dev_acc = (torch.sum(dev_preds == labels[dev_mask]).float() /\n",
        "                       labels[dev_mask].shape[0]).detach().cpu().numpy()\n",
        "        test_acc = (torch.sum(test_preds == labels[test_mask]).float() /\n",
        "                        labels[test_mask].shape[0]).detach().cpu().numpy()\n",
        "\n",
        "        dev_accs.append(dev_acc * 100)\n",
        "        test_accs.append(test_acc * 100)\n",
        "\n",
        "    dev_accs = np.stack(dev_accs)\n",
        "    test_accs = np.stack(test_accs)\n",
        "\n",
        "    dev_acc, dev_std = dev_accs.mean(), dev_accs.std()\n",
        "    test_acc, test_std = test_accs.mean(), test_accs.std()\n",
        "\n",
        "    print('Evaluate node classification results')\n",
        "    print('** Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(dev_acc, dev_std, test_acc, test_std))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Evaluate clustering and return NMI and ARI scores\n",
        "param: embeddings (tensor): Embeddings to evaluate.\n",
        "param: dataset (Dataset): Dataset containing true labels.\n",
        "print: clustering NMI score\n",
        "print: clustering homogeniety(ARI) score\n",
        "\"\"\"\n",
        "def evaluate_clustering(embeddings, dataset):\n",
        "    x, edge_index, y = dataset[0]\n",
        "    embeddings = F.normalize(torch.from_numpy(embeddings), dim = -1, p = 2).detach().cpu().numpy()\n",
        "    nb_class = len(y[1].unique())\n",
        "    true_y = y[1].detach().cpu().numpy()\n",
        "\n",
        "    estimator = KMeans(n_clusters = nb_class)\n",
        "\n",
        "    NMI_list = []\n",
        "    h_list = []\n",
        "\n",
        "    for i in range(10):\n",
        "        estimator.fit(embeddings)\n",
        "        y_pred = estimator.predict(embeddings)\n",
        "\n",
        "        h_score = metrics.homogeneity_score(true_y, y_pred)\n",
        "        s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')\n",
        "        NMI_list.append(s1)\n",
        "        h_list.append(h_score)\n",
        "\n",
        "    s1 = sum(NMI_list) / len(NMI_list)\n",
        "    h_score = sum(h_list) / len(h_list)\n",
        "    print('Evaluate clustering results')\n",
        "    print('** Clustering NMI: {:.4f} | homogeneity score: {:.4f} **'.format(s1, h_score))\n",
        "\n",
        "\"\"\"\n",
        "Run similarity search based on embeddings\n",
        "prints out cosine similarity scores\n",
        "param: embeddings (tensor): Embeddings to perform similarity search\n",
        "param: dataset (Dataset): Dataset containing true labels\n",
        "\"\"\"\n",
        "def run_similarity_search(embeddings, dataset):\n",
        "\n",
        "    test_embs = embeddings.detach().cpu().numpy()\n",
        "    test_lbls = dataset.y.detach().cpu().numpy()\n",
        "    numRows = test_embs.shape[0]\n",
        "\n",
        "    cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)\n",
        "    st = []\n",
        "    for N in [5, 10]:\n",
        "        indices = np.argsort(cos_sim_array, axis=1)[:, -N:]\n",
        "        tmp = np.tile(test_lbls, (numRows, 1))\n",
        "        selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)\n",
        "        original_label = np.repeat(test_lbls, N).reshape(numRows,N)\n",
        "        st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))\n",
        "\n",
        "    print('Evaluate similarity search results')\n",
        "    print(\"** sim@5 : {} | sim@10 : {} **\".format(st[0], st[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvdSPgdXHVnx",
        "outputId": "db66d3d2-072c-455c-8806-26013025fd55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate clustering results\n",
            "** Clustering NMI: 0.8353 | homogeneity score: 0.8294 **\n",
            "time: 57.7 s (started: 2024-05-01 20:02:35 +00:00)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    embedding = torch.load(\"/content/model_checkpoints/embeddings_adam_clustering_localtest.pt\")\n",
        "    dataset = torch.load(\"/content/data_testing/processed/data.pt\")\n",
        "    evaluate_clustering(embedding,dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvCugL3lGyfY"
      },
      "source": [
        "## (1) Analysis\n",
        "\n",
        "\n",
        "These are the scores we get from running the trained model on the test dataset:\n",
        "\n",
        "*   NMI: 0.84   \n",
        "*   ARI (Homogeneity): 0.83\n",
        "\n",
        "Our first Hypothesis was: \"Hypothesis: We hypothesize that scGCL will exhibit better clustering performance and imputation performance than other existing methods.\"\n",
        "\n",
        "From the table in the evaluation section, you can compare the NMI and ARI scores of the scGCL model and other existing models. However, from the evaluation scores, we can see that our hypothesis is not 100% true. Our trained model performs better than most existing methods but does not perform better than the existing scTAG method in both NMI and ARI scores. This deviates from the results presented by the original paper which reports much higher ARI and NMI scores. We believe this may have something to do with the randomness introduced into the model - the random values used in the original paper are different than ours, so we would have to experiment with different hyper-parameters to train a better model.\n",
        "\n",
        "The original paper reports these scores on the test dataset:\n",
        "\n",
        "*   NMI: 0.89   \n",
        "*   ARI (Homogeneity): 0.91\n",
        "\n",
        "As mentioned before, these scores are higher from the ones we get from our reproduced model. We think is is due to the randomness that is introduced into our model. The random number generator that we use in our environment (colab) is likely different from the one used in the original paper. This is likely the cause of the difference. If we had more time, we could run more iterations of the model while varying the hyper-parameters to try to increase the current performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsVcMe2KveK6"
      },
      "source": [
        "## (2) Ablation Study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG0_Lgt4vzzB"
      },
      "source": [
        "Although not implemented in our project, the paper also conducted ablation studies against the Adam dataset. Two key components were systematically removed to assess their impact on the system's performance:\n",
        "\n",
        "The first scenario involved the removal of the ZINB-based encoder while maintaining the AFGRL and graph convolutional models. This allowed us to observe how the system performs without the contribution of the ZINB encoder, focusing solely on the effects of the AFGRL framework and the graph convolutional approach.\n",
        "\n",
        "In the second scenario, the ZINB-based encoder was retained, but the graph convolution and AFGRL models were excluded. This setup aimed to isolate and evaluate the significance of the ZINB-based encoder by removing the influence of the other two components, offering insight into its standalone contribution to handling the datasets.\n",
        "\n",
        "As shown by the values in the below Table, the method performs better with the ZINB distribution included, which captures the global probability distribution of scRNA-seq data. For the Adam dataset, having both the ZINB-based encoder and AFGRL model produced the best results.\n",
        "\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1psEUY5DRJA5H8FS9paVNAudAeCqoZX36)\n",
        "Figure C. Ablation Study Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "**Was the paper reproducible?**\n",
        "\n",
        "We found that the paper was reproducible, although with a few minor revisions. We needed to update a few deprecated functions to the new versions, as well as edit some code to make it work in a colab notebook. Also, we could not get the same performance as the original paper with the listed hyper-parameters. We theorize that this is because of the random noise added into the model. Our (Colab's) random number generator is most likely different from the random number generator used in the original paper. Overall, we think the paper was fairly reporoducible, with minor revisions needed.\n",
        "\n",
        "**What was easy and what wasn't**\n",
        "\n",
        "Easy: Tasks, including data preprocessing, clustering, training, etc. were intuitively split up and easy to dissect. Each one of these tasks were in their own .py file in the original repository\n",
        "\n",
        "Difficult: Parts of code were deprecated and many portions of code were not necessary. Not all datasets are readily or easily accessible either. Figuring out how the raw dataset is structured also was difficult due to the lack of documentation. Some annotations in the code were simply incorrect as they were copied from online tutorials.\n",
        "\n",
        "**Suggestions to make reproduction better**\n",
        "\n",
        "Better annotated code and more readily available data would help greatly. Also, updating the code to replace any deprecated functions with the updated version.\n",
        "\n",
        "**Plans for the next phase**\n",
        "\n",
        "*   Experiment with hyperparameters to increase performance\n",
        "*   Add more detailed annotations\n",
        "*   Clean up any unnecessary code\n",
        "*   Transfer code to github repo\n",
        "*   Create data download instructions\n",
        "*   Video report\n",
        "*   Fill in any missing write up requirements\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Project Github Repository: https://github.com/juehoujhou4/CS598DLH-FinalProject\n",
        "\n",
        "2.  Zehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, Bo Wang, scGCL: an imputation method for scRNA-seq data based on graph contrastive learning, Bioinformatics, Volume 39, Issue 3, March 2023, btad098, https://doi.org/10.1093/bioinformatics/btad098\n",
        "\n",
        "3. Original Github Repository: https://github.com/zehaoxiong123/scGCL/tree/main\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}