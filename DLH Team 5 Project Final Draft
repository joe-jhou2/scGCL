{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb","timestamp":1710432025769},{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}],"toc_visible":true,"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Github Repository: https://github.com/juehoujhou4/CS598DLH-FinalProject"],"metadata":{"id":"J5z3M58Ir1BD"}},{"cell_type":"markdown","source":["# Introduction\n"],"metadata":{"id":"MQ0sNuMePBXx"}},{"cell_type":"markdown","source":["## **scGCL: an imputation method for scRNA-seq data based on Graph Contrastive Learning**\n","\n","### **Background**\n","The sparse nature of single-cell RNA sequencing (scRNA-seq) data arises from the biological and technical challenges encountered when measuring gene expression at the individual cell level. This sparsity indicates that numerous genes go undetected in the examined cells within the dataset. Several factors contribute to this phenomenon, including the complexities of capturing and analyzing the minute amounts of RNA present in single cells, such as:\n","\n","1. Low RNA content- Low content makes it challenging to detect all mRNA molecules present; some cells naturally contain more mRNA molecules than others, which have more chance to detect the genes.\n","2. Low gene expression or gene expression variance- Natural differences in gene expression levels between cells, even within the same cell type, can contribute to the observed sparsity. Some genes are only expressed in specific cell states or conditions, leading to a large number of zeros in the data matrix where a particular gene is not active in most cells.\n","3. Technical variance- The process of isolating single cells, reverse transcribing RNA into cDNA, and amplifying the cDNA before sequencing introduces technical variability. Some mRNA molecules may be lost or degraded during these steps, resulting in incomplete detection of the transcriptome.\n","4. Dropout events- The most important factor contributing to sparsity, dropout events occur when mRNA molecules present in the cell are not detected, leading to a zero count for genes that are actually expressed but missed during sequencing. This is often due to inefficiencies in reverse transcription, amplification, or sequencing steps. Addressing dropout events through imputation in scRNA-seq data can significantly streamline the analysis process by filling in gaps where gene expression values are missing or undetected.\n","\n","The importance of addressing this sparsity problem lies in several key areas:\n","1. Enhanced Data Quality: Sparsity in scRNA-seq data means that many gene expression readings are missing or near zero, leading to a dataset filled with a lot of noise and few signals. By effectively imputing these missing values, the quality of the data can be significantly enhanced, leading to more reliable and interpretable results.\n","2. Improved Biological Insights: The main goal of scRNA-seq analysis is to uncover the complex mechanisms of cellular processes and heterogeneity among cells in different states or environments. Addressing the sparsity issue allows for a more accurate identification of gene expression patterns, cell types, and developmental states, thereby facilitating deeper biological insights and discoveries.\n","3. Enabling Advanced Analysis: Many downstream analyses, such as clustering, trajectory inference, and differential expression analysis, require robust datasets without extensive missing values. By solving the sparsity problem, these analyses can be performed more effectively, leading to more nuanced understanding of cellular behavior and interactions.\n","4. Increased Comparability and Integration: As the field moves towards large-scale studies involving multiple datasets, the ability to accurately impute missing data becomes crucial for integrating and comparing datasets from different sources. This comparability is essential for drawing broader conclusions and for the reproducibility of findings across studies.\n","\n","The difficulty of addressing the sparsity in scRNA-seq data include:\n","1. Lack of Ground Truth: In many cases, there is no \"ground truth\" for what the imputed values should be. This makes it difficult to train models and evaluate their performance objectively. The best that can be done is to rely on biological validation or downstream analysis outcomes, which can be time-consuming and not always definitive.\n","2. Data complexity: The high dimensionality of the data, coupled with the sparsity of gene expression (many genes are not expressed in many cells), makes it difficult to accurately impute missing values without introducing bias or losing critical information.\n","3. Noise: scRNA-seq data is also plagued by technical noise and dropout events, where genes are expressed but not detected due to limitations in sequencing depth or efficiency. Differentiating between true biological zeros and technical dropouts is a significant challenge.\n","\n","The ideal approach we desired:\n","1. The approach not only diminishes noise in the data but also enhances the precision of clustering and classification efforts.\n","2. It bolsters the accuracy of differential expression analyses, and aids in the integration of data from various sources, thereby simplifying the overall complexity of handling scRNA-seq datasets.\n","\n","### **Paper proposed**\n","The paper proposes a novel method, scGCL (single-cell Graph Contrastive Learning), specifically designed for imputing missing values in single-cell RNA sequencing (scRNA-seq) data. This method innovatively combines graph contrastive learning with the Zero-inflated Negative Binomial (ZINB) distribution model to estimate dropout events accurately. By employing contrastive learning within a graph theory framework, scGCL is adept at capturing the complex relationships between cells, thereby enhancing the prediction and reconstruction of missing gene expression values.\n","\n","\n","The innovation of the scGCL method lies in its unique approach to leveraging the strengths of graph contrastive learning, tailored specifically to the graph domain of scRNA-seq data. This allows for a sophisticated encapsulation of both global and local semantic information within the data. Furthermore, scGCL introduces a strategic selection of positive samples that significantly improve the representation of target nodes. This is complemented by the utilization of an autoencoder framework based on the ZINB distribution, specifically designed to model the global probability distribution of gene expression data effectively. This nuanced approach provides a robust solution to dropout imputation challenges, setting it apart from existing methods.\n","\n","\n","The contribution of scGCL to the research regime is substantial, addressing critical challenges outlined in the background of sparse data, technical noise, and the need for advanced computational strategies in scRNA-seq data analysis. By providing a more accurate and efficient method for imputing missing values, scGCL enhances the quality of scRNA-seq data analysis, leading to deeper biological insights and facilitating advancements in personalized medicine and genomics. This approach not only solves a significant problem in the field but also pushes the boundaries of what is possible with self-supervised learning in genomics, highlighting the importance of this paper to the ongoing research and development within the domain."],"metadata":{"id":"lgIJyogv6pcv"}},{"cell_type":"markdown","source":["# Scope of Reproducibility\n","\n"],"metadata":{"id":"uygL9tTPSVHB"}},{"cell_type":"markdown","source":["Hypotheses to be tested:\n","\n","Hypothesis 1: We hypothesize that scGCL will exhibit better clustering performance and imputation performance than other existing methods."],"metadata":{"id":"ZOvDrGaofN5T"}},{"cell_type":"markdown","source":["# Methodology"],"metadata":{"id":"xWAHJ_1CdtaA"}},{"cell_type":"markdown","source":["## (1) Environment\n","\n","The below two cells download and import the necessary packages for the model. The following is a list of versions for each necessary package that we used to recreate the original code. This code has only been tested in the Google colab environment.\n","\n","\n","*   Python: 3.10\n","*   numpy:  1.25.2\n","*   pandas:  2.0.3\n","*   sklearn:  1.2.2\n","*   h5py 3.9.0\n","*   scipy:  1.11.4\n","*   torch:  2.2.1+cu121\n","*   faiss 1.8.0\n","*   argparse:  1.1\n","*   anndata: 0.10.7\n","*   scanpy: 1.10.1\n","*   torch_geometric: 2.5.2\n","*   tensorboardX: 2.6.2.2\n","*   matplotlib: 3.7.1\n","\n","Please add the following folder as a shortcut on your 'My Drive' to access the project files: https://drive.google.com/drive/folders/1jRK-MyK9mvC5eoPjtSZ8qIbYdjCT6Lwl?usp=sharing\n","\n","**This is necessary to run the notebook as it needs access to the data files and the pre-trained model.**\n","\n","Please run the below cells to download and import the necessary packages:"],"metadata":{"id":"f3Z6I7smTuL_"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"sfk8Zrul_E8V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948097854,"user_tz":420,"elapsed":17071,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"1099b63a-620b-49a6-803c-217ec5cf477c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install scanpy\n","!pip install torch_geometric\n","!pip install tensorboardX\n","!pip install faiss-cpu\n","\n","!pip install ipython-autotime\n","%load_ext autotime"],"metadata":{"id":"bdA8lWvyPCkS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948147902,"user_tz":420,"elapsed":50052,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"d3eb35aa-7f30-4d2f-ea56-aa7a4e6cc81c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scanpy\n","  Downloading scanpy-1.10.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting anndata>=0.8 (from scanpy)\n","  Downloading anndata-0.10.7-py3-none-any.whl (122 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.9.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.0)\n","Collecting legacy-api-wrap>=1.4 (from scanpy)\n","  Downloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n","Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.7.1)\n","Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n","Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.3)\n","Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.58.1)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.25.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.0)\n","Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.0.3)\n","Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n","Collecting pynndescent>=0.5 (from scanpy)\n","  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.2.2)\n","Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.11.4)\n","Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.1)\n","Collecting session-info (from scanpy)\n","  Downloading session_info-1.0.0.tar.gz (24 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.2)\n","Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n","  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n","  Downloading array_api_compat-1.6-py3-none-any.whl (36 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.41.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.4.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n","Collecting stdlib_list (from session-info->scanpy)\n","  Downloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: session-info\n","  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8026 sha256=eb21dc999f95859874283db2200703c45185ac5d35ebb9f18939031a16fd685e\n","  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n","Successfully built session-info\n","Installing collected packages: array-api-compat, stdlib_list, legacy-api-wrap, session-info, pynndescent, anndata, umap-learn, scanpy\n","Successfully installed anndata-0.10.7 array-api-compat-1.6 legacy-api-wrap-1.4 pynndescent-0.5.12 scanpy-1.10.1 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n","Collecting torch_geometric\n","  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n","Installing collected packages: torch_geometric\n","Successfully installed torch_geometric-2.5.2\n","Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n","Installing collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.8.0\n","Collecting ipython-autotime\n","  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n","Collecting jedi>=0.16 (from ipython->ipython-autotime)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.43)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n","Installing collected packages: jedi, ipython-autotime\n","Successfully installed ipython-autotime-0.3.2 jedi-0.19.1\n","time: 287 µs (started: 2024-04-12 18:55:47 +00:00)\n"]}]},{"cell_type":"code","source":["# import packages you need\n","from google.colab import drive\n","import sys\n","from tensorboardX import SummaryWriter\n","import os\n","import os.path as osp\n","import copy\n","import faiss\n","import argparse\n","from datetime import datetime\n","\n","import csv\n","import cmath as cm\n","import h5py\n","from matplotlib import rcParams\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import random\n","import math\n","from anndata import AnnData\n","\n","import sklearn\n","from sklearn.neighbors import kneighbors_graph\n","from sklearn.decomposition import PCA\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.cluster import KMeans\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn import metrics\n","from sklearn.metrics import normalized_mutual_info_score, pairwise, adjusted_rand_score,silhouette_score\n","\n","import scanpy as sc\n","import scipy\n","import scipy.stats\n","from scipy import sparse as sp\n","from scipy.optimize import linear_sum_assignment\n","from scipy.sparse import csr_matrix\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.autograd import Variable\n","from torch_geometric.data import InMemoryDataset, download_url, Data\n","from torch_geometric.io import read_npz\n","from torch_geometric.utils import remove_self_loops, to_undirected, dropout_adj\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.loader import DataLoader\n","import torch_geometric.transforms as T\n","from torch_geometric.utils.loop import add_self_loops"],"metadata":{"id":"yu61Jp1xrnKk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948163667,"user_tz":420,"elapsed":15769,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"22cf779b-9e5c-41a8-ac70-9f75bb9a5705"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 15.8 s (started: 2024-04-12 18:55:47 +00:00)\n"]}]},{"cell_type":"markdown","source":["## (2) Data\n"],"metadata":{"id":"lZtoSbHqWkfx"}},{"cell_type":"markdown","source":["\n","*   Source of the data: The paper actually runs the experiment on 14 different datasets. However, we will run our project on a single dataset which the paper has called \"Adam\" because it comes from this paper ([Adam et al. (2017)](https://journals.biologists.com/dev/article/144/19/3625/48196/Psychrophilic-proteases-dramatically-reduce-single)). The dataset is a set of RNA sequences that is taken from kidney cells of mice. This dataset contains data for 3,660 cells, 23,797 genes and 8 cell types. The dataset is available to download [here](https://github.com/zehaoxiong123/scGCL/tree/main/test_csv/Adam) from the original github repository.\n","\n","*  A gene expression matrix is a matrix that contains cell samples and a gene. In our case, each cell is a row in the matrix and each gene is is a column. Each entry of the matrix is a numerical value that is >= 0 that represents the read counts of each gene in a particular cell. For example, the entry in row 2; col 6 represents the read count of gene 6 in cell sample 2.\n","\n","*   Data Process: The expression matrix of the scRNA-seq data is taken as the input raw data. To reduce noise in the scRNA-seq data, the paper pre-process the raw gene expression profiles using the following pre-processing methods:\n","\n","  1.   Data filtering and quality control are the first steps in scRNA-seq data pre-processing. Therefore, we only keep genes with non-zero expression in more than 1% of cells and cells with non-zero expression in more than 1% of genes.\n","  2.   Since the data in the count matrix are discrete and affected by the size factor, we normalize it by the size factor then transform discrete values through the log function. Finally, we select the top $t$ highly variable genes based on the normalized discrete values computed by the scanpy package.\n","  \n","    Generally, we select 2048 highly variable genes for training and use a consistent pre-processing method before running all baseline methods.\n","\n","\n","*   Illustration: ![picture](https://drive.google.com/uc?id=1ECCyhcVFhvKKIU3q-MM1EbTwvnjDlEJq)"],"metadata":{"id":"XzVUQS0CHry0"}},{"cell_type":"markdown","source":["### (2.1) Dataset Information\n","\n","Run cell below to see info about the dataset"],"metadata":{"id":"oaPncTenyBEF"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"BZScZNbROw-N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168385,"user_tz":420,"elapsed":4721,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"716779f3-4866-4708-f434-420528bef9ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Organism that provided cell samples:  Mus musculus\n","Lifestage of when cell sample was taken:  post natal day 1\n","Organ that cell sample was taken from:  Kidney\n","Num of unique cell types:  8\n","List of unique cell types:  ['ED', 'LH', 'CM', 'Podocytes', 'Distal tubule', 'Ureteric bud', 'Stromal', 'PT']\n","Num of cell samples (Rows):  3660\n","Num of genes (Columns):  23797\n","This is an example row from the expression matrix:\n","[0. 1. 1. ... 0. 0. 0.]\n","time: 4.66 s (started: 2024-04-12 18:56:03 +00:00)\n"]}],"source":["# dir and function to load raw data\n","raw_data_dir = '/content/drive/MyDrive/CS598 DLH Project/test_csv/data.h5'\n","\n","# load raw file\n","f = h5py.File(raw_data_dir, 'r')\n","\n","data = np.array(f[\"exprs/data\"])\n","indices = np.array(f[\"exprs/indices\"])\n","indptr = np.array(f[\"exprs/indptr\"])\n","\n","matrix = csr_matrix((data, indices, indptr), shape=list(f[\"exprs/shape\"]))\n","\n","# list out organism\n","organism = str(list(f[\"obs/organism\"])[0])[2:-1] # string slicing just to get rid of extra characters\n","print(\"Organism that provided cell samples: \", organism)\n","\n","# list out lifestage it was taken\n","lifestage = str(list(f[\"obs/lifestage\"])[0])[2:-1]\n","print(\"Lifestage of when cell sample was taken: \", lifestage)\n","\n","# list out organ taken sample from\n","organ = str(list(f[\"obs/organ\"])[0])[2:-1]\n","print(\"Organ that cell sample was taken from: \", organ)\n","\n","# num of unique cell types\n","cell_types = list(set(list(f[\"obs/cell_type1\"])))\n","cell_types = [str(cell)[2:-1] for cell in cell_types]\n","\n","print(\"Num of unique cell types: \", len(cell_types))\n","# list out unique cell types\n","print(\"List of unique cell types: \", cell_types)\n","\n","# num of cell samples\n","print(\"Num of cell samples (Rows): \", matrix.shape[0])\n","# num of genes\n","print(\"Num of genes (Columns): \", matrix.shape[1])\n","\n","# print out sample row (A single cell sample)\n","print(\"This is an example row from the expression matrix:\")\n","print(matrix.toarray()[0])"]},{"cell_type":"markdown","source":["### (2.2) Data preprocess functions\n","\n"],"metadata":{"id":"gA_1dMTB4Y30"}},{"cell_type":"code","source":["#data preprocess\n","def normalize_for_AF(filename,gene_num,raw, sparsify = False, skip_exprs = False):\n","    with h5py.File(filename, \"r\") as f:\n","        obs = np.array(f[\"obs_names\"][...])\n","        var = np.array(f[\"var_names\"][...])\n","        cell_name = np.array(f[\"obs\"][\"cell_type1\"])\n","        cell_type, cell_label = np.unique(cell_name, return_inverse=True)\n","        class_num = np.array(cell_label).max() + 1\n","        data_label = []\n","        data_array = []\n","        for i in range(cell_label.shape[0]):\n","            x = np.zeros(class_num)\n","            x[cell_label[i]] = 1\n","            data_label.append(x)\n","        data_label = np.array(data_label)\n","        cell_type = np.array(cell_type)\n","        if not skip_exprs:\n","            exprs_handle = f[\"exprs\"]\n","            if isinstance(exprs_handle, h5py.Group):\n","                mat = sp.csr_matrix((exprs_handle[\"data\"][...], exprs_handle[\"indices\"][...],\n","                                         exprs_handle[\"indptr\"][...]), shape=exprs_handle[\"shape\"][...])\n","            else:\n","                mat = exprs_handle[...].astype(np.float32)\n","                if sparsify:\n","                    mat = sp.csr_matrix(mat)\n","        else:\n","            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))\n","        X = np.array(mat.toarray())\n","        np.int = int\n","        X = np.ceil(X).astype(np.int)\n","        adata = sc.AnnData(X)\n","        adata.obs['Group'] = cell_label\n","        adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=raw, logtrans_input=True)\n","        count = adata.X\n","        return count,adata.obs['Group']\n","\n","def nomalize_for_AD(file_name,label_path,gene_num):\n","    data = pd.read_csv(file_name, header=None, sep=\",\")\n","    label = pd.read_csv(label_path, header=0, sep=\",\")\n","    # data_label = []\n","    label = np.array(label)[:, 2]\n","    cell_type, cell_label = np.unique(label, return_inverse=True)\n","    data_label = []\n","    for i in range(len(cell_label)):\n","        data_label.append(cell_type[cell_label[i]])\n","    data_label = np.array(data_label)\n","    print(data_label)\n","    arr1 = np.array(data)\n","    gene_name = np.array(arr1[1:, 0])\n","    cell_name = np.array(arr1[0, 1:])\n","    X = arr1[1:, 1:].T\n","    adata = sc.AnnData(X)\n","    # print(cell_type.shape)\n","    adata.obs['Group'] = data_label\n","    adata.obs['cell_name'] = cell_name\n","    adata.var['Gene'] = gene_name\n","    adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=False,\n","                      logtrans_input=True)\n","    count = adata.X\n","    return count, cell_label,adata.obs['size_factors'],adata.var['Gene']\n","\n","def acc(y_true, y_pred):\n","    print(\"DATA_PREPROCESS ACC\")\n","    \"\"\"\n","    Calculate clustering accuracy. Require scikit-learn installed\n","    # Arguments\n","        y: true labels, numpy.array with shape `(n_samples,)`\n","        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n","    # Return\n","        accuracy, in [0,1]\n","    \"\"\"\n","    y_true = y_true.astype(np.int64)\n","    assert y_pred.size == y_true.size\n","    D = max(y_pred.max(), y_true.max()) + 1\n","    w = np.zeros((D, D), dtype=np.int64)\n","    for i in range(y_pred.size):\n","        w[y_pred[i], y_true[i]] += 1\n","    ind = linear_sum_assignment(w.max() - w)\n","    ind = np.array(ind).T\n","    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n","\n","def normalize(adata, copy=True, highly_genes = None, filter_min_counts=True, size_factors=True, normalize_input=True, logtrans_input=True):\n","    if isinstance(adata, sc.AnnData):\n","        if copy:\n","            adata = adata.copy()\n","    elif isinstance(adata, str):\n","        adata = sc.read(adata)\n","    else:\n","        raise NotImplementedError\n","\n","    norm_error = 'Make sure that the dataset (adata.X) contains unnormalized count data.'\n","    assert 'n_count' not in adata.obs, norm_error\n","\n","    if adata.X.size < 50e6: # check if adata.X is integer only if array is small\n","        if sp.issparse(adata.X):\n","            assert (adata.X.astype(int) != adata.X).nnz == 0, norm_error\n","        else:\n","            assert np.all(adata.X.astype(int) == adata.X), norm_error\n","\n","    if filter_min_counts:\n","        sc.pp.filter_genes(adata, min_counts=1)\n","        sc.pp.filter_cells(adata, min_counts=1)\n","\n","    if size_factors or normalize_input or logtrans_input:\n","        adata.raw = adata.copy()\n","    else:\n","        adata.raw = adata\n","\n","    if size_factors:\n","        # sc.pp.normalize_per_cell(adata)\n","        sc.pp.normalize_total(adata)\n","        adata.obs['size_factors'] = adata.obs.n_counts / np.median(adata.obs.n_counts)\n","    else:\n","        adata.obs['size_factors'] = 1.0\n","\n","    if logtrans_input:\n","        sc.pp.log1p(adata)\n","\n","    if highly_genes is not None:\n","        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, n_top_genes = highly_genes, subset=True)\n","\n","    if normalize_input:\n","        sc.pp.scale(adata)\n","\n","    return adata"],"metadata":{"id":"JGzYFVPoMtb6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168385,"user_tz":420,"elapsed":6,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"05ebb3ac-67eb-4140-94be-83b82b771028"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 4.2 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"markdown","source":["## (3) Helper functions\n","\n","The below functions are helper and utility functions that make generating the graph, data processing, and imputation easier. The last helper function defines the single cell class. These cells need to be run before training the model."],"metadata":{"id":"NwJLGGS-31Yy"}},{"cell_type":"markdown","source":["### (3.1) Graph based functions\n","\n","Please see comments below for explanation on each function\n"],"metadata":{"id":"ojFgpkeP4RpW"}},{"cell_type":"code","source":["# preprocessing pipeline for graph-based data analysis\n","\n","# this function constructs an adjacency matrix and its normalized count data\n","# Optionally applies PCA to reduce dimensionality before computing\n","# the k-nearest neighbors graph.\n","def get_adj(count, k=15, pca=50, mode=\"connectivity\"):\n","    if pca:\n","        countp = dopca(count, dim=pca)\n","    else:\n","        countp = count\n","    A = kneighbors_graph(countp, k, mode=mode, metric=\"cosine\", include_self=True)\n","    adj = A.toarray()\n","    adj_n = norm_adj(adj)\n","    return adj, adj_n\n","\n","# This function computes a diagonal matrix where each diagonal element\n","# is the degree of the corresponding node in the graph\n","def degree_power(A, k):\n","    degrees = np.power(np.array(A.sum(1)), k).flatten()\n","    degrees[np.isinf(degrees)] = 0.\n","    if sp.issparse(A):\n","        D = sp.diags(degrees)\n","    else:\n","        D = np.diag(degrees)\n","    return D\n","\n","# This function normalizes the adjacency matrix using the degree matrix.\n","def norm_adj(A):\n","    normalized_D = degree_power(A, -0.5)\n","    output = normalized_D.dot(A).dot(normalized_D)\n","    return output\n","\n","# This function applies PCA to the data to reduce its dimensionality\n","def dopca(X, dim=10):\n","    pcaten = PCA(n_components=dim)\n","    X_10 = pcaten.fit_transform(X)\n","    return X_10"],"metadata":{"id":"1P1vo3v0MVHM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168385,"user_tz":420,"elapsed":4,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"b3a45cc0-043a-4b73-f41a-575f391cf1b3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.22 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"markdown","source":["### (3.2) Imputation functions"],"metadata":{"id":"S7ZpXd2j471b"}},{"cell_type":"code","source":["# Revised from Original version in scVI\n","# Ref:\n","# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py\n","\n","\n","def impute_dropout(X, seed=1, rate=0.1):\n","    \"\"\"\n","    X: original testing set\n","    ========\n","    returns:\n","    X_zero: copy of X with zeros\n","    i, j, ix: indices of where dropout is applied\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        X_zero = np.copy(X)\n","        # select non-zero subset\n","        i, j = np.nonzero(X_zero)\n","    # If the input is a sparse matrix\n","    else:\n","        X_zero = scipy.sparse.lil_matrix.copy(X)\n","        # select non-zero subset\n","        i, j = X_zero.nonzero()\n","\n","    np.random.seed(seed)\n","    # changes here:\n","    # choice number 1 : select 10 percent of the non zero values (so that distributions overlap enough)\n","    ix = np.random.choice(range(len(i)), int(\n","        np.floor(rate * len(i))), replace=False)\n","    # X_zero[i[ix], j[ix]] *= np.random.binomial(1, rate)\n","    X_zero[i[ix], j[ix]] = 0.0\n","\n","    # choice number 2, focus on a few but corrupt binomially\n","    #ix = np.random.choice(range(len(i)), int(slice_prop * np.floor(len(i))), replace=False)\n","    #X_zero[i[ix], j[ix]] = np.random.binomial(X_zero[i[ix], j[ix]].astype(np.int), rate)\n","    return X_zero, i, j, ix\n","\n","# IMPUTATION METRICS\n","# Revised freom Original version in scVI\n","# Ref:\n","# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py\n","\n","\n","def imputation_error(X_mean, X, X_zero, i, j, ix):\n","    print(\"DATA_PREPROCESS IMPUTATION ERROR\")\n","    \"\"\"\n","    X_mean: imputed dataset\n","    X: original dataset\n","    X_zero: zeros dataset, does not need\n","    i, j, ix: indices of where dropout was applied\n","    ========\n","    returns:\n","    median L1 distance between datasets at indices given\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        all_index = i[ix], j[ix]\n","        x, y = X_mean[all_index], X[all_index]\n","        result = np.abs(x - y)\n","    # If the input is a sparse matrix\n","    else:\n","        all_index = i[ix], j[ix]\n","        x = X_mean[all_index[0], all_index[1]]\n","        y = X[all_index[0], all_index[1]]\n","        yuse = scipy.sparse.lil_matrix.todense(y)\n","        yuse = np.asarray(yuse).reshape(-1)\n","        result = np.abs(x - yuse)\n","    # return np.median(np.abs(x - yuse))\n","    return np.mean(result), np.median(result), np.min(result), np.max(result)\n","\n","def imputation_cosine(X_mean, X, X_zero, i, j, ix):\n","    print(\"DATA_PREPROCESS IMPUTATION COSINE\")\n","    \"\"\"\n","    X_mean: imputed dataset\n","    X: original dataset\n","    X_zero: zeros dataset, does not need\n","    i, j, ix: indices of where dropout was applied\n","    ========\n","    returns:\n","    cosine similarity between datasets at indices given\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        all_index = i[ix], j[ix]\n","        x, y = X_mean[all_index], X[all_index]\n","        x = x.reshape(1, -1)\n","        y = y.reshape(1, -1)\n","\n","        print(x)\n","        print(y)\n","        result = cosine_similarity(x, y)\n","    # If the input is a sparse matrix\n","    else:\n","        all_index = i[ix], j[ix]\n","        x = X_mean[all_index[0], all_index[1]]\n","        y = X[all_index[0], all_index[1]]\n","        yuse = scipy.sparse.lil_matrix.todense(y)\n","        yuse = np.asarray(yuse).reshape(-1)\n","        x = x.reshape(1, -1)\n","        yuse = yuse.reshape(1, -1)\n","        result = cosine_similarity(x, yuse)\n","    # return np.median(np.abs(x - yuse))\n","    return result[0][0]\n"],"metadata":{"id":"hV3XDMVq5Biw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168557,"user_tz":420,"elapsed":175,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"15ee2c49-cc00-419d-a590-a8230e71485d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 12.7 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"markdown","source":["### (3.3) Define Single Cell class"],"metadata":{"id":"VAmmbiNB5H4o"}},{"cell_type":"code","source":["class Singlecell(InMemoryDataset):\n","\n","    def __init__(self, root, name, filepath, transform=None, pre_transform=None):\n","        self.name = name.lower()\n","        self.filepath = filepath\n","        self.labelpath = \"./test_csv/Alzheimer/GSE138852_covariates.csv\"\n","        # self.labelpath = \"./test_csv/Zeisel/label.csv\"\n","        super(Singlecell, self).__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","\n","    @property\n","    def raw_dir(self):\n","        return osp.join(self.root, self.name.capitalize(), 'raw')\n","\n","    @property\n","    def processed_dir(self):\n","        return osp.join(self.root, self.name.capitalize(), 'processed')\n","\n","    @property\n","    def raw_file_names(self):\n","        return 'amazon_electronics_{}.npz'.format(self.name.lower())\n","\n","    @property\n","    def processed_file_names(self):\n","        return 'data.pt'\n","\n","    def download(self):\n","\n","        pass\n","\n","    def process(self):\n","\n","        raw = False\n","        data, data_label = normalize_for_AF(self.filepath, 2048,raw);\n","\n","        x = torch.tensor(np.array(data),dtype=torch.float32)\n","        y = torch.tensor(data_label, dtype=torch.long)\n","\n","        adj, adj_n = get_adj(data)\n","        adj = sp.coo_matrix(adj_n)\n","        edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)\n","        edge_index, _ = remove_self_loops(edge_index)\n","        edge_index = to_undirected(edge_index, x.size(0))  # Internal coalesce.\n","        data = Data(x=x, edge_index=edge_index, y=y)\n","        data = data if self.pre_transform is None else self.pre_transform(data)\n","        data, slices = self.collate([data])\n","        print(self.processed_paths[0])\n","        torch.save((data, slices), self.processed_paths[0])\n","\n","    def __repr__(self):\n","        return '{}{}()'.format(self.__class__.__name__, self.name.capitalize())"],"metadata":{"id":"XE6NZLDIMPhH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168557,"user_tz":420,"elapsed":5,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"de8149f5-b63f-4569-a0f7-47e88c5fa50f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.99 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"markdown","source":["### (3.4) Utility functions"],"metadata":{"id":"FtFeVsVqxyrU"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","\n","def currentTime():\n","    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--embedder\", type=str, default=\"AFGRL\")\n","    parser.add_argument(\"--dataset\", type=str, default=\"adam\", help=\"Name of the dataset. Supported names are: wikics, cs, computers, photo, and physics\")\n","\n","    parser.add_argument('--checkpoint_dir', type=str, default = '/content/drive/MyDrive/CS598 DLH Project/model_checkpoints', help='directory to save checkpoint')\n","    parser.add_argument(\"--root\", type=str, default=\"data\")\n","    parser.add_argument(\"--task\", type=str, default=\"clustering\", help=\"Downstream task. Supported tasks are: node, clustering, similarity\")\n","\n","    parser.add_argument(\"--layers\", nargs='?', default='[2048]', help=\"The number of units of each layer of the GNN. Default is [256]\")\n","    parser.add_argument(\"--pred_hid\", type=int, default=2048, help=\"The number of hidden units of layer of the predictor. Default is 512\")\n","\n","    parser.add_argument(\"--topk\", type=int, default=4, help=\"The number of neighbors to search\")\n","    parser.add_argument(\"--clus_num_iters\", type=int, default=20)\n","    parser.add_argument(\"--num_centroids\", type=int, default=9, help=\"The number of centroids for K-means Clustering\")\n","    parser.add_argument(\"--num_kmeans\", type=int, default=5, help=\"The number of K-means Clustering for being robust to randomness\")\n","    parser.add_argument(\"--eval_freq\", type=float, default=5, help=\"The frequency of model evaluation\")\n","    parser.add_argument(\"--mad\", type=float, default=0.9, help=\"Moving Average Decay for Teacher Network\")\n","    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n","    parser.add_argument(\"--es\", type=int, default=300, help=\"Early Stopping Criterion\")\n","    parser.add_argument(\"--device\", type=int, default=0)\n","    parser.add_argument(\"--epochs\", type=int, default=1) #For demonstration purposes, set to 1. Model has been pre-trained with 300 epochs.\n","    parser.add_argument(\"--dropout\", type=float, default=0.0)\n","    parser.add_argument(\"--aug_params\", \"-p\", nargs=\"+\", default=[0.3, 0.4, 0.3, 0.2],help=\"Hyperparameters for augmentation (p_f1, p_f2, p_e1, p_e2). Default is [0.2, 0.1, 0.2, 0.3]\")\n","    return parser.parse_known_args()\n","\n","class Augmentation:\n","\n","    def __init__(self, p_f1=0.2, p_f2=0.1, p_e1=0.2, p_e2=0.3):\n","        \"\"\"\n","        two simple graph augmentation functions --> \"Node feature masking\" and \"Edge masking\"\n","        Random binary node feature mask following Bernoulli distribution with parameter p_f\n","        Random binary edge mask following Bernoulli distribution with parameter p_e\n","        \"\"\"\n","        self.p_f1 = p_f1\n","        self.p_f2 = p_f2\n","        self.p_e1 = p_e1\n","        self.p_e2 = p_e2\n","        self.method = \"BGRL\"\n","\n","    def _feature_masking(self, data, device):\n","        feat_mask1 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f1\n","        feat_mask2 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f2\n","        feat_mask1, feat_mask2 = feat_mask1.to(device), feat_mask2.to(device)\n","        x1, x2 = data.x.clone(), data.x.clone()\n","        x1, x2 = x1 * feat_mask1, x2 * feat_mask2\n","\n","        edge_index1, edge_attr1 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e1)\n","        edge_index2, edge_attr2 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e2)\n","\n","        new_data1, new_data2 = data.clone(), data.clone()\n","        new_data1.x, new_data2.x = x1, x2\n","        new_data1.edge_index, new_data2.edge_index = edge_index1, edge_index2\n","        new_data1.edge_attr, new_data2.edge_attr = edge_attr1, edge_attr2\n","\n","        return new_data1, new_data2\n","\n","    def __call__(self, data):\n","        return self._feature_masking(data)\n","\n","def decide_config(root, dataset):\n","    \"\"\"\n","    Create a configuration to download datasets\n","    :param root: A path to a root directory where data will be stored\n","    :param dataset: The name of the dataset to be downloaded\n","    :return: A modified root dir, the name of the dataset class, and parameters associated to the class\n","    \"\"\"\n","    dataset = dataset.lower()\n","    if dataset == \"adam\":\n","        dataset = \"Adam\"\n","        root = osp.join(root, \"pyg\")\n","        filepath = '/content/drive/MyDrive/CS598 DLH Project/test_csv/data.h5'\n","        params = {\"kwargs\": {\"root\": root, \"name\": dataset, \"filepath\": filepath},\n","                  \"name\": dataset, \"class\": Singlecell, \"src\": \"pyg\"}\n","    else:\n","        raise Exception(\n","            f\"Unknown dataset name {dataset}, name has to be one of the following 'cora', 'citeseer', 'pubmed', 'photo', 'computers', 'cs', 'physics'\")\n","    return params\n","\n","\n","def create_dirs(dirs):\n","    for dir_tree in dirs:\n","        sub_dirs = dir_tree.split(\"/\")\n","        path = \"\"\n","        for sub_dir in sub_dirs:\n","            path = osp.join(path, sub_dir)\n","            os.makedirs(path, exist_ok=True)\n","\n","def tensor2var(x, grad=False):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return Variable(x, requires_grad=grad)\n","\n","def create_masks(data):\n","    \"\"\"\n","    Splits data into training, validation, and test splits in a stratified manner if\n","    it is not already splitted. Each split is associated with a mask vector, which\n","    specifies the indices for that split. The data will be modified in-place\n","    :param data: Data object\n","    :return: The modified data\n","    \"\"\"\n","    if not hasattr(data, \"val_mask\"):\n","\n","        data.train_mask = data.dev_mask = data.test_mask = None\n","\n","        for i in range(20):\n","            labels = data.y.numpy()\n","            dev_size = int(labels.shape[0] * 0.1)\n","            test_size = int(labels.shape[0] * 0.8)\n","\n","            perm = np.random.permutation(labels.shape[0])\n","            test_index = perm[:test_size]\n","            dev_index = perm[test_size:test_size + dev_size]\n","\n","            data_index = np.arange(labels.shape[0])\n","            test_mask = torch.tensor(np.in1d(data_index, test_index), dtype=torch.bool)\n","            dev_mask = torch.tensor(np.in1d(data_index, dev_index), dtype=torch.bool)\n","            train_mask = ~(dev_mask + test_mask)\n","            test_mask = test_mask.reshape(1, -1)\n","            dev_mask = dev_mask.reshape(1, -1)\n","            train_mask = train_mask.reshape(1, -1)\n","\n","            if 'train_mask' not in data:\n","                data.train_mask = train_mask\n","                data.val_mask = dev_mask\n","                data.test_mask = test_mask\n","            else:\n","                data.train_mask = torch.cat((data.train_mask, train_mask), dim=0)\n","                data.val_mask = torch.cat((data.val_mask, dev_mask), dim=0)\n","                data.test_mask = torch.cat((data.test_mask, test_mask), dim=0)\n","\n","    else:  # in the case of WikiCS\n","        data.train_mask = data.train_mask.T\n","        data.val_mask = data.val_mask.T\n","\n","    return data\n","\n","\n","class EMA:\n","    def __init__(self, beta, epochs):\n","        super().__init__()\n","        self.beta = beta\n","        self.step = 0\n","        self.total_steps = epochs\n","\n","    def update_average(self, old, new):\n","        if old is None:\n","            return new\n","\n","        beta = 1 - (1 - self.beta) * (np.cos(np.pi * self.step / self.total_steps) + 1) / 2.0\n","        self.step += 1\n","        return old * beta + (1 - beta) * new\n","\n","\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","\n","\n","def loss_fn(x, y):\n","    x = F.normalize(x, dim=-1, p=2)\n","    y = F.normalize(y, dim=-1, p=2)\n","\n","    return 2 - 2 * (x * y).sum(dim=-1)\n","\n","\n","def l2_normalize(x):\n","    return x / torch.sqrt(torch.sum(x**2, dim=1).unsqueeze(1))\n","\n","\n","def update_moving_average(ema_updater, ma_model, current_model):\n","    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n","        old_weight, up_weight = ma_params.data, current_params.data\n","        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n","\n","\n","def set_requires_grad(model, val):\n","    for p in model.parameters():\n","        p.requires_grad = val\n","\n","\n","def enumerateConfig(args):\n","    args_names = []\n","    args_vals = []\n","    for arg in vars(args):\n","        args_names.append(arg)\n","        args_vals.append(getattr(args, arg))\n","\n","    return args_names, args_vals\n","\n","\n","def config2string(args):\n","    args_names, args_vals = enumerateConfig(args)\n","    st = ''\n","    for name, val in zip(args_names, args_vals):\n","        if val == False:\n","            continue\n","        if name in ['dataset']:\n","            st_ = \"{}_{}_\".format(name, val)\n","            st += st_\n","\n","    return st[:-1]\n","\n","\n","def printConfig(args):\n","    args_names, args_vals = enumerateConfig(args)\n","    print(args_names)\n","    print(args_vals)\n","\n","\n","def repeat_1d_tensor(t, num_reps):\n","    return t.unsqueeze(1).expand(-1, num_reps)\n"],"metadata":{"id":"Yi_REaQFNTGw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168558,"user_tz":420,"elapsed":5,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"9e2ea937-a072-4d28-ad23-f056800e30c7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 11.2 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"code","source":["np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","\n","def download_pyg_data(config):\n","    \"\"\"\n","    Downloads a dataset from the PyTorch Geometric library\n","\n","    :param config: A dict containing info on the dataset to be downloaded\n","    :return: A tuple containing (root directory, dataset name, data directory)\n","    \"\"\"\n","    leaf_dir = config[\"kwargs\"][\"root\"].split(\"/\")[-1].strip()\n","    data_dir = osp.join(config[\"kwargs\"][\"root\"], \"\" if config[\"name\"] == leaf_dir else config[\"name\"])\n","    dst_path = osp.join(data_dir, \"raw\", \"data.pt\")\n","    if not osp.exists(dst_path):\n","        DatasetClass = config[\"class\"]\n","        if config[\"name\"] == \"WikiCS\":\n","            dataset = DatasetClass(data_dir, transform=T.NormalizeFeatures())\n","            size_factor = dataset.size_factors\n","        else :\n","            dataset = DatasetClass(**config[\"kwargs\"])\n","        create_masks(data=dataset.data)\n","        torch.save((dataset.data, dataset.slices), dst_path)\n","    return config[\"kwargs\"][\"root\"], config[\"name\"], data_dir\n","\n","\n","def download_data(root, dataset):\n","    \"\"\"\n","    Download data from different repositories. Currently only PyTorch Geometric is supported\n","\n","    :param root: The root directory of the dataset\n","    :param name: The name of the dataset\n","    :return:\n","    \"\"\"\n","    config = decide_config(root=root, dataset=dataset)\n","    if config[\"src\"] == \"pyg\":\n","        return download_pyg_data(config)\n","\n","\n","class Dataset(InMemoryDataset):\n","\n","    \"\"\"\n","    A PyTorch InMemoryDataset to build multi-view dataset through graph data augmentation\n","    \"\"\"\n","\n","    def __init__(self, root=\"data\", dataset='Adam', transform=None, pre_transform=None):\n","        self.root, self.dataset, self.data_dir = download_data(root=root, dataset=dataset)\n","        create_dirs(self.dirs)\n","        super().__init__(root=self.data_dir, transform=transform, pre_transform=pre_transform)\n","        self.process()\n","        path = osp.join(self.data_dir, \"processed\", self.processed_file_names[0])\n","        self.data, self.slices = torch.load(path)\n","\n","    def process_full_batch_data(self, data):\n","\n","        print(\"Processing full batch data\")\n","        nodes = torch.tensor(np.arange(data.num_nodes), dtype=torch.long)\n","\n","        edge_index, edge_attr = add_self_loops(data.edge_index, data.edge_attr)\n","\n","        data = Data(nodes=nodes, edge_index=data.edge_index, edge_attr=data.edge_attr, x=data.x, y=data.y,\n","                    train_mask=data.train_mask, val_mask=data.val_mask, test_mask=data.test_mask,\n","                    num_nodes=data.num_nodes, neighbor_index=edge_index, neighbor_attr=edge_attr)\n","\n","        return [data]\n","\n","    def process(self):\n","        \"\"\"\n","        Process either a full batch or cluster data.\n","\n","        :return:\n","        \"\"\"\n","        processed_path = osp.join(self.processed_dir, self.processed_file_names[0])\n","        if not osp.exists(processed_path):\n","            path = osp.join(self.raw_dir, self.raw_file_names[0])\n","            data, _ = torch.load(path)\n","            edge_attr = data.edge_attr\n","            edge_attr = torch.ones(data.edge_index.shape[1]) if edge_attr is None else edge_attr\n","            data.edge_attr = edge_attr\n","\n","\n","            data_list = self.process_full_batch_data(data)\n","\n","            data, slices = self.collate(data_list)\n","            torch.save((data, slices), processed_path)\n","\n","\n","    @property\n","    def raw_file_names(self):\n","        return [\"data.pt\"]\n","\n","    @property\n","    def processed_file_names(self):\n","        return [f'byg.data.pt']\n","\n","    @property\n","    def raw_dir(self):\n","        return osp.join(self.data_dir, \"raw\")\n","\n","    @property\n","    def processed_dir(self):\n","        return osp.join(self.data_dir, \"processed\")\n","\n","    @property\n","    def model_dir(self):\n","        return osp.join(self.data_dir, \"model\")\n","\n","    @property\n","    def result_dir(self):\n","        return osp.join(self.data_dir, \"results\")\n","\n","    @property\n","    def dirs(self):\n","        return [self.raw_dir, self.processed_dir, self.model_dir, self.result_dir]\n","\n","    def download(self):\n","        pass"],"metadata":{"id":"V2Bgey9kLm9n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168558,"user_tz":420,"elapsed":4,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"1e10f80f-abde-40a4-911b-1871ac5d00b5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 5.85 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"markdown","source":["##  (4) Model\n","\n","**References:**\n","\n","1.  Zehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, Bo Wang, scGCL: an imputation method for scRNA-seq data based on graph contrastive learning, Bioinformatics, Volume 39, Issue 3, March 2023, btad098, https://doi.org/10.1093/bioinformatics/btad098\n","\n","\n","2. Link to the original github repository: https://github.com/zehaoxiong123/scGCL/tree/main\n","\n","\n","\n","\n","**Model architectures:** There are two different models that make up the overall scGCL algorithm\n","  1. The Augment-Free Graph Representation Learning (AFGRL):\n","  This GNN model is used to capture the topographical information of the cell graph\n","    *  Layers: 2048\n","  \n","  2. The ZINB Autoencoder:\n","  This model is used to capture the characteristics of scRNA sequence data.\n","    *  Layers: 2048\n","\n","Both of these models combined are used to update the scGCL method that aims to reduce the similarity between two nodes on the cell graph.\n","\n","\n","\n","**Training objectives:**\n","  1. Optimizer: We use the AdamW optimizer method\n","  2. Loss function: The scGCL loss value is calculated from combining the contrast loss, ZINB loss and the reconstruction loss of scRNA-seq profiles. You can see this loss calculation in the formula below:\n","\n","  $L = γ_1L_{Contrast} + γ_2L_{ZINB} + γ_3L_{Reconstruct}$\n","\n","  where $γ_1$, $γ_2$ and $γ_3$ are hyperparameters that are used to control the loss.\n","\n","\n","\n","**Others:**\n","\n","There is a pretrained model saved but you can train another model with 1 epoch by running the training cells.\n","\n","The pre-trained model is stored in a file in the 'model_checkpoints' folder. The evaluate funtion is set to use this pre-trained model.\n","\n"],"metadata":{"id":"3muyDPFPbozY"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, num_dim, num_class):\n","        super().__init__()\n","        self.linear = nn.Linear(num_dim, num_class)\n","        torch.nn.init.xavier_uniform_(self.linear.weight.data)\n","        self.linear.bias.data.fill_(0.0)\n","        self.cross_entropy = nn.CrossEntropyLoss()\n","\n","    def forward(self, x, y):\n","        logits = self.linear(x)\n","        loss = self.cross_entropy(logits, y)\n","        return logits, loss"],"metadata":{"id":"FmZ5fq8tQDK5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168558,"user_tz":420,"elapsed":3,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"2d16202f-ad0c-46ad-ed51-435f110bb201"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.15 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"code","source":["torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","np.random.seed(0)\n","random.seed(0)\n","\n","# scGCL Model\n","# Revised freom Original version in AFGRL\n","# Ref:\n","# https://github.com/Namkyeong/AFGRL/tree/master/embedder.py\n","\n","class embedder:\n","    def __init__(self, args):\n","        self.args = args\n","        self.hidden_layers = eval(args.layers)\n","        printConfig(args)\n","\n","    def infer_embeddings(self, epoch):\n","        self._model.train(False)\n","        self._embeddings = self._labels = None\n","        self._train_mask = self._dev_mask = self._test_mask = None\n","\n","        for bc, batch_data in enumerate(self._loader):\n","            # augmentation = utils.Augmentation(float(self._args.aug_params[0]), float(self._args.aug_params[1]),\n","            #                                   float(self._args.aug_params[2]), float(self._args.aug_params[3]))\n","\n","            batch_data.to(self._device)\n","            # view1, view2 = augmentation._feature_masking(batch_data, self._device)\n","\n","            emb, loss = self._model(x = batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,\n","                                                                           neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],\n","                                                                           edge_weight=batch_data.edge_attr, epoch=epoch)\n","            emb = emb.detach()\n","            y = batch_data.y.detach()\n","            if self._embeddings is None:\n","                self._embeddings, self._labels = emb, y\n","            else:\n","                self._embeddings = torch.cat([self._embeddings, emb])\n","                self._labels = torch.cat([self._labels, y])\n","\n","\n","    def evaluate(self, task, epoch, sillog):\n","        if task == \"node\":\n","            self.evaluate_node(epoch)\n","        elif task == \"clustering\":\n","            self.evaluate_clustering(epoch,sillog)\n","        elif task == \"similarity\":\n","            self.run_similarity_search(epoch)\n","\n","\n","    def evaluate_node(self, epoch):\n","        emb_dim, num_class = self._embeddings.shape[1], self._labels.unique().shape[0]\n","\n","        dev_accs, test_accs = [], []\n","\n","        for i in range(20):\n","\n","            self._train_mask = self._dataset[0].train_mask[i]\n","            self._dev_mask = self._dataset[0].val_mask[i]\n","            if self._args.dataset == \"wikics\":\n","                self._test_mask = self._dataset[0].test_mask\n","            else:\n","                self._test_mask = self._dataset[0].test_mask[i]\n","\n","            classifier = LogisticRegression(emb_dim, num_class).to(self._device)\n","            optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)\n","\n","            for _ in range(100):\n","                classifier.train()\n","                logits, loss = classifier(self._embeddings[self._train_mask], self._labels[self._train_mask])\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","            dev_logits, _ = classifier(self._embeddings[self._dev_mask], self._labels[self._dev_mask])\n","            test_logits, _ = classifier(self._embeddings[self._test_mask], self._labels[self._test_mask])\n","            dev_preds = torch.argmax(dev_logits, dim=1)\n","            test_preds = torch.argmax(test_logits, dim=1)\n","\n","            dev_acc = (torch.sum(dev_preds == self._labels[self._dev_mask]).float() /\n","                       self._labels[self._dev_mask].shape[0]).detach().cpu().numpy()\n","            test_acc = (torch.sum(test_preds == self._labels[self._test_mask]).float() /\n","                        self._labels[self._test_mask].shape[0]).detach().cpu().numpy()\n","\n","            dev_accs.append(dev_acc * 100)\n","            test_accs.append(test_acc * 100)\n","\n","        dev_accs = np.stack(dev_accs)\n","        test_accs = np.stack(test_accs)\n","\n","        dev_acc, dev_std = dev_accs.mean(), dev_accs.std()\n","        test_acc, test_std = test_accs.mean(), test_accs.std()\n","\n","        print('** [{}] [Epoch: {}] Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(self.args.embedder, epoch, dev_acc, dev_std, test_acc, test_std))\n","\n","        if dev_acc > self.best_dev_acc:\n","            self.best_dev_acc = dev_acc\n","            self.best_test_acc = test_acc\n","            self.best_dev_std = dev_std\n","            self.best_test_std = test_std\n","            self.best_epoch = epoch\n","\n","        self.best_dev_accs.append(self.best_dev_acc)\n","        self.st_best = '** [Best epoch: {}] Best val | Best test: {:.4f} ({:.4f}) / {:.4f} ({:.4f})**\\n'.format(\n","            self.best_epoch, self.best_dev_acc, self.best_dev_std, self.best_test_acc, self.best_test_std)\n","        print(self.st_best)\n","\n","\n","    def evaluate_clustering(self, epoch,sillog):\n","        embeddings = F.normalize(self._embeddings, dim = -1, p = 2).detach().cpu().numpy()\n","\n","        nb_class = len(self._dataset[0].y.unique())\n","        true_y = self._dataset[0].y.detach().cpu().numpy()\n","\n","        estimator = KMeans(n_clusters = nb_class)\n","\n","        NMI_list = []\n","\n","        for i in range(10):\n","            estimator.fit(embeddings)\n","            y_pred = estimator.predict(embeddings)\n","            s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')\n","            NMI_list.append(s1)\n","        estimator.fit(embeddings)\n","        y_pred = estimator.predict(embeddings)\n","        silhid = metrics.silhouette_score(self._embeddings.detach().cpu().numpy(), y_pred, metric='euclidean')\n","        s1 = sum(NMI_list) / len(NMI_list)\n","        sillog.append(silhid)\n","        arr_sil = np.array(sillog)\n","\n","        print('** [{}] [Current Epoch {}] this epoch NMI values: {:.4f} ** and this epoch sil values: {}'.format(self.args.embedder, epoch, s1,silhid))\n","\n","        if math.floor(silhid*100) >= math.floor(self.best_dev_acc*100):\n","\n","            self.best_dev_acc = round(silhid, 2)\n","            self.best_embeddings = embeddings\n","            self.best_test_acc = s1\n","            print(\"~~~~~~~~~~~~~~~~~~\")\n","            print(silhid)\n","            if self._args.checkpoint_dir != '':\n","                print('Saving checkpoint...')\n","                torch.save(embeddings, os.path.join(self._args.checkpoint_dir,\n","                                                    'embeddings_{}_{}.pt'.format(self._args.dataset, self._args.task)))\n","                # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)\n","                a = pd.DataFrame(self.best_embeddings).T\n","                a.to_csv(\"/content/drive/MyDrive/CS598 DLH Project/results/scGCL-Tosches_turtle-euclidean.csv\")\n","            print(\"save\")\n","            print(\"~~~~~~~~~~~~~~~~~~\")\n","\n","\n","    def run_similarity_search(self, epoch):\n","        test_embs = self._embeddings.detach().cpu().numpy()\n","        test_lbls = self._dataset[0].y.detach().cpu().numpy()\n","        numRows = test_embs.shape[0]\n","\n","        cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)\n","        st = []\n","        for N in [5, 10]:\n","            indices = np.argsort(cos_sim_array, axis=1)[:, -N:]\n","            tmp = np.tile(test_lbls, (numRows, 1))\n","            selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)\n","            original_label = np.repeat(test_lbls, N).reshape(numRows,N)\n","            st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))\n","\n","        print(\"** [{}] [Current Epoch {}] sim@5 : {} | sim@10 : {} **\".format(self.args.embedder, epoch, st[0], st[1]))\n","\n","        if st[0] > self.best_dev_acc:\n","            self.best_dev_acc = st[0]\n","            self.best_test_acc = st[1]\n","            self.best_epoch = epoch\n","\n","        self.best_dev_accs.append(self.best_dev_acc)\n","        self.st_best = '** [Best epoch: {}] Best @5 : {} | Best @10: {} **\\n'.format(self.best_epoch, self.best_dev_acc, self.best_test_acc)\n","        print(self.st_best)\n","\n","        return st\n","\n","\n","class Encoder(nn.Module):\n","\n","    def __init__(self, layer_config, dropout=None, project=False, **kwargs):\n","        super().__init__()\n","        self.stacked_gnn = nn.ModuleList([GCNConv(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])\n","        # self.stacked_en = nn.ModuleList(\n","        #     [nn.Linear(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])\n","        self.stacked_bns = nn.ModuleList([nn.BatchNorm1d(layer_config[i], momentum=0.01) for i in range(1, len(layer_config))])\n","        self.stacked_prelus = nn.ModuleList([nn.ReLU() for _ in range(1, len(layer_config))])\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        #print(\"EMBEDDER ENCODER FORWARD\")\n","        for i, gnn in enumerate(self.stacked_gnn):\n","            x = gnn(x, edge_index, edge_weight=edge_weight)\n","            #x = gnn(x)\n","            x = self.stacked_bns[i](x)\n","            x = self.stacked_prelus[i](x)\n","\n","        return x"],"metadata":{"id":"rQFonakSKhzg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168706,"user_tz":420,"elapsed":150,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"e926b75a-ad75-4ae8-9685-b8ab886cc915"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 14.9 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"code","source":["np.random.seed(0)\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# scGCL Model\n","# Revised freom Original version in AFGRL\n","# Ref:\n","# https://github.com/Namkyeong/AFGRL/tree/master/models/AFGRL.py\n","\n","class AFGRL_ModelTrainer(embedder):\n","\n","    def __init__(self, args):\n","        embedder.__init__(self, args)\n","        self._args = args\n","        self._init()\n","        self.config_str = config2string(args)\n","        print(\"\\n[Config] {}\\n\".format(self.config_str))\n","        self.writer = SummaryWriter(log_dir=\"runs/{}\".format(self.config_str))\n","\n","    def _init(self):\n","        args = self._args\n","        self._task = args.task\n","        print(\"Downstream Task : {}\".format(self._task))\n","        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)\n","        # self._device = f'cuda:{args.device}' if torch.cuda.is_available() else \"cpu\"\n","        self._device = \"cpu\"\n","        # torch.cuda.set_device(self._device)\n","        self._dataset = Dataset(root=args.root, dataset=args.dataset)\n","        self._loader = DataLoader(dataset=self._dataset)\n","        layers = [self._dataset.data.x.shape[1]] + self.hidden_layers\n","        self._model = AFGRL(layers, args).to(self._device)\n","        self._optimizer = optim.AdamW(params=self._model.parameters(), lr=args.lr, weight_decay= 1e-5)\n","\n","    def train(self):\n","\n","        self.best_test_acc, self.best_dev_acc, self.best_test_std, self.best_dev_std, self.best_epoch = 0, 0, 0, 0, 0\n","        self.best_dev_accs = []\n","        self.best_embeddings = None\n","        sillog = []\n","        # get Random Initial accuracy\n","        self.infer_embeddings(0)\n","        print(\"initial accuracy \")\n","        self.evaluate(self._task, 0, sillog)\n","\n","        f_final = open(\"/content/drive/MyDrive/CS598 DLH Project/results/{}.txt\".format(self._args.embedder), \"a\")\n","\n","        # Start Model Training\n","        print(\"Training Start!\")\n","        self._model.train()\n","        for epoch in range(self._args.epochs):\n","            for bc, batch_data in enumerate(self._loader):\n","                batch_data.to(self._device)\n","                # view1, view2 = augmentation._feature_masking(batch_data, self._device)\n","\n","                emb, loss = self._model(x=batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,\n","                                     neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],\n","                                     edge_weight=batch_data.edge_attr, epoch=epoch)\n","\n","                self._optimizer.zero_grad()\n","                loss.backward()\n","                self._optimizer.step()\n","                self._model.update_moving_average()\n","\n","                st = '[{}][Epoch {}/{}] Loss: {:.4f}'.format(currentTime(), epoch, self._args.epochs, loss.item())\n","                print(st)\n","\n","            if (epoch) % 5 == 0:\n","                self.infer_embeddings(epoch)\n","                # self.evaluate(self._task, epoch)\n","                self.evaluate(self._task, epoch, sillog)\n","\n","\n","\n","        print(\"\\nTraining Done!\")\n","        self.st_best = '** [last epoch: {}] last NMI: {:.4f} **\\n'.format(self._args.epochs, self.best_test_acc)\n","        print(\"[Final] {}\".format(self.st_best))\n","        print('Saving checkpoint...')\n","        torch.save(self.best_embeddings, os.path.join(self._args.checkpoint_dir,\n","                                            'embeddings_{}_{}.pt'.format(self._args.dataset,\n","                                                                         self._args.task)))\n","        # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)\n","        a = pd.DataFrame(self.best_embeddings).T\n","        a.to_csv(\"/content/drive/MyDrive/CS598 DLH Project/results/scGCL-Tosches_turtle-euclidean.csv\")\n","        f_final.write(\"{} -> {}\\n\".format(self.config_str, self.st_best))\n","\n","class AFGRL(nn.Module):\n","    def __init__(self, layer_config, args, **kwargs):\n","        super().__init__()\n","        dec_dim = [512, 256]\n","        self.student_encoder = Encoder(layer_config=layer_config, dropout=args.dropout, **kwargs)\n","        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n","        set_requires_grad(self.teacher_encoder, False)\n","        self.teacher_ema_updater = EMA(args.mad, args.epochs)\n","        self.neighbor = Neighbor(args)\n","        rep_dim = layer_config[-1]\n","        rep_dim_o = layer_config[0]\n","        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, args.pred_hid), nn.BatchNorm1d(args.pred_hid), nn.ReLU(), nn.Linear(args.pred_hid, rep_dim), nn.ReLU())\n","        self.ZINB_Encoder = nn.Sequential(nn.Linear(rep_dim, dec_dim[0]), nn.ReLU(),\n","                                          nn.Linear(dec_dim[0], dec_dim[1]), nn.ReLU())\n","        self.pi_Encoder =  nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o),nn.Sigmoid())\n","        self.disp_Encoder = nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o), nn.Softplus())\n","        self.mean_Encoder = nn.Linear(dec_dim[1], rep_dim_o)\n","        self.student_predictor.apply(init_weights)\n","        self.relu = nn.ReLU()\n","        self.topk = args.topk\n","        # self._device = args.device\n","        self._device = \"cpu\"\n","    def clip_by_tensor(self,t, t_min, t_max):\n","        \"\"\"\n","        clip_by_tensor\n","        :param t: tensor\n","        :param t_min: min\n","        :param t_max: max\n","        :return: cliped tensor\n","        \"\"\"\n","        t = torch.tensor(t,dtype = torch.float32)\n","        t_min = torch.tensor(t_min,dtype = torch.float32)\n","        t_max = torch.tensor(t_max,dtype = torch.float32)\n","\n","        result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n","        result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n","        return result\n","\n","    def reset_moving_average(self):\n","        del self.teacher_encoder\n","        self.teacher_encoder = None\n","\n","    def update_moving_average(self):\n","        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n","        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n","\n","    def forward(self, x, y, edge_index, neighbor, edge_weight=None, epoch=None):\n","        student = self.student_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n","        # student_ = self.student_encoder(x=x2, edge_index=edge_index_2, edge_weight=edge_weight_2)\n","        pred = self.student_predictor(student)\n","        # pred_ = self.student_predictor(student_)\n","        z = self.ZINB_Encoder(student)\n","        pi = self.pi_Encoder(z)\n","        disp = self.disp_Encoder(z)\n","        disp = self.clip_by_tensor(disp,1e-4,1e4)\n","        mean = self.mean_Encoder(z)\n","        mean = self.clip_by_tensor(torch.exp(mean),1e-5,1e6)\n","        modify = 0\n","        with torch.no_grad():\n","            teacher = self.teacher_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n","        if edge_weight == None:\n","            adj = torch.sparse.FloatTensor(neighbor[0], torch.ones_like(neighbor[0][0]), [x.shape[0], x.shape[0]])\n","        else:\n","            adj = torch.sparse.FloatTensor(neighbor[0], neighbor[1], [x.shape[0], x.shape[0]])\n","        #\n","        ind, k = self.neighbor(adj, F.normalize(student, dim=-1, p=2), F.normalize(teacher, dim=-1, p=2), self.topk, epoch)\n","        zinb = ZINB(pi, theta=disp, ridge_lambda=0, debug=False)\n","        zinb_loss = zinb.loss(x, mean, mean=True)\n","\n","        loss1 = loss_fn(pred[ind[0]], teacher[ind[1]].detach())\n","        loss2 = loss_fn(pred[ind[1]], teacher[ind[0]].detach())\n","        recon_loss = torch.nn.MSELoss(reduction='mean')\n","        recon_loss_ = recon_loss(x,student)\n","        # adj_recon_ = recon_loss(adj.to_dense(),adj_recon)\n","        loss_reforce = (loss1 + loss2)\n","        if modify == 0:\n","            loss = zinb_loss + loss_reforce + recon_loss_\n","        elif modify == 1:\n","            loss = loss_reforce + recon_loss_\n","        elif modify == 2:\n","            loss = zinb_loss\n","        return student, loss.mean()\n","\n","class Neighbor(nn.Module):\n","    def __init__(self, args):\n","        super(Neighbor, self).__init__()\n","        # self.device = args.device\n","        self.device = \"cpu\"\n","        self.num_centroids = args.num_centroids\n","        self.num_kmeans = args.num_kmeans\n","        self.clus_num_iters = args.clus_num_iters\n","\n","    def __get_close_nei_in_back(self, indices, each_k_idx, cluster_labels, back_nei_idxs, k):\n","        # get which neighbors are close in the background set\n","        batch_labels = cluster_labels[each_k_idx][indices]\n","        top_cluster_labels = cluster_labels[each_k_idx][back_nei_idxs]\n","        batch_labels = repeat_1d_tensor(batch_labels, k)\n","\n","        curr_close_nei = torch.eq(batch_labels, top_cluster_labels)\n","        return curr_close_nei\n","\n","    def forward(self, adj, student, teacher, top_k, epoch):\n","        n_data, d = student.shape\n","        similarity = torch.matmul(student, torch.transpose(teacher, 1, 0).detach())\n","        similarity += torch.eye(n_data, device=self.device) * 10\n","\n","        _, I_knn = similarity.topk(k=top_k, dim=1, largest=True, sorted=True)\n","        tmp = torch.LongTensor(np.arange(n_data)).unsqueeze(-1).to(self.device)\n","\n","        knn_neighbor = self.create_sparse(I_knn)\n","        locality = knn_neighbor * adj\n","\n","        ncentroids = self.num_centroids\n","        niter = self.clus_num_iters\n","\n","        pred_labels = []\n","        # d_means = []\n","        for seed in range(self.num_kmeans):\n","            kmeans = faiss.Kmeans(d, ncentroids, niter=niter, gpu=False, seed=seed + 1234)\n","            kmeans.train(teacher.cpu().numpy())\n","            _, I_kmeans = kmeans.index.search(teacher.cpu().numpy(), 1)\n","\n","            clust_labels = I_kmeans[:,0]\n","            # d_means.append(D_kmeans)\n","            pred_labels.append(clust_labels)\n","        pred_labels = np.stack(pred_labels, axis=0)\n","        cluster_labels = torch.from_numpy(pred_labels).float()\n","\n","        all_close_nei_in_back = None\n","        with torch.no_grad():\n","            for each_k_idx in range(self.num_kmeans):\n","                curr_close_nei = self.__get_close_nei_in_back(tmp.squeeze(-1), each_k_idx, cluster_labels, I_knn, I_knn.shape[1])\n","\n","                if all_close_nei_in_back is None:\n","                    all_close_nei_in_back = curr_close_nei\n","                else:\n","                    all_close_nei_in_back = all_close_nei_in_back | curr_close_nei\n","\n","        all_close_nei_in_back = all_close_nei_in_back.to(self.device)\n","\n","        globality = self.create_sparse_revised(I_knn, all_close_nei_in_back)\n","\n","        pos_ = locality + globality\n","\n","        return pos_.coalesce()._indices(), I_knn.shape[1]\n","\n","    def create_sparse(self, I):\n","\n","        similar = I.reshape(-1).tolist()\n","        index = np.repeat(range(I.shape[0]), I.shape[1])\n","\n","        assert len(similar) == len(index)\n","        indices = torch.tensor([index, similar],dtype=torch.int32).to(self.device)\n","        result = torch.sparse_coo_tensor(indices, torch.ones_like(I.reshape(-1)), [I.shape[0], I.shape[0]])\n","\n","        return result\n","\n","    def create_sparse_revised(self, I, all_close_nei_in_back):\n","        n_data, k = I.shape[0], I.shape[1]\n","\n","        index = []\n","        similar = []\n","        for j in range(I.shape[0]):\n","            for i in range(k):\n","                index.append(int(j))\n","                similar.append(I[j][i].item())\n","\n","        index = torch.masked_select(torch.LongTensor(index).to(self.device), all_close_nei_in_back.reshape(-1))\n","        similar = torch.masked_select(torch.LongTensor(similar).to(self.device), all_close_nei_in_back.reshape(-1))\n","\n","        assert len(similar) == len(index)\n","        indices = torch.tensor([index.cpu().numpy().tolist(), similar.cpu().numpy().tolist()]).to(self.device)\n","        result = torch.sparse_coo_tensor(indices, torch.ones(len(index)).to(self.device), [n_data, n_data])\n","\n","        return result"],"metadata":{"id":"KI8a96_bORbk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168834,"user_tz":420,"elapsed":130,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"533eb5e7-a7ee-46a6-ce42-e7844474be95"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 13.5 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"code","source":["#ZINB.py\n","\n","def _nan2zero(x):\n","    return torch.where(torch.isnan(x), torch.zeros_like(x), x)\n","\n","def _nan2inf(x):\n","    return torch.where(torch.isnan(x), torch.zeros_like(x)+np.inf, x)\n","\n","def _nelem(x):\n","    nelem = torch.sum(torch.tensor(~torch.isnan(x),dtype = torch.float32))\n","    return torch.tensor(torch.where(torch.equal(nelem, 0.), 1., nelem), dtype = x.dtype)\n","\n","def _reduce_mean(x):\n","    nelem = _nelem(x)\n","    x = _nan2zero(x)\n","    return torch.divide(torch.sum(x), nelem)\n","\n","def mse_loss(y_true, y_pred):\n","    ret = torch.square(y_pred - y_true)\n","\n","    return _reduce_mean(ret)\n","\n","def poisson_loss(y_true, y_pred):\n","    y_pred = torch.tensor(y_pred, dtype = torch.float32)\n","    y_true = torch.tensor(y_true, dtype = torch.float32)\n","    nelem = _nelem(y_true)\n","    y_true = _nan2zero(y_true)\n","    ret = y_pred - y_true*torch.log(y_pred+1e-10) + torch.lgamma(y_true+1.0)\n","\n","    return torch.divide(torch.sum(ret), nelem)\n","\n","class NB(object):\n","    def __init__(self, theta=None, masking=False, scope='nbinom_loss/',\n","                 scale_factor=1.0, debug=False):\n","\n","        # for numerical stability\n","        self.eps = 1e-10\n","        self.scale_factor = scale_factor\n","        self.debug = debug\n","        self.scope = scope\n","        self.masking = masking\n","        self.theta = theta\n","\n","    def loss(self, y_true, y_pred, mean=True):\n","        scale_factor = self.scale_factor\n","        eps = self.eps\n","\n","        y_true = torch.tensor(y_true, dtype = torch.float32)\n","        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n","\n","        if self.masking:\n","            nelem = _nelem(y_true)\n","            y_true = _nan2zero(y_true)\n","\n","            # Clip theta\n","        theta = torch.minimum(self.theta,torch.tensor(1e6))\n","\n","        t1 = torch.lgamma(theta+eps) + torch.lgamma(y_true+1.0) - torch.lgamma(y_true+theta+eps)\n","        t2 = (theta+y_true) * torch.log(1.0 + (y_pred/(theta+eps))) + (y_true * (torch.log(theta+eps) - torch.log(y_pred+eps)))\n","\n","\n","        final = t1 + t2\n","\n","        final = _nan2inf(final)\n","\n","        if mean:\n","            if self.masking:\n","                final = torch.divide(torch.sum(final), nelem)\n","            else:\n","                final = torch.sum(final)\n","\n","\n","        return final\n","\n","class ZINB(NB):\n","    def __init__(self, pi, ridge_lambda=0.0, scope='zinb_loss/', **kwargs):\n","        super().__init__(scope=scope, **kwargs)\n","        self.pi = pi\n","        self.ridge_lambda = ridge_lambda\n","\n","    def loss(self, y_true, y_pred, mean=True):\n","        scale_factor = self.scale_factor\n","        eps = self.eps\n","\n","\n","            # reuse existing NB neg.log.lik.\n","            # mean is always False here, because everything is calculated\n","            # element-wise. we take the mean only in the end\n","        nb_case = super().loss(y_true, y_pred, mean=False) - torch.log(1.0-self.pi+eps)\n","\n","        y_true = torch.tensor(y_true, dtype = torch.float32)\n","        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n","        theta = torch.minimum(self.theta,torch.tensor(1e6))\n","\n","        zero_nb = torch.pow(theta/(theta+y_pred+eps), theta)\n","        zero_case = -torch.log(self.pi + ((1.0-self.pi)*zero_nb)+eps)\n","        result = torch.where(torch.less(y_true, 1e-8), zero_case, nb_case)\n","        ridge = self.ridge_lambda*torch.square(self.pi)\n","        result += ridge\n","\n","        if mean:\n","            if self.masking:\n","                result = _reduce_mean(result)\n","            else:\n","                result = torch.sum(result)\n","\n","        result = _nan2inf(result)\n","\n","        return result"],"metadata":{"id":"77f15u4oKZO-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948168834,"user_tz":420,"elapsed":3,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"3f85b1f2-6fe1-4120-96c4-93438e5ded40"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.1 ms (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"markdown","source":["## (5) Training"],"metadata":{"id":"k_cGXNgjM6Ie"}},{"cell_type":"markdown","source":["### (5.1) Hyperparameter Analysis"],"metadata":{"id":"8EAWAy_LwHlV"}},{"cell_type":"markdown","source":["Hyper-parameters we used and their values:\n","\n","*   Learning Rate: lr=0.001 (10^-3)\n","*   k-value (For K-NN method): k=15\n","*   Distance measurement algorithm: 'cosine'\n","\n","**Hyper-parameter analysis:**\n","\n","For hyperparameter analysis, the k and distances measures are used as metrics for comparison. In this experiment, only the baseline method scTAG uses the K-NN method to construct cell graphs. The distance measures were set to ‘cityblock’, ‘cosine’, ‘euclidean’, ‘L2’, and ‘manhattan’ while the k values were set to 5, 10, 15, 20, and 25.\n","\n","From the results, the ‘cosine’ distance measure had both the highest ARI and NMI for the Adam dataset. This means it captures the distance information/relationships between cells most effectively, which is explained due to the high sparsity of scRNA-seq data. Out of the k values, the ARI and NMI values are highest when k is set to 15. Additionally, scGCL always performs better than the baseline method scTAG across all distance measures and k values.\n","\n","Table 1. Imputation results by scGCL with different distance measures\n","![picture](https://drive.google.com/uc?id=1Sx2FwHMp2MWKr4YJLoMveGPhmO-9ZcpR)\n","\n","Table 2. Imputation results by scTAG with different distance measures\n","![picture](https://drive.google.com/uc?id=1k-v0YVeIG_slf9Y1YqATMvGxysHomtkK)\n","\n","\n","Table 3. Imputation results by scGCL with different k values\n","![picture](https://drive.google.com/uc?id=1jK3pvQCk_H9HhePlQ9GlaY-m6K7ydUIr)\n","\n","\n","Table 4. Imputation results by scTAG with different k values\n","![picture](https://drive.google.com/uc?id=1gxihue0XLR4e-mBvAoMupg8QSl0QzDni)\n","\n","\n","Average imputation results across all datasets\n","![picture](https://drive.google.com/uc?id=12GAOU96VDpt8Idiz5sSPfgwPP0Zfzcp8)\n","\n"],"metadata":{"id":"4GjEyLBIOXZw"}},{"cell_type":"markdown","source":["### (5.2) Computational requirements\n","\n","\n","\n","1.   Num of epochs used for training: 300\n","2.   Avg. run time of one epoch: ~24 seconds per epoch\n","3.   Total runtime for 300 epochs: 1 hour 59 minutes 46 seconds\n","4.   Type of hardware used: GPU (on google colab)\n","\n","\n","\n","For efficiency purposes, we have pre-trained the model and stored the checkpoint to use in evaluation. Executing the code below will train the model for 1 epoch for demonstration purposes.\n"],"metadata":{"id":"LREeENsZUmjn"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","random.seed(0)\n","np.random.seed(0)\n","\n","def main():\n","    args, unknown = parse_args()\n","\n","    if args.embedder == 'AFGRL':\n","        embedder = AFGRL_ModelTrainer(args)\n","\n","    embedder.train()\n","    embedder.writer.close()\n","\n","def imputation(file_path):\n","    args, unknown = utils.parse_args()\n","    imputation_m = torch.load(file_path).detach().data.cpu().numpy()\n","    a = pd.DataFrame(imputation_m).T\n","    a.to_csv(\"/content/drive/MyDrive/CS598 DLH Project/results/AFGRL-imputed-\" + args.dataset + \".csv\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"BapPqIue-WzR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948357324,"user_tz":420,"elapsed":188492,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"dac76c60-23f8-47f9-b8ff-709680a0bc5b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["['embedder', 'dataset', 'checkpoint_dir', 'root', 'task', 'layers', 'pred_hid', 'topk', 'clus_num_iters', 'num_centroids', 'num_kmeans', 'eval_freq', 'mad', 'lr', 'es', 'device', 'epochs', 'dropout', 'aug_params']\n","['AFGRL', 'adam', '/content/drive/MyDrive/CS598 DLH Project/model_checkpoints', 'data', 'clustering', '[2048]', 2048, 4, 20, 9, 5, 5, 0.9, 0.001, 300, 0, 1, 0.0, [0.3, 0.4, 0.3, 0.2]]\n","Downstream Task : clustering\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-f06465743278>:41: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n","  edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)\n"]},{"output_type":"stream","name":"stdout","text":["data/pyg/Adam/processed/data.pt\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Processing full batch data\n","\n","[Config] dataset_adam\n","\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-5b02be7cc96d>:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  t = torch.tensor(t,dtype = torch.float32)\n","<ipython-input-13-5b02be7cc96d>:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n","<ipython-input-13-5b02be7cc96d>:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n","<ipython-input-13-5b02be7cc96d>:151: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n","  adj = torch.sparse.FloatTensor(neighbor[0], neighbor[1], [x.shape[0], x.shape[0]])\n","<ipython-input-14-2a895cb5cb39>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_true = torch.tensor(y_true, dtype = torch.float32)\n","<ipython-input-14-2a895cb5cb39>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n","<ipython-input-14-2a895cb5cb39>:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_true = torch.tensor(y_true, dtype = torch.float32)\n","<ipython-input-14-2a895cb5cb39>:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n"]},{"output_type":"stream","name":"stdout","text":["initial accuracy \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["** [AFGRL] [Current Epoch 0] this epoch NMI values: 0.7946 ** and this epoch sil values: 0.26154962182044983\n","~~~~~~~~~~~~~~~~~~\n","0.26154962\n","Saving checkpoint...\n","save\n","~~~~~~~~~~~~~~~~~~\n","Training Start!\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-13-5b02be7cc96d>:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  t = torch.tensor(t,dtype = torch.float32)\n","<ipython-input-13-5b02be7cc96d>:122: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n","<ipython-input-13-5b02be7cc96d>:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n","<ipython-input-14-2a895cb5cb39>:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_true = torch.tensor(y_true, dtype = torch.float32)\n","<ipython-input-14-2a895cb5cb39>:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n","<ipython-input-14-2a895cb5cb39>:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_true = torch.tensor(y_true, dtype = torch.float32)\n","<ipython-input-14-2a895cb5cb39>:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n"]},{"output_type":"stream","name":"stdout","text":["[2024-04-12 18:58:05][Epoch 0/1] Loss: 3031691.5000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["** [AFGRL] [Current Epoch 0] this epoch NMI values: 0.7982 ** and this epoch sil values: 0.27419140934944153\n","~~~~~~~~~~~~~~~~~~\n","0.2741914\n","Saving checkpoint...\n","save\n","~~~~~~~~~~~~~~~~~~\n","\n","Training Done!\n","[Final] ** [last epoch: 1] last NMI: 0.7982 **\n","\n","Saving checkpoint...\n","time: 3min 8s (started: 2024-04-12 18:56:08 +00:00)\n"]}]},{"cell_type":"markdown","source":["## (6) Evaluation\n","\n","\n","\n"],"metadata":{"id":"tIL6kpRQUzO7"}},{"cell_type":"markdown","source":["The two metrics used to evaluate the performance of scGCL are adjusted rand index (ARI) and normalized mutual information (NMI.) These numbers are compared against four “state-of-the-art” baseline imputation methods: GraphSCI, scTAG, AutoClass, and MAGIC. The clustering results of scGCL on the Adam dataset outperforms all four of these methods, at 0.91 ARI and 0.89 NMI, respectively. From the paper itself including metrics from the other 13 datasets, the average ARI and NMI of scGCL across all datasets are 0.82 and 0.80 with the second best values of 0.80 and 0.75.\n","\n","![picture](https://drive.google.com/uc?id=1A_NayMELO5dLzEDAFJplTy9GWiwT7pmW)\n","Figure a. scGCL ARI results compared to other baseline methods\n","\n","\n","![picture](https://drive.google.com/uc?id=1ObVpExjWopnaCyVsjdI79WyZOdG07GLB)\n","Figure b. scGCLl NMI compared to other baseline methods\n","\n","\n","\n","You can run our evaluation code below. This code uses the pre-trained model stored in the 'model_checkpoints' folder\n","\n","Clustering NMI is the same as the NMI score.\n","Homogeneity score is the same as the ARI score."],"metadata":{"id":"fA43LNBkvkGl"}},{"cell_type":"code","source":["torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","random.seed(0)\n","\n","# scGCL Model\n","# Revised freom Original version in AFGRL\n","# Ref:\n","# https://github.com/Namkyeong/AFGRL/tree/master/evaluate.py\n","\n","def evaluate_node(embeddings, dataset, name):\n","\n","    labels = dataset.y\n","    emb_dim, num_class = embeddings.shape[1], dataset.y.unique().shape[0]\n","\n","    dev_accs, test_accs = [], []\n","\n","    for i in range(20):\n","\n","        train_mask = dataset.train_mask[i]\n","        dev_mask = dataset.val_mask[i]\n","        if name == \"wikics\":\n","            test_mask = dataset.test_mask\n","        else:\n","            test_mask = dataset.test_mask[i]\n","\n","        classifier = LogisticRegression(emb_dim, num_class)\n","        optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)\n","\n","        for _ in range(100):\n","            classifier.train()\n","            logits, loss = classifier(embeddings[train_mask], labels[train_mask])\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        dev_logits, _ = classifier(embeddings[dev_mask], labels[dev_mask])\n","        test_logits, _ = classifier(embeddings[test_mask], labels[test_mask])\n","        dev_preds = torch.argmax(dev_logits, dim=1)\n","        test_preds = torch.argmax(test_logits, dim=1)\n","\n","        dev_acc = (torch.sum(dev_preds == labels[dev_mask]).float() /\n","                       labels[dev_mask].shape[0]).detach().cpu().numpy()\n","        test_acc = (torch.sum(test_preds == labels[test_mask]).float() /\n","                        labels[test_mask].shape[0]).detach().cpu().numpy()\n","\n","        dev_accs.append(dev_acc * 100)\n","        test_accs.append(test_acc * 100)\n","\n","    dev_accs = np.stack(dev_accs)\n","    test_accs = np.stack(test_accs)\n","\n","    dev_acc, dev_std = dev_accs.mean(), dev_accs.std()\n","    test_acc, test_std = test_accs.mean(), test_accs.std()\n","\n","    print('Evaluate node classification results')\n","    print('** Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(dev_acc, dev_std, test_acc, test_std))\n","\n","\n","def evaluate_clustering(embeddings, dataset):\n","    x, edge_index, y = dataset[0]\n","    #print(y[1])\n","    embeddings = F.normalize(torch.from_numpy(embeddings), dim = -1, p = 2).detach().cpu().numpy()\n","    nb_class = len(y[1].unique())\n","    true_y = y[1].detach().cpu().numpy()\n","\n","    estimator = KMeans(n_clusters = nb_class)\n","\n","    NMI_list = []\n","    h_list = []\n","\n","    for i in range(10):\n","        estimator.fit(embeddings)\n","        y_pred = estimator.predict(embeddings)\n","\n","        h_score = metrics.homogeneity_score(true_y, y_pred)\n","        s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')\n","        NMI_list.append(s1)\n","        h_list.append(h_score)\n","\n","    s1 = sum(NMI_list) / len(NMI_list)\n","    h_score = sum(h_list) / len(h_list)\n","    print('Evaluate clustering results')\n","    print('** Clustering NMI: {:.4f} | homogeneity score: {:.4f} **'.format(s1, h_score))\n","\n","\n","def run_similarity_search(embeddings, dataset):\n","\n","    test_embs = embeddings.detach().cpu().numpy()\n","    test_lbls = dataset.y.detach().cpu().numpy()\n","    numRows = test_embs.shape[0]\n","\n","    cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)\n","    st = []\n","    for N in [5, 10]:\n","        indices = np.argsort(cos_sim_array, axis=1)[:, -N:]\n","        tmp = np.tile(test_lbls, (numRows, 1))\n","        selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)\n","        original_label = np.repeat(test_lbls, N).reshape(numRows,N)\n","        st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))\n","\n","    print('Evaluate similarity search results')\n","    print(\"** sim@5 : {} | sim@10 : {} **\".format(st[0], st[1]))"],"metadata":{"id":"6DI4V0UK-m31","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948357324,"user_tz":420,"elapsed":4,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"dd154406-07e2-4c57-e660-933866a54e16"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 10.8 ms (started: 2024-04-12 18:59:17 +00:00)\n"]}]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    embedding = torch.load(\"/content/drive/MyDrive/CS598 DLH Project/model_checkpoints/embeddings_adam_clustering_localtest.pt\")\n","    dataset = torch.load(\"/content/drive/MyDrive/CS598 DLH Project/data/processed/data.pt\")\n","    evaluate_clustering(embedding,dataset)"],"metadata":{"id":"lvdSPgdXHVnx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712948404925,"user_tz":420,"elapsed":47603,"user":{"displayName":"Angela Mei","userId":"01570793763863687077"}},"outputId":"f3abb80b-52da-4038-a141-4cb6feaf1238"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Evaluate clustering results\n","** Clustering NMI: 0.8353 | homogeneity score: 0.8294 **\n","time: 47.6 s (started: 2024-04-12 18:59:17 +00:00)\n"]}]},{"cell_type":"markdown","source":["# Results\n"],"metadata":{"id":"gX6bCcZNuxmz"}},{"cell_type":"markdown","source":["## (1) Analysis\n","\n","\n","These are the scores we get from running the trained model on the test dataset:\n","\n","*   NMI: 0.83   \n","*   ARI (Homogeneity): 0.82\n","\n","Our first Hypothesis was: \"Hypothesis: We hypothesize that scGCL will exhibit better clustering performance and imputation performance than other existing methods.\"\n","\n","From the table in the evaluation section, you can compare the NMI and ARI scores of the scGCL model and other existing models. However, from the evaluation scores, we can see that our hypothesis is not 100% true. Our trained model performs better than most existing methods but does not perform better than the existing scTAG method in both NMI and ARI scores. This deviates from the results presented by the original paper which reports much higher ARI and NMI scores. We believe this may have something to do with the randomness introduced into the model - the random values used in the original paper are different than ours, so we would have to experiment with different hyper-parameters to train a better model.\n","\n","Part of the next plans for the next phase is to train multiple iterations of the model to see if we can produce higher scores.\n","\n"],"metadata":{"id":"FvCugL3lGyfY"}},{"cell_type":"markdown","source":["## (2) Ablation Study"],"metadata":{"id":"SsVcMe2KveK6"}},{"cell_type":"markdown","source":["Although not implemented in our project, the paper also conducted ablation studies against the Adam dataset. Two key components were systematically removed to assess their impact on the system's performance:\n","\n","The first scenario involved the removal of the ZINB-based encoder while maintaining the AFGRL and graph convolutional models. This allowed us to observe how the system performs without the contribution of the ZINB encoder, focusing solely on the effects of the AFGRL framework and the graph convolutional approach.\n","\n","In the second scenario, the ZINB-based encoder was retained, but the graph convolution and AFGRL models were excluded. This setup aimed to isolate and evaluate the significance of the ZINB-based encoder by removing the influence of the other two components, offering insight into its standalone contribution to handling the datasets.\n","\n","As shown by the values in the below Table, the method performs better with the ZINB distribution included, which captures the global probability distribution of scRNA-seq data. For the Adam dataset, having both the ZINB-based encoder and AFGRL model produced the best results.\n","\n","\n","![picture](https://drive.google.com/uc?id=1psEUY5DRJA5H8FS9paVNAudAeCqoZX36)\n","Figure C. Ablation Study Metrics"],"metadata":{"id":"SG0_Lgt4vzzB"}},{"cell_type":"markdown","source":["# Discussion\n","\n","**Was the paper reproducible?**\n","\n","We found that the paper was reproducible, although with a few minor revisions. We needed to update a few deprecated functions to the new versions, as well as edit some code to make it work in a colab notebook.\n","\n","**What was easy and what wasn't**\n","\n","Easy: Tasks, including data preprocessing, clustering, training, etc. were intuitively split up and easy to dissect. Each one of these tasks were in their own .py file in the original repository\n","\n","Difficult: Parts of code were deprecated and many portions of code were not necessary. Not all datasets are readily or easily accessible either. Figuring out how the raw dataset is structured also was difficult due to the lack of documentation. Some annotations in the code were simply incorrect as they were copied from online tutorials.\n","\n","**Suggestions to make reproduction better**\n","\n","Better annotated code and more readily available data would help greatly\n","\n","**Plans for the next phase**\n","\n","*   Experiment with hyperparameters to increase performance\n","*   Add more detailed annotations\n","*   Clean up any unnecessary code\n","*   Transfer code to github repo\n","*   Create data download instructions\n","*   Video report\n","*   Fill in any missing write up requirements\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"qH75TNU71eRH"}},{"cell_type":"markdown","source":["# References\n","\n","1. Project Github Repository: https://github.com/juehoujhou4/CS598DLH-FinalProject\n","\n","2.  Zehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, Bo Wang, scGCL: an imputation method for scRNA-seq data based on graph contrastive learning, Bioinformatics, Volume 39, Issue 3, March 2023, btad098, https://doi.org/10.1093/bioinformatics/btad098\n","\n","3. Original Github Repository: https://github.com/zehaoxiong123/scGCL/tree/main\n"],"metadata":{"id":"SHMI2chl9omn"}}]}