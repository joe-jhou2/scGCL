# -*- coding: utf-8 -*-
"""DLH Team 5 Project Final Submission

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xt6CvyWiQBu_v8UwNq5uxTFqmuC2yxS8

Github Repository: https://github.com/juehoujhou4/CS598DLH-FinalProject

Video Presentation: https://drive.google.com/file/d/1NWSuSIKHH33VoLWO0wG8AwtK-E-SEnj4/view?usp=drive_link

# Introduction

## **scGCL: an imputation method for scRNA-seq data based on Graph Contrastive Learning**

### **Background**
The sparse nature of single-cell RNA sequencing (scRNA-seq) data arises from the biological and technical challenges encountered when measuring gene expression at the individual cell level. This sparsity indicates that numerous genes go undetected in the examined cells within the dataset. Several factors contribute to this phenomenon, including the complexities of capturing and analyzing the minute amounts of RNA present in single cells, such as:

1. Low RNA content- Low content makes it challenging to detect all mRNA molecules present; some cells naturally contain more mRNA molecules than others, which have more chance to detect the genes.
2. Low gene expression or gene expression variance- Natural differences in gene expression levels between cells, even within the same cell type, can contribute to the observed sparsity. Some genes are only expressed in specific cell states or conditions, leading to a large number of zeros in the data matrix where a particular gene is not active in most cells.
3. Technical variance- The process of isolating single cells, reverse transcribing RNA into cDNA, and amplifying the cDNA before sequencing introduces technical variability. Some mRNA molecules may be lost or degraded during these steps, resulting in incomplete detection of the transcriptome.
4. Dropout events- The most important factor contributing to sparsity, dropout events occur when mRNA molecules present in the cell are not detected, leading to a zero count for genes that are actually expressed but missed during sequencing. This is often due to inefficiencies in reverse transcription, amplification, or sequencing steps. Addressing dropout events through imputation in scRNA-seq data can significantly streamline the analysis process by filling in gaps where gene expression values are missing or undetected.

The importance of addressing this sparsity problem lies in several key areas:
1. Enhanced Data Quality: Sparsity in scRNA-seq data means that many gene expression readings are missing or near zero, leading to a dataset filled with a lot of noise and few signals. By effectively imputing these missing values, the quality of the data can be significantly enhanced, leading to more reliable and interpretable results.
2. Improved Biological Insights: The main goal of scRNA-seq analysis is to uncover the complex mechanisms of cellular processes and heterogeneity among cells in different states or environments. Addressing the sparsity issue allows for a more accurate identification of gene expression patterns, cell types, and developmental states, thereby facilitating deeper biological insights and discoveries.
3. Enabling Advanced Analysis: Many downstream analyses, such as clustering, trajectory inference, and differential expression analysis, require robust datasets without extensive missing values. By solving the sparsity problem, these analyses can be performed more effectively, leading to more nuanced understanding of cellular behavior and interactions.
4. Increased Comparability and Integration: As the field moves towards large-scale studies involving multiple datasets, the ability to accurately impute missing data becomes crucial for integrating and comparing datasets from different sources. This comparability is essential for drawing broader conclusions and for the reproducibility of findings across studies.

The difficulty of addressing the sparsity in scRNA-seq data include:
1. Lack of Ground Truth: In many cases, there is no "ground truth" for what the imputed values should be. This makes it difficult to train models and evaluate their performance objectively. The best that can be done is to rely on biological validation or downstream analysis outcomes, which can be time-consuming and not always definitive.
2. Data complexity: The high dimensionality of the data, coupled with the sparsity of gene expression (many genes are not expressed in many cells), makes it difficult to accurately impute missing values without introducing bias or losing critical information.
3. Noise: scRNA-seq data is also plagued by technical noise and dropout events, where genes are expressed but not detected due to limitations in sequencing depth or efficiency. Differentiating between true biological zeros and technical dropouts is a significant challenge.

The ideal approach we desired:
1. The approach not only diminishes noise in the data but also enhances the precision of clustering and classification efforts.
2. It bolsters the accuracy of differential expression analyses, and aids in the integration of data from various sources, thereby simplifying the overall complexity of handling scRNA-seq datasets.

### **Paper proposed**
The paper proposes a novel method, scGCL (single-cell Graph Contrastive Learning), specifically designed for imputing missing values in single-cell RNA sequencing (scRNA-seq) data. This method innovatively combines graph contrastive learning with the Zero-inflated Negative Binomial (ZINB) distribution model to estimate dropout events accurately. By employing contrastive learning within a graph theory framework, scGCL is adept at capturing the complex relationships between cells, thereby enhancing the prediction and reconstruction of missing gene expression values.


The innovation of the scGCL method lies in its unique approach to leveraging the strengths of graph contrastive learning, tailored specifically to the graph domain of scRNA-seq data. This allows for a sophisticated encapsulation of both global and local semantic information within the data. Furthermore, scGCL introduces a strategic selection of positive samples that significantly improve the representation of target nodes. This is complemented by the utilization of an autoencoder framework based on the ZINB distribution, specifically designed to model the global probability distribution of gene expression data effectively. This nuanced approach provides a robust solution to dropout imputation challenges, setting it apart from existing methods.


The contribution of scGCL to the research regime is substantial, addressing critical challenges outlined in the background of sparse data, technical noise, and the need for advanced computational strategies in scRNA-seq data analysis. By providing a more accurate and efficient method for imputing missing values, scGCL enhances the quality of scRNA-seq data analysis, leading to deeper biological insights and facilitating advancements in personalized medicine and genomics. This approach not only solves a significant problem in the field but also pushes the boundaries of what is possible with self-supervised learning in genomics, highlighting the importance of this paper to the ongoing research and development within the domain.

![picture](https://drive.google.com/uc?id=1bd4LYsKIcrAg7bcmHICGLksy7oFEuXQQ)

# Scope of Reproducibility

Hypotheses to be tested:

**Hypothesis 1**: We hypothesize that scGCL will exhibit better clustering performance and imputation performance than other existing methods.

# Methodology

## (1) Environment

The below two cells download and import the necessary packages for the model. The following is a list of versions for each necessary package that we used to recreate the original code. This code has only been tested in the Google colab environment.


*   Python: 3.10
*   numpy:  1.25.2
*   pandas:  2.0.3
*   sklearn:  1.2.2
*   h5py 3.9.0
*   scipy:  1.11.4
*   torch:  2.2.1+cu121
*   faiss 1.8.0
*   argparse:  1.1
*   anndata: 0.10.7
*   scanpy: 1.10.1
*   torch_geometric: 2.5.2
*   tensorboardX: 2.6.2.2
*   matplotlib: 3.7.1

We use the auto_time module to time each cell. This package is not necessary to run the model.

Please add the following folder as a shortcut on your 'My Drive' to access the project files: https://drive.google.com/drive/folders/1jRK-MyK9mvC5eoPjtSZ8qIbYdjCT6Lwl?usp=sharing

**This is necessary to run the notebook as it needs access to the data files and the pre-trained model.**

Please run the below cells to download and import the necessary packages:
"""

#import drive module from google drive package
# from google.colab import drive

#mount notebook to google drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#install necessary packages
!pip install scanpy
!pip install torch_geometric
!pip install tensorboardX
!pip install faiss-cpu

#install and load python autotime module to time each cell
!pip install ipython-autotime
# %load_ext autotime

# import packages you need
import sys
from tensorboardX import SummaryWriter
import os
import os.path as osp
import copy
import faiss
import argparse
from datetime import datetime

import csv
import cmath as cm
import h5py
from matplotlib import rcParams
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import math
from anndata import AnnData

import sklearn
from sklearn.neighbors import kneighbors_graph
from sklearn.decomposition import PCA
from sklearn.utils.class_weight import compute_class_weight
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics
from sklearn.metrics import normalized_mutual_info_score, pairwise, adjusted_rand_score,silhouette_score

import scanpy as sc
import scipy
import scipy.stats
from scipy import sparse as sp
from scipy.optimize import linear_sum_assignment
from scipy.sparse import csr_matrix

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import optim
from torch.autograd import Variable
from torch_geometric.data import InMemoryDataset, download_url, Data
from torch_geometric.io import read_npz
from torch_geometric.utils import remove_self_loops, to_undirected, dropout_adj
from torch_geometric.nn import GCNConv
from torch_geometric.loader import DataLoader
import torch_geometric.transforms as T
from torch_geometric.utils.loop import add_self_loops

"""## (2) Data

*   Source of the data: The paper actually runs the experiment on 14 different datasets. However, we will run our project on a single dataset which the paper has called "Adam" because it comes from this paper ([Adam et al. (2017)](https://journals.biologists.com/dev/article/144/19/3625/48196/Psychrophilic-proteases-dramatically-reduce-single)). The dataset is a set of RNA sequences that is taken from kidney cells of mice. This dataset contains data for 3,660 cells, 23,797 genes and 8 cell types. The dataset is available to download [here](https://github.com/zehaoxiong123/scGCL/tree/main/test_csv/Adam) from the original github repository.

*  A gene expression matrix is a matrix that contains cell samples and a gene. In our case, each cell is a row in the matrix and each gene is is a column. Each entry of the matrix is a numerical value that is >= 0 that represents the read counts of each gene in a particular cell. For example, the entry in row 2; col 6 represents the read count of gene 6 in cell sample 2.

*   Data Process: The expression matrix of the scRNA-seq data is taken as the input raw data. To reduce noise in the scRNA-seq data, the paper pre-process the raw gene expression profiles using the following pre-processing methods:

  1.   Data filtering and quality control are the first steps in scRNA-seq data pre-processing. Therefore, we only keep genes with non-zero expression in more than 1% of cells and cells with non-zero expression in more than 1% of genes.
  2.   Since the data in the count matrix are discrete and affected by the size factor, we normalize it by the size factor then transform discrete values through the log function. Finally, we select the top $t$ highly variable genes based on the normalized discrete values computed by the scanpy package.
  
    Generally, we select 2048 highly variable genes for training and use a consistent pre-processing method before running all baseline methods.


*   Illustration: ![picture](https://drive.google.com/uc?id=1ECCyhcVFhvKKIU3q-MM1EbTwvnjDlEJq)

### (2.1) Dataset Information

Run the cells below to download the dataset and print out basic statistics.
"""

# Paths for the new directories
results_path = '/content/results'
images_path = '/content/images'

# Create "results" directory if it doesn't exist
if not os.path.exists(results_path):
    os.makedirs(results_path)

# Create "images" directory if it doesn't exist
if not os.path.exists(images_path):
    os.makedirs(images_path)

# download data required for subsequential analysis
import gdown

# raw data downloading
url_data = "16OVBx4kZnYfrWnkc6gLWI9FZ1hd1b_Sf"

gdown.download(id=url_data, output="data.tgz", quiet=False)
!tar -xzf data.tgz

# model checkpoint downloading
url_model = "1iWA-wRjqR_cN8GeuFZAg9XyJ1NQglGpj"

gdown.download(id=url_model, output="model_checkpoints.tgz", quiet=False)
!tar -xzf model_checkpoints.tgz

# dir and function to load raw data
raw_data_dir = 'data_testing/test_csv/data.h5'

# load raw file
f = h5py.File(raw_data_dir, 'r')

data = np.array(f["exprs/data"])
indices = np.array(f["exprs/indices"])
indptr = np.array(f["exprs/indptr"])

matrix = csr_matrix((data, indices, indptr), shape=list(f["exprs/shape"]))

# list out organism
organism = str(list(f["obs/organism"])[0])[2:-1] # string slicing just to get rid of extra characters
print("Organism that provided cell samples: ", organism)

# list out lifestage it was taken
lifestage = str(list(f["obs/lifestage"])[0])[2:-1] # string slicing just to get rid of extra characters
print("Lifestage of when cell sample was taken: ", lifestage)

# list out organ taken sample from
organ = str(list(f["obs/organ"])[0])[2:-1] # string slicing just to get rid of extra characters
print("Organ that cell sample was taken from: ", organ)

# num of unique cell types
cell_types = list(set(list(f["obs/cell_type1"])))
cell_types = [str(cell)[2:-1] for cell in cell_types]

print("Num of unique cell types: ", len(cell_types))
# list out unique cell types
print("List of unique cell types: ", cell_types)

# num of cell samples
print("Num of cell samples (Rows): ", matrix.shape[0])
# num of genes
print("Num of genes (Columns): ", matrix.shape[1])

# print out sample row (A single cell sample)
print("This is an example row from the expression matrix:")
print(matrix.toarray()[0])

"""### (2.2) Data preprocess functions


"""

#Data preprocess functions:
#This cell defines functions that are used to preprocess the data
#This process can be seen in an illustration that is shown at the beginning of the data section


'''
  Preprocesses the raw data.
  Takes in raw datafile to filter and normalize data. Calls the later defined normalize function
  The actual filtering and normalization takes place in the called normalize funcion.
  Most of the function is reading the file and formatting into correct shape for the normalize function.
  Requires the anndata package and the h5py package to read in the h5 file
  # Arguments
      filename: filename of the raw datafile (in .h5 format)
      gene_num: number of genes that you want to process. Comes from an hyper-parameter (default for our model is 2048)
      raw: bool. Used to determine if the the input into the normalize function needs to normalized or not.
           Used only if the dataset is already normalized
  # Return
      count, adata.obs (anndata object that contains normalized matrix and annotations)
'''
def normalize_for_AF(filename,gene_num,raw, sparsify = False, skip_exprs = False):
    with h5py.File(filename, "r") as f:
        obs = np.array(f["obs_names"][...])
        var = np.array(f["var_names"][...])
        cell_name = np.array(f["obs"]["cell_type1"])
        cell_type, cell_label = np.unique(cell_name, return_inverse=True)
        class_num = np.array(cell_label).max() + 1
        data_label = []
        data_array = []
        for i in range(cell_label.shape[0]):
            x = np.zeros(class_num)
            x[cell_label[i]] = 1
            data_label.append(x)
        data_label = np.array(data_label)
        cell_type = np.array(cell_type)
        if not skip_exprs:
            exprs_handle = f["exprs"]
            if isinstance(exprs_handle, h5py.Group):
                mat = sp.csr_matrix((exprs_handle["data"][...], exprs_handle["indices"][...],
                                         exprs_handle["indptr"][...]), shape=exprs_handle["shape"][...])
            else:
                mat = exprs_handle[...].astype(np.float32)
                if sparsify:
                    mat = sp.csr_matrix(mat)
        else:
            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))
        X = np.array(mat.toarray())
        np.int = int
        X = np.ceil(X).astype(np.int)
        adata = sc.AnnData(X)
        adata.obs['Group'] = cell_label
        adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=raw, logtrans_input=True)
        count = adata.X
        return count,adata.obs['Group']

def nomalize_for_AD(file_name,label_path,gene_num):
    data = pd.read_csv(file_name, header=None, sep=",")
    label = pd.read_csv(label_path, header=0, sep=",")
    # data_label = []
    label = np.array(label)[:, 2]
    cell_type, cell_label = np.unique(label, return_inverse=True)
    data_label = []
    for i in range(len(cell_label)):
        data_label.append(cell_type[cell_label[i]])
    data_label = np.array(data_label)
    print(data_label)
    arr1 = np.array(data)
    gene_name = np.array(arr1[1:, 0])
    cell_name = np.array(arr1[0, 1:])
    X = arr1[1:, 1:].T
    adata = sc.AnnData(X)
    adata.obs['Group'] = data_label
    adata.obs['cell_name'] = cell_name
    adata.var['Gene'] = gene_name
    adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=False,
                      logtrans_input=True)
    count = adata.X
    return count, cell_label,adata.obs['size_factors'],adata.var['Gene']


"""
  Calculate clustering accuracy. Require scikit-learn installed
  # Arguments
      y: true labels, numpy.array with shape `(n_samples,)`
      y_pred: predicted labels, numpy.array with shape `(n_samples,)`
  # Return
      accuracy (in [0,1])
"""
def acc(y_true, y_pred):

    y_true = y_true.astype(np.int64)
    assert y_pred.size == y_true.size
    D = max(y_pred.max(), y_true.max()) + 1
    w = np.zeros((D, D), dtype=np.int64)
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1
    ind = linear_sum_assignment(w.max() - w)
    ind = np.array(ind).T
    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size



'''
  Filters and Normalizes a given dataset and returns the matrix
  Requires the anndata package
  # Arguments
      adata: filename of the raw datafile (in .h5 format)
      rest of the arguments are options for the normalization
  # Return
      adata (anndata object that contains normalized matrix and annotations)
'''
def normalize(adata, copy=True, highly_genes = None, filter_min_counts=True, size_factors=True, normalize_input=True, logtrans_input=True):
    if isinstance(adata, sc.AnnData):
        if copy:
            adata = adata.copy()
    elif isinstance(adata, str):
        adata = sc.read(adata)
    else:
        raise NotImplementedError

    norm_error = 'Make sure that the dataset (adata.X) contains unnormalized count data.'
    assert 'n_count' not in adata.obs, norm_error

    if adata.X.size < 50e6: # check if adata.X is integer only if array is small
        if sp.issparse(adata.X):
            assert (adata.X.astype(int) != adata.X).nnz == 0, norm_error
        else:
            assert np.all(adata.X.astype(int) == adata.X), norm_error

    if filter_min_counts:
        sc.pp.filter_genes(adata, min_counts=1)
        sc.pp.filter_cells(adata, min_counts=1)

    if size_factors or normalize_input or logtrans_input:
        adata.raw = adata.copy()
    else:
        adata.raw = adata

    if size_factors:
        # sc.pp.normalize_per_cell(adata)
        sc.pp.normalize_total(adata)
        adata.obs['size_factors'] = adata.obs.n_counts / np.median(adata.obs.n_counts)
    else:
        adata.obs['size_factors'] = 1.0

    if logtrans_input:
        sc.pp.log1p(adata)

    if highly_genes is not None:
        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, n_top_genes = highly_genes, subset=True)

    if normalize_input:
        sc.pp.scale(adata)

    return adata

"""## (3) Helper functions

The below functions are helper and utility functions that make generating the graph, data processing, and imputation easier. The last helper function defines the single cell class. These cells need to be run before training the model.

### (3.1) Graph based functions

Please see comments below for explanation on each function
"""

# preprocessing pipeline for graph-based data analysis

# this function constructs an adjacency matrix and its normalized count data
# Optionally applies PCA to reduce dimensionality before computing
# the k-nearest neighbors graph.
def get_adj(count, k=15, pca=50, mode="connectivity"):
    if pca:
        countp = dopca(count, dim=pca)
    else:
        countp = count
    A = kneighbors_graph(countp, k, mode=mode, metric="cosine", include_self=True)
    adj = A.toarray()
    adj_n = norm_adj(adj)
    return adj, adj_n

# This function computes a diagonal matrix where each diagonal element
# is the degree of the corresponding node in the graph
def degree_power(A, k):
    degrees = np.power(np.array(A.sum(1)), k).flatten()
    degrees[np.isinf(degrees)] = 0.
    if sp.issparse(A):
        D = sp.diags(degrees)
    else:
        D = np.diag(degrees)
    return D

# This function normalizes the adjacency matrix using the degree matrix.
def norm_adj(A):
    normalized_D = degree_power(A, -0.5)
    output = normalized_D.dot(A).dot(normalized_D)
    return output

# This function applies PCA to the data to reduce its dimensionality
def dopca(X, dim=10):
    pcaten = PCA(n_components=dim)
    X_10 = pcaten.fit_transform(X)
    return X_10

"""### (3.2) Imputation functions"""

# Revised from Original version in scVI
# Ref:
# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py

"""
  X: original testing set
  ========
  returns:
  X_zero: copy of X with zeros
  i, j, ix: indices of where dropout is applied
"""
def impute_dropout(X, seed=1, rate=0.1):


    # If the input is a dense matrix
    if isinstance(X, np.ndarray):
        X_zero = np.copy(X)
        # select non-zero subset
        i, j = np.nonzero(X_zero)
    # If the input is a sparse matrix
    else:
        X_zero = scipy.sparse.lil_matrix.copy(X)
        # select non-zero subset
        i, j = X_zero.nonzero()

    np.random.seed(seed)
    ix = np.random.choice(range(len(i)), int(
        np.floor(rate * len(i))), replace=False)
    X_zero[i[ix], j[ix]] = 0.0

    return X_zero, i, j, ix

# IMPUTATION METRICS
# Revised freom Original version in scVI
# Ref:
# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py


"""
  X_mean: imputed dataset
  X: original dataset
  X_zero: zeros dataset, does not need
  i, j, ix: indices of where dropout was applied
  ========
  returns:
  median L1 distance between datasets at indices given
"""
def imputation_error(X_mean, X, X_zero, i, j, ix):

    # If the input is a dense matrix
    if isinstance(X, np.ndarray):
        all_index = i[ix], j[ix]
        x, y = X_mean[all_index], X[all_index]
        result = np.abs(x - y)
    # If the input is a sparse matrix
    else:
        all_index = i[ix], j[ix]
        x = X_mean[all_index[0], all_index[1]]
        y = X[all_index[0], all_index[1]]
        yuse = scipy.sparse.lil_matrix.todense(y)
        yuse = np.asarray(yuse).reshape(-1)
        result = np.abs(x - yuse)
    # return np.median(np.abs(x - yuse))
    return np.mean(result), np.median(result), np.min(result), np.max(result)


"""
  X_mean: imputed dataset
  X: original dataset
  X_zero: zeros dataset, does not need
  i, j, ix: indices of where dropout was applied
  ========
  returns:
  cosine similarity between datasets at indices given
"""
def imputation_cosine(X_mean, X, X_zero, i, j, ix):

    # If the input is a dense matrix
    if isinstance(X, np.ndarray):
        all_index = i[ix], j[ix]
        x, y = X_mean[all_index], X[all_index]
        x = x.reshape(1, -1)
        y = y.reshape(1, -1)

        print(x)
        print(y)
        result = cosine_similarity(x, y)
    # If the input is a sparse matrix
    else:
        all_index = i[ix], j[ix]
        x = X_mean[all_index[0], all_index[1]]
        y = X[all_index[0], all_index[1]]
        yuse = scipy.sparse.lil_matrix.todense(y)
        yuse = np.asarray(yuse).reshape(-1)
        x = x.reshape(1, -1)
        yuse = yuse.reshape(1, -1)
        result = cosine_similarity(x, yuse)
    # return np.median(np.abs(x - yuse))
    return result[0][0]

"""### (3.3) Define Single Cell class"""

#This class defines a class for a single cell
#it initializes with a filepath to the raw dataset
#It then runs the process method and then uses that to create nodes and edges based on the data
class Singlecell(InMemoryDataset):

    def __init__(self, root, name, filepath, transform=None, pre_transform=None):
        self.name = name.lower()
        self.filepath = filepath
        super(Singlecell, self).__init__(root, transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_dir(self):
        return osp.join(self.root, self.name.capitalize(), 'raw')

    @property
    def processed_dir(self):
        return osp.join(self.root, self.name.capitalize(), 'processed')

    @property
    def raw_file_names(self):
        return 'amazon_electronics_{}.npz'.format(self.name.lower())

    @property
    def processed_file_names(self):
        return 'data.pt'

    def download(self):

        pass

    def process(self):

        raw = False
        data, data_label = normalize_for_AF(self.filepath, 2048,raw);

        x = torch.tensor(np.array(data),dtype=torch.float32)
        y = torch.tensor(data_label, dtype=torch.long)

        adj, adj_n = get_adj(data)
        adj = sp.coo_matrix(adj_n)
        edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)
        edge_index, _ = remove_self_loops(edge_index)
        edge_index = to_undirected(edge_index, x.size(0))  # Internal coalesce.
        data = Data(x=x, edge_index=edge_index, y=y)
        data = data if self.pre_transform is None else self.pre_transform(data)
        data, slices = self.collate([data])
        print(self.processed_paths[0])
        torch.save((data, slices), self.processed_paths[0])

    def __repr__(self):
        return '{}{}()'.format(self.__class__.__name__, self.name.capitalize())

"""### (3.4) Utility functions"""

#set seed for random function
#Even though the seed is the same as the paper, the random number generator is different
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

#Returns the current time using datetime
def currentTime():
    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

#This function parses the command line arguments
#In a notebook, there are no command line arguments but this function still returns the default values
#You can see all the options and their defaults to run the model
#Change the default values to vary the hyperparameters of the model
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--embedder", type=str, default="AFGRL")
    parser.add_argument("--dataset", type=str, default="adam", help="Name of the dataset. Supported names are: wikics, cs, computers, photo, and physics")

    parser.add_argument('--checkpoint_dir', type=str, default = '/content/model_checkpoints', help='directory to save checkpoint')
    parser.add_argument("--root", type=str, default="data")
    parser.add_argument("--task", type=str, default="clustering", help="Downstream task. Supported tasks are: node, clustering, similarity")

    parser.add_argument("--layers", nargs='?', default='[2048]', help="The number of units of each layer of the GNN. Default is [256]")
    parser.add_argument("--pred_hid", type=int, default=2048, help="The number of hidden units of layer of the predictor. Default is 512")

    parser.add_argument("--topk", type=int, default=4, help="The number of neighbors to search")
    parser.add_argument("--clus_num_iters", type=int, default=20)
    parser.add_argument("--num_centroids", type=int, default=9, help="The number of centroids for K-means Clustering")
    parser.add_argument("--num_kmeans", type=int, default=5, help="The number of K-means Clustering for being robust to randomness")
    parser.add_argument("--eval_freq", type=float, default=5, help="The frequency of model evaluation")
    parser.add_argument("--mad", type=float, default=0.9, help="Moving Average Decay for Teacher Network")
    parser.add_argument("--lr", type=float, default=0.001, help="learning rate")
    parser.add_argument("--es", type=int, default=300, help="Early Stopping Criterion")
    parser.add_argument("--device", type=int, default=0)
    parser.add_argument("--epochs", type=int, default=1) #For demonstration purposes, set to 1. Model has been pre-trained with 300 epochs.
    parser.add_argument("--dropout", type=float, default=0.0)
    parser.add_argument("--aug_params", "-p", nargs="+", default=[0.3, 0.4, 0.3, 0.2],help="Hyperparameters for augmentation (p_f1, p_f2, p_e1, p_e2). Default is [0.2, 0.1, 0.2, 0.3]")
    return parser.parse_known_args()

class Augmentation:

    def __init__(self, p_f1=0.2, p_f2=0.1, p_e1=0.2, p_e2=0.3):
        """
        two simple graph augmentation functions --> "Node feature masking" and "Edge masking"
        Random binary node feature mask following Bernoulli distribution with parameter p_f
        Random binary edge mask following Bernoulli distribution with parameter p_e
        """
        self.p_f1 = p_f1
        self.p_f2 = p_f2
        self.p_e1 = p_e1
        self.p_e2 = p_e2
        self.method = "BGRL"

    def _feature_masking(self, data, device):
        feat_mask1 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f1
        feat_mask2 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f2
        feat_mask1, feat_mask2 = feat_mask1.to(device), feat_mask2.to(device)
        x1, x2 = data.x.clone(), data.x.clone()
        x1, x2 = x1 * feat_mask1, x2 * feat_mask2

        edge_index1, edge_attr1 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e1)
        edge_index2, edge_attr2 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e2)

        new_data1, new_data2 = data.clone(), data.clone()
        new_data1.x, new_data2.x = x1, x2
        new_data1.edge_index, new_data2.edge_index = edge_index1, edge_index2
        new_data1.edge_attr, new_data2.edge_attr = edge_attr1, edge_attr2

        return new_data1, new_data2

    def __call__(self, data):
        return self._feature_masking(data)

def decide_config(root, dataset):
    """
    Create a configuration to download datasets
    :param root: A path to a root directory where data will be stored
    :param dataset: The name of the dataset to be downloaded
    :return: A modified root dir, the name of the dataset class, and parameters associated to the class
    """
    dataset = dataset.lower()
    if dataset == "adam":
        dataset = "Adam"
        root = osp.join(root, "pyg")
        filepath = '/content/data_testing/test_csv/data.h5'
        params = {"kwargs": {"root": root, "name": dataset, "filepath": filepath},
                  "name": dataset, "class": Singlecell, "src": "pyg"}
    else:
        raise Exception(
            f"Unknown dataset name {dataset}, name has to be one of the following 'cora', 'citeseer', 'pubmed', 'photo', 'computers', 'cs', 'physics'")
    return params


def create_dirs(dirs):
    """
    Create directories based on the directory structure provided
    This function takes a list of directory paths and creates the corresponding directory structure
    param: dirs (list): A list of directory paths where each path represents a directory structure

    """
    for dir_tree in dirs:
        sub_dirs = dir_tree.split("/")
        path = ""
        for sub_dir in sub_dirs:
            path = osp.join(path, sub_dir)
            os.makedirs(path, exist_ok=True)

def tensor2var(x, grad=False):
    """
    Convert a PyTorch tensor to a PyTorch Variable, optionally specifying gradient computation
    param: x (torch.Tensor): The input PyTorch tensor to convert
    param: grad (bool, optional): Whether to compute gradients for the variable. Defaults to False
    return: torch.autograd.Variable: The converted PyTorch Variable
    """
    if torch.cuda.is_available():
        x = x.cuda()
    return Variable(x, requires_grad=grad)

def create_masks(data):
    """
    Splits data into training, validation, and test splits in a stratified manner if
    it is not already splitted. Each split is associated with a mask vector, which
    specifies the indices for that split. The data will be modified in-place
    :param data: Data object
    :return: The modified data
    """
    if not hasattr(data, "val_mask"):

        data.train_mask = data.dev_mask = data.test_mask = None

        for i in range(20):
            labels = data.y.numpy()
            dev_size = int(labels.shape[0] * 0.1)
            test_size = int(labels.shape[0] * 0.8)

            perm = np.random.permutation(labels.shape[0])
            test_index = perm[:test_size]
            dev_index = perm[test_size:test_size + dev_size]

            data_index = np.arange(labels.shape[0])
            test_mask = torch.tensor(np.in1d(data_index, test_index), dtype=torch.bool)
            dev_mask = torch.tensor(np.in1d(data_index, dev_index), dtype=torch.bool)
            train_mask = ~(dev_mask + test_mask)
            test_mask = test_mask.reshape(1, -1)
            dev_mask = dev_mask.reshape(1, -1)
            train_mask = train_mask.reshape(1, -1)

            if 'train_mask' not in data:
                data.train_mask = train_mask
                data.val_mask = dev_mask
                data.test_mask = test_mask
            else:
                data.train_mask = torch.cat((data.train_mask, train_mask), dim=0)
                data.val_mask = torch.cat((data.val_mask, dev_mask), dim=0)
                data.test_mask = torch.cat((data.test_mask, test_mask), dim=0)

    else:  # in the case of WikiCS
        data.train_mask = data.train_mask.T
        data.val_mask = data.val_mask.T

    return data


class EMA:
    def __init__(self, beta, epochs):
        super().__init__()
        self.beta = beta
        self.step = 0
        self.total_steps = epochs

    def update_average(self, old, new):
        if old is None:
            return new

        beta = 1 - (1 - self.beta) * (np.cos(np.pi * self.step / self.total_steps) + 1) / 2.0
        self.step += 1
        return old * beta + (1 - beta) * new


def init_weights(m):
    """
    Initialize weights of a neural network module using Xavier uniform initialization
    Initialization and sets the bias to a value of 0.01
    param: m (nn.Module): The neural network module
    """

    if type(m) == nn.Linear:
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)


def loss_fn(x, y):
    """
    Calculate the cosine similarity loss between two vectors
    param: x (torch.Tensor): The first input tensor
    param: y (torch.Tensor): The second input tensor
    return: torch.Tensor: The cosine similarity loss
    """
    x = F.normalize(x, dim=-1, p=2)
    y = F.normalize(y, dim=-1, p=2)

    return 2 - 2 * (x * y).sum(dim=-1)


def l2_normalize(x):
    """
    Perform L2 normalization on a tensor
    param: x (torch.Tensor): The input tensor
    returns: torch.Tensor: The L2 normalized tensor
    """
    return x / torch.sqrt(torch.sum(x**2, dim=1).unsqueeze(1))


def update_moving_average(ema_updater, ma_model, current_model):
    """
    Update the moving average of a model's parameters
    This function updates the moving average parameters of 'ma_model' based on the current parameters
    of 'current_model' using the provided 'ema_updater' object
    param: ema_updater: The object responsible for updating the moving average
    param: ma_model (nn.Module): The model with the moving average parameters
    param: current_model (nn.Module): The model with the current parameters
    """
    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):
        old_weight, up_weight = ma_params.data, current_params.data
        ma_params.data = ema_updater.update_average(old_weight, up_weight)


def set_requires_grad(model, val):
    """
    Set the requires_grad attribute of all parameters in a given model
    param: model (nn.Module): The neural network model
    param: val (bool): The value to set for requires_grad
    """
    for p in model.parameters():
        p.requires_grad = val


def enumerateConfig(args):
    """
    Enumerate and return the names and values of attributes in an object
    param: args: An object containing attributes to enumerate
    return: tuple: A tuple containing lists of attribute names and values

    This function extracts the names and corresponding values of attributes from the input object
    """
    args_names = []
    args_vals = []
    for arg in vars(args):
        args_names.append(arg)
        args_vals.append(getattr(args, arg))

    return args_names, args_vals


def config2string(args):
    """
    This function converts specified attributes of the input object to a formatted string
    param: args: An object containing attributes
    return: str: A formatted string representing the attributes
    """
    args_names, args_vals = enumerateConfig(args)
    st = ''
    for name, val in zip(args_names, args_vals):
        if val == False:
            continue
        if name in ['dataset']:
            st_ = "{}_{}_".format(name, val)
            st += st_

    return st[:-1]


def printConfig(args):
    """
    Print the names and values of the givne args
    param: args: An object containing attributes
    """
    args_names, args_vals = enumerateConfig(args)
    print(args_names)
    print(args_vals)


def repeat_1d_tensor(t, num_reps):
    """
    This function repeats the input 1D tensor 'num_reps' times along a new dimension
    param: t (torch.Tensor): The input 1D tensor
    param: num_reps (int): The number of times to repeat the tensor
    return: torch.Tensor: The repeated tensor.
    """
    return t.unsqueeze(1).expand(-1, num_reps)

np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)

def download_pyg_data(config):
    """
    Downloads a dataset from the PyTorch Geometric library

    :param config: A dict containing info on the dataset to be downloaded
    :return: A tuple containing (root directory, dataset name, data directory)
    """
    leaf_dir = config["kwargs"]["root"].split("/")[-1].strip()
    data_dir = osp.join(config["kwargs"]["root"], "" if config["name"] == leaf_dir else config["name"])
    dst_path = osp.join(data_dir, "raw", "data.pt")
    if not osp.exists(dst_path):
        DatasetClass = config["class"]
        if config["name"] == "WikiCS":
            dataset = DatasetClass(data_dir, transform=T.NormalizeFeatures())
            size_factor = dataset.size_factors
        else :
            dataset = DatasetClass(**config["kwargs"])
        create_masks(data=dataset.data)
        torch.save((dataset.data, dataset.slices), dst_path)
    return config["kwargs"]["root"], config["name"], data_dir


def download_data(root, dataset):
    """
    Download data from different repositories. Currently only PyTorch Geometric is supported

    :param root: The root directory of the dataset
    :param name: The name of the dataset
    :return:
    """
    config = decide_config(root=root, dataset=dataset)
    if config["src"] == "pyg":
        return download_pyg_data(config)


class Dataset(InMemoryDataset):

    """
    A PyTorch InMemoryDataset to build multi-view dataset through graph data augmentation
    """

    def __init__(self, root="data", dataset='Adam', transform=None, pre_transform=None):
        self.root, self.dataset, self.data_dir = download_data(root=root, dataset=dataset)
        create_dirs(self.dirs)
        super().__init__(root=self.data_dir, transform=transform, pre_transform=pre_transform)
        self.process()
        path = osp.join(self.data_dir, "processed", self.processed_file_names[0])
        self.data, self.slices = torch.load(path)

    def process_full_batch_data(self, data):

        print("Processing full batch data")
        nodes = torch.tensor(np.arange(data.num_nodes), dtype=torch.long)

        edge_index, edge_attr = add_self_loops(data.edge_index, data.edge_attr)

        data = Data(nodes=nodes, edge_index=data.edge_index, edge_attr=data.edge_attr, x=data.x, y=data.y,
                    train_mask=data.train_mask, val_mask=data.val_mask, test_mask=data.test_mask,
                    num_nodes=data.num_nodes, neighbor_index=edge_index, neighbor_attr=edge_attr)

        return [data]

    def process(self):
        """
        Process either a full batch or cluster data.

        :return:
        """
        processed_path = osp.join(self.processed_dir, self.processed_file_names[0])
        if not osp.exists(processed_path):
            path = osp.join(self.raw_dir, self.raw_file_names[0])
            data, _ = torch.load(path)
            edge_attr = data.edge_attr
            edge_attr = torch.ones(data.edge_index.shape[1]) if edge_attr is None else edge_attr
            data.edge_attr = edge_attr


            data_list = self.process_full_batch_data(data)

            data, slices = self.collate(data_list)
            torch.save((data, slices), processed_path)


    @property
    def raw_file_names(self):
        return ["data.pt"]

    @property
    def processed_file_names(self):
        return [f'byg.data.pt']

    @property
    def raw_dir(self):
        return osp.join(self.data_dir, "raw")

    @property
    def processed_dir(self):
        return osp.join(self.data_dir, "processed")

    @property
    def model_dir(self):
        return osp.join(self.data_dir, "model")

    @property
    def result_dir(self):
        return osp.join(self.data_dir, "results")

    @property
    def dirs(self):
        return [self.raw_dir, self.processed_dir, self.model_dir, self.result_dir]

    def download(self):
        pass

"""##  (4) Model

**References:**

1.  Zehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, Bo Wang, scGCL: an imputation method for scRNA-seq data based on graph contrastive learning, Bioinformatics, Volume 39, Issue 3, March 2023, btad098, https://doi.org/10.1093/bioinformatics/btad098


2. Link to the original github repository: https://github.com/zehaoxiong123/scGCL/tree/main




**Model architectures:** There are two different models that make up the overall scGCL algorithm
  1. The Augment-Free Graph Representation Learning (AFGRL):
  This GNN model is used to capture the topographical information of the cell graph
    *  Layers: 2048
  
  2. The ZINB Autoencoder:
  This model is used to capture the characteristics of scRNA sequence data.
    *  Layers: 2048

Both of these models combined are used to update the scGCL method that aims to reduce the similarity between two nodes on the cell graph.



**Training objectives:**
  1. Optimizer: We use the AdamW optimizer method
  2. Loss function: The scGCL loss value is calculated from combining the contrast loss, ZINB loss and the reconstruction loss of scRNA-seq profiles. You can see this loss calculation in the formula below:

  $L = γ_1L_{Contrast} + γ_2L_{ZINB} + γ_3L_{Reconstruct}$

  where $γ_1$, $γ_2$ and $γ_3$ are hyperparameters that are used to control the loss.



**Others:**

There is a pretrained model saved but you can train another model with 1 epoch by running the training cells.

The pre-trained model is stored in a file in the 'model_checkpoints' folder. The evaluate funtion is set to use this pre-trained model.


"""

torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
random.seed(0)
np.random.seed(0)

class LogisticRegression(nn.Module):
    def __init__(self, num_dim, num_class):
        """
        This class defines a logistic regression model with a linear layer and cross-entropy loss.
        param: num_dim (int): Number of input dimensions.
        param: num_class (int): Number of classes.
        """
        super().__init__()
        self.linear = nn.Linear(num_dim, num_class)
        torch.nn.init.xavier_uniform_(self.linear.weight.data)
        self.linear.bias.data.fill_(0.0)
        self.cross_entropy = nn.CrossEntropyLoss()

    def forward(self, x, y):
        logits = self.linear(x)
        loss = self.cross_entropy(logits, y)
        return logits, loss

torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
np.random.seed(0)
random.seed(0)

# scGCL Model
# Revised freom Original version in AFGRL
# Ref:
# https://github.com/Namkyeong/AFGRL/tree/master/embedder.py

class embedder:
    """
    This class implements methods for inference and evaluation of embeddings
    This model defined here is a revised version of the AFGRL model
    params: args: args object that defines the parameters of the embeddings model
    """
    def __init__(self, args):
        self.args = args
        self.hidden_layers = eval(args.layers)
        printConfig(args)

    def infer_embeddings(self, epoch):
        self._model.train(False)
        self._embeddings = self._labels = None
        self._train_mask = self._dev_mask = self._test_mask = None

        for bc, batch_data in enumerate(self._loader):
            # augmentation = utils.Augmentation(float(self._args.aug_params[0]), float(self._args.aug_params[1]),
            #                                   float(self._args.aug_params[2]), float(self._args.aug_params[3]))

            batch_data.to(self._device)
            # view1, view2 = augmentation._feature_masking(batch_data, self._device)

            emb, loss = self._model(x = batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,
                                                                           neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],
                                                                           edge_weight=batch_data.edge_attr, epoch=epoch)
            emb = emb.detach()
            y = batch_data.y.detach()
            if self._embeddings is None:
                self._embeddings, self._labels = emb, y
            else:
                self._embeddings = torch.cat([self._embeddings, emb])
                self._labels = torch.cat([self._labels, y])


    def evaluate(self, task, epoch, sillog):
        if task == "node":
            self.evaluate_node(epoch)
        elif task == "clustering":
            self.evaluate_clustering(epoch,sillog)
        elif task == "similarity":
            self.run_similarity_search(epoch)


    def evaluate_node(self, epoch):
        emb_dim, num_class = self._embeddings.shape[1], self._labels.unique().shape[0]

        dev_accs, test_accs = [], []

        for i in range(20):

            self._train_mask = self._dataset[0].train_mask[i]
            self._dev_mask = self._dataset[0].val_mask[i]
            if self._args.dataset == "wikics":
                self._test_mask = self._dataset[0].test_mask
            else:
                self._test_mask = self._dataset[0].test_mask[i]

            classifier = LogisticRegression(emb_dim, num_class).to(self._device)
            optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)

            for _ in range(100):
                classifier.train()
                logits, loss = classifier(self._embeddings[self._train_mask], self._labels[self._train_mask])
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

            dev_logits, _ = classifier(self._embeddings[self._dev_mask], self._labels[self._dev_mask])
            test_logits, _ = classifier(self._embeddings[self._test_mask], self._labels[self._test_mask])
            dev_preds = torch.argmax(dev_logits, dim=1)
            test_preds = torch.argmax(test_logits, dim=1)

            dev_acc = (torch.sum(dev_preds == self._labels[self._dev_mask]).float() /
                       self._labels[self._dev_mask].shape[0]).detach().cpu().numpy()
            test_acc = (torch.sum(test_preds == self._labels[self._test_mask]).float() /
                        self._labels[self._test_mask].shape[0]).detach().cpu().numpy()

            dev_accs.append(dev_acc * 100)
            test_accs.append(test_acc * 100)

        dev_accs = np.stack(dev_accs)
        test_accs = np.stack(test_accs)

        dev_acc, dev_std = dev_accs.mean(), dev_accs.std()
        test_acc, test_std = test_accs.mean(), test_accs.std()

        print('** [{}] [Epoch: {}] Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(self.args.embedder, epoch, dev_acc, dev_std, test_acc, test_std))

        if dev_acc > self.best_dev_acc:
            self.best_dev_acc = dev_acc
            self.best_test_acc = test_acc
            self.best_dev_std = dev_std
            self.best_test_std = test_std
            self.best_epoch = epoch

        self.best_dev_accs.append(self.best_dev_acc)
        self.st_best = '** [Best epoch: {}] Best val | Best test: {:.4f} ({:.4f}) / {:.4f} ({:.4f})**\n'.format(
            self.best_epoch, self.best_dev_acc, self.best_dev_std, self.best_test_acc, self.best_test_std)
        print(self.st_best)


    def evaluate_clustering(self, epoch,sillog):
        embeddings = F.normalize(self._embeddings, dim = -1, p = 2).detach().cpu().numpy()

        nb_class = len(self._dataset[0].y.unique())
        true_y = self._dataset[0].y.detach().cpu().numpy()

        estimator = KMeans(n_clusters = nb_class)

        NMI_list = []

        for i in range(10):
            estimator.fit(embeddings)
            y_pred = estimator.predict(embeddings)
            s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')
            NMI_list.append(s1)
        estimator.fit(embeddings)
        y_pred = estimator.predict(embeddings)
        silhid = metrics.silhouette_score(self._embeddings.detach().cpu().numpy(), y_pred, metric='euclidean')
        s1 = sum(NMI_list) / len(NMI_list)
        sillog.append(silhid)
        arr_sil = np.array(sillog)

        print('** [{}] [Current Epoch {}] this epoch NMI values: {:.4f} ** and this epoch sil values: {}'.format(self.args.embedder, epoch, s1,silhid))

        if math.floor(silhid*100) >= math.floor(self.best_dev_acc*100):

            self.best_dev_acc = round(silhid, 2)
            self.best_embeddings = embeddings
            self.best_test_acc = s1
            print("~~~~~~~~~~~~~~~~~~")
            print(silhid)
            if self._args.checkpoint_dir != '':
                print('Saving checkpoint...')
                torch.save(embeddings, os.path.join(self._args.checkpoint_dir,
                                                    'embeddings_{}_{}.pt'.format(self._args.dataset, self._args.task)))
                # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)
                a = pd.DataFrame(self.best_embeddings).T
                a.to_csv("/content/results/scGCL-Tosches_turtle-euclidean.csv")
            print("save")
            print("~~~~~~~~~~~~~~~~~~")


    def run_similarity_search(self, epoch):
        test_embs = self._embeddings.detach().cpu().numpy()
        test_lbls = self._dataset[0].y.detach().cpu().numpy()
        numRows = test_embs.shape[0]

        cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)
        st = []
        for N in [5, 10]:
            indices = np.argsort(cos_sim_array, axis=1)[:, -N:]
            tmp = np.tile(test_lbls, (numRows, 1))
            selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)
            original_label = np.repeat(test_lbls, N).reshape(numRows,N)
            st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))

        print("** [{}] [Current Epoch {}] sim@5 : {} | sim@10 : {} **".format(self.args.embedder, epoch, st[0], st[1]))

        if st[0] > self.best_dev_acc:
            self.best_dev_acc = st[0]
            self.best_test_acc = st[1]
            self.best_epoch = epoch

        self.best_dev_accs.append(self.best_dev_acc)
        self.st_best = '** [Best epoch: {}] Best @5 : {} | Best @10: {} **\n'.format(self.best_epoch, self.best_dev_acc, self.best_test_acc)
        print(self.st_best)

        return st


class Encoder(nn.Module):
    """
    This class defines an encoder module for the graph neural network
    param: layer_config (list): Configuration of layers.
    param: dropout (float): Dropout rate.
    param: project (bool): Whether to project.
    """

    def __init__(self, layer_config, dropout=None, project=False, **kwargs):
        super().__init__()
        self.stacked_gnn = nn.ModuleList([GCNConv(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])
        # self.stacked_en = nn.ModuleList(
        #     [nn.Linear(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])
        self.stacked_bns = nn.ModuleList([nn.BatchNorm1d(layer_config[i], momentum=0.01) for i in range(1, len(layer_config))])
        self.stacked_prelus = nn.ModuleList([nn.ReLU() for _ in range(1, len(layer_config))])

    def forward(self, x, edge_index, edge_weight=None):
        for i, gnn in enumerate(self.stacked_gnn):
            x = gnn(x, edge_index, edge_weight=edge_weight)
            #x = gnn(x)
            x = self.stacked_bns[i](x)
            x = self.stacked_prelus[i](x)

        return x

np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# scGCL Model
# Revised freom Original version in AFGRL
# Ref:
# https://github.com/Namkyeong/AFGRL/tree/master/models/AFGRL.py

class AFGRL_ModelTrainer(embedder):
    """
    This class initializes an Embedder object and implements training functionality for AFGRL
    param: args: args object that defines the parameters of the embeddings model
    """
    def __init__(self, args):
        embedder.__init__(self, args)
        self._args = args
        self._init()
        self.config_str = config2string(args)
        print("\n[Config] {}\n".format(self.config_str))
        self.writer = SummaryWriter(log_dir="runs/{}".format(self.config_str))

    def _init(self):
        args = self._args
        self._task = args.task
        print("Downstream Task : {}".format(self._task))
        os.environ["CUDA_VISIBLE_DEVICES"] = str(args.device)
        # self._device = f'cuda:{args.device}' if torch.cuda.is_available() else "cpu"
        self._device = "cpu"
        # torch.cuda.set_device(self._device)
        self._dataset = Dataset(root=args.root, dataset=args.dataset)
        self._loader = DataLoader(dataset=self._dataset)
        layers = [self._dataset.data.x.shape[1]] + self.hidden_layers
        self._model = AFGRL(layers, args).to(self._device)
        self._optimizer = optim.AdamW(params=self._model.parameters(), lr=args.lr, weight_decay= 1e-5)

    def train(self):

        self.best_test_acc, self.best_dev_acc, self.best_test_std, self.best_dev_std, self.best_epoch = 0, 0, 0, 0, 0
        self.best_dev_accs = []
        self.best_embeddings = None
        sillog = []
        # get Random Initial accuracy
        self.infer_embeddings(0)
        print("initial accuracy ")
        self.evaluate(self._task, 0, sillog)

        f_final = open("/content/results/{}.txt".format(self._args.embedder), "a")

        # Start Model Training
        print("Training Start!")
        self._model.train()
        for epoch in range(self._args.epochs):
            for bc, batch_data in enumerate(self._loader):
                batch_data.to(self._device)
                # view1, view2 = augmentation._feature_masking(batch_data, self._device)

                emb, loss = self._model(x=batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,
                                     neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],
                                     edge_weight=batch_data.edge_attr, epoch=epoch)

                self._optimizer.zero_grad()
                loss.backward()
                self._optimizer.step()
                self._model.update_moving_average()

                st = '[{}][Epoch {}/{}] Loss: {:.4f}'.format(currentTime(), epoch, self._args.epochs, loss.item())
                print(st)

            if (epoch) % 5 == 0:
                self.infer_embeddings(epoch)
                # self.evaluate(self._task, epoch)
                self.evaluate(self._task, epoch, sillog)



        print("\nTraining Done!")
        self.st_best = '** [last epoch: {}] last NMI: {:.4f} **\n'.format(self._args.epochs, self.best_test_acc)
        print("[Final] {}".format(self.st_best))
        print('Saving checkpoint...')
        torch.save(self.best_embeddings, os.path.join(self._args.checkpoint_dir,
                                            'embeddings_{}_{}.pt'.format(self._args.dataset,
                                                                         self._args.task)))
        # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)
        a = pd.DataFrame(self.best_embeddings).T
        a.to_csv("/content/results/scGCL-Tosches_turtle-euclidean.csv")
        f_final.write("{} -> {}\n".format(self.config_str, self.st_best))

class AFGRL(nn.Module):
    """
    This class defines the AFGRL model architecture and operations
    param: layer_config (list): Configuration of layers
    param: args: args object that defines the parameters of the embeddings model
    """
    def __init__(self, layer_config, args, **kwargs):
        super().__init__()
        dec_dim = [512, 256]
        self.student_encoder = Encoder(layer_config=layer_config, dropout=args.dropout, **kwargs)
        self.teacher_encoder = copy.deepcopy(self.student_encoder)
        set_requires_grad(self.teacher_encoder, False)
        self.teacher_ema_updater = EMA(args.mad, args.epochs)
        self.neighbor = Neighbor(args)
        rep_dim = layer_config[-1]
        rep_dim_o = layer_config[0]
        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, args.pred_hid), nn.BatchNorm1d(args.pred_hid), nn.ReLU(), nn.Linear(args.pred_hid, rep_dim), nn.ReLU())
        self.ZINB_Encoder = nn.Sequential(nn.Linear(rep_dim, dec_dim[0]), nn.ReLU(),
                                          nn.Linear(dec_dim[0], dec_dim[1]), nn.ReLU())
        self.pi_Encoder =  nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o),nn.Sigmoid())
        self.disp_Encoder = nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o), nn.Softplus())
        self.mean_Encoder = nn.Linear(dec_dim[1], rep_dim_o)
        self.student_predictor.apply(init_weights)
        self.relu = nn.ReLU()
        self.topk = args.topk
        # self._device = args.device
        self._device = "cpu"
    def clip_by_tensor(self,t, t_min, t_max):
        """
        clip_by_tensor
        :param t: tensor
        :param t_min: min
        :param t_max: max
        :return: cliped tensor
        """
        t = torch.tensor(t,dtype = torch.float32)
        t_min = torch.tensor(t_min,dtype = torch.float32)
        t_max = torch.tensor(t_max,dtype = torch.float32)

        result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min
        result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max
        return result

    def reset_moving_average(self):
        del self.teacher_encoder
        self.teacher_encoder = None

    def update_moving_average(self):
        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'
        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)

    def forward(self, x, y, edge_index, neighbor, edge_weight=None, epoch=None):
        student = self.student_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)
        # student_ = self.student_encoder(x=x2, edge_index=edge_index_2, edge_weight=edge_weight_2)
        pred = self.student_predictor(student)
        # pred_ = self.student_predictor(student_)
        z = self.ZINB_Encoder(student)
        pi = self.pi_Encoder(z)
        disp = self.disp_Encoder(z)
        disp = self.clip_by_tensor(disp,1e-4,1e4)
        mean = self.mean_Encoder(z)
        mean = self.clip_by_tensor(torch.exp(mean),1e-5,1e6)
        modify = 0
        with torch.no_grad():
            teacher = self.teacher_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)
        if edge_weight == None:
            adj = torch.sparse.FloatTensor(neighbor[0], torch.ones_like(neighbor[0][0]), [x.shape[0], x.shape[0]])
        else:
            adj = torch.sparse.FloatTensor(neighbor[0], neighbor[1], [x.shape[0], x.shape[0]])
        #
        ind, k = self.neighbor(adj, F.normalize(student, dim=-1, p=2), F.normalize(teacher, dim=-1, p=2), self.topk, epoch)
        zinb = ZINB(pi, theta=disp, ridge_lambda=0, debug=False)
        zinb_loss = zinb.loss(x, mean, mean=True)

        loss1 = loss_fn(pred[ind[0]], teacher[ind[1]].detach())
        loss2 = loss_fn(pred[ind[1]], teacher[ind[0]].detach())
        recon_loss = torch.nn.MSELoss(reduction='mean')
        recon_loss_ = recon_loss(x,student)
        # adj_recon_ = recon_loss(adj.to_dense(),adj_recon)
        loss_reforce = (loss1 + loss2)
        if modify == 0:
            loss = zinb_loss + loss_reforce + recon_loss_
        elif modify == 1:
            loss = loss_reforce + recon_loss_
        elif modify == 2:
            loss = zinb_loss
        return student, loss.mean()

class Neighbor(nn.Module):
    """
    This class defines the Neighbor module for AFGRL
    param: args: args object that defines the parameters of the embeddings model
    """
    def __init__(self, args):
        super(Neighbor, self).__init__()
        # self.device = args.device
        self.device = "cpu"
        self.num_centroids = args.num_centroids
        self.num_kmeans = args.num_kmeans
        self.clus_num_iters = args.clus_num_iters

    def __get_close_nei_in_back(self, indices, each_k_idx, cluster_labels, back_nei_idxs, k):
        # get which neighbors are close in the background set
        batch_labels = cluster_labels[each_k_idx][indices]
        top_cluster_labels = cluster_labels[each_k_idx][back_nei_idxs]
        batch_labels = repeat_1d_tensor(batch_labels, k)

        curr_close_nei = torch.eq(batch_labels, top_cluster_labels)
        return curr_close_nei

    def forward(self, adj, student, teacher, top_k, epoch):
        n_data, d = student.shape
        similarity = torch.matmul(student, torch.transpose(teacher, 1, 0).detach())
        similarity += torch.eye(n_data, device=self.device) * 10

        _, I_knn = similarity.topk(k=top_k, dim=1, largest=True, sorted=True)
        tmp = torch.LongTensor(np.arange(n_data)).unsqueeze(-1).to(self.device)

        knn_neighbor = self.create_sparse(I_knn)
        locality = knn_neighbor * adj

        ncentroids = self.num_centroids
        niter = self.clus_num_iters

        pred_labels = []
        # d_means = []
        for seed in range(self.num_kmeans):
            kmeans = faiss.Kmeans(d, ncentroids, niter=niter, gpu=False, seed=seed + 1234)
            kmeans.train(teacher.cpu().numpy())
            _, I_kmeans = kmeans.index.search(teacher.cpu().numpy(), 1)

            clust_labels = I_kmeans[:,0]
            # d_means.append(D_kmeans)
            pred_labels.append(clust_labels)
        pred_labels = np.stack(pred_labels, axis=0)
        cluster_labels = torch.from_numpy(pred_labels).float()

        all_close_nei_in_back = None
        with torch.no_grad():
            for each_k_idx in range(self.num_kmeans):
                curr_close_nei = self.__get_close_nei_in_back(tmp.squeeze(-1), each_k_idx, cluster_labels, I_knn, I_knn.shape[1])

                if all_close_nei_in_back is None:
                    all_close_nei_in_back = curr_close_nei
                else:
                    all_close_nei_in_back = all_close_nei_in_back | curr_close_nei

        all_close_nei_in_back = all_close_nei_in_back.to(self.device)

        globality = self.create_sparse_revised(I_knn, all_close_nei_in_back)

        pos_ = locality + globality

        return pos_.coalesce()._indices(), I_knn.shape[1]

    def create_sparse(self, I):

        similar = I.reshape(-1).tolist()
        index = np.repeat(range(I.shape[0]), I.shape[1])

        assert len(similar) == len(index)
        indices = torch.tensor([index, similar],dtype=torch.int32).to(self.device)
        result = torch.sparse_coo_tensor(indices, torch.ones_like(I.reshape(-1)), [I.shape[0], I.shape[0]])

        return result

    def create_sparse_revised(self, I, all_close_nei_in_back):
        n_data, k = I.shape[0], I.shape[1]

        index = []
        similar = []
        for j in range(I.shape[0]):
            for i in range(k):
                index.append(int(j))
                similar.append(I[j][i].item())

        index = torch.masked_select(torch.LongTensor(index).to(self.device), all_close_nei_in_back.reshape(-1))
        similar = torch.masked_select(torch.LongTensor(similar).to(self.device), all_close_nei_in_back.reshape(-1))

        assert len(similar) == len(index)
        indices = torch.tensor([index.cpu().numpy().tolist(), similar.cpu().numpy().tolist()]).to(self.device)
        result = torch.sparse_coo_tensor(indices, torch.ones(len(index)).to(self.device), [n_data, n_data])

        return result

#ZINB.py


"""
Convert NaN values to zero in a tensor
param: x (torch.Tensor): Input tensor
return: torch.Tensor: Tensor with NaN values replaced by zeros
"""
def _nan2zero(x):
    return torch.where(torch.isnan(x), torch.zeros_like(x), x)

"""
Convert NaN values to inf in a tensor
param: x (torch.Tensor): Input tensor
return: torch.Tensor: Tensor with NaN values replaced by inf
"""
def _nan2inf(x):
    return torch.where(torch.isnan(x), torch.zeros_like(x)+np.inf, x)

"""
Count the number of non-NaN elements in a tensor
param: x (torch.Tensor): Input tensor
return: torch.Tensor: Number of non-NaN elements
"""
def _nelem(x):
    nelem = torch.sum(torch.tensor(~torch.isnan(x),dtype = torch.float32))
    return torch.tensor(torch.where(torch.equal(nelem, 0.), 1., nelem), dtype = x.dtype)

"""
Calculate the mean of non-NaN elements in a tensor
param: x (torch.Tensor): Input tensor
return: torch.Tensor: Mean value of non-NaN elements
"""
def _reduce_mean(x):
    nelem = _nelem(x)
    x = _nan2zero(x)
    return torch.divide(torch.sum(x), nelem)

"""
Calculate the Mean Squared Error (MSE) loss
param: y_true (torch.Tensor): True values tensor
param: y_pred (torch.Tensor): Predicted values tensor
return: torch.Tensor: MSE loss
"""
def mse_loss(y_true, y_pred):
    ret = torch.square(y_pred - y_true)

    return _reduce_mean(ret)

"""
Calculate the Poisson loss
param: y_true (torch.Tensor): True values tensor
param: y_pred (torch.Tensor): Predicted values tensor
return: torch.Tensor: Poisson loss
"""
def poisson_loss(y_true, y_pred):
    y_pred = torch.tensor(y_pred, dtype = torch.float32)
    y_true = torch.tensor(y_true, dtype = torch.float32)
    nelem = _nelem(y_true)
    y_true = _nan2zero(y_true)
    ret = y_pred - y_true*torch.log(y_pred+1e-10) + torch.lgamma(y_true+1.0)

    return torch.divide(torch.sum(ret), nelem)

"""
Defines the Negative Binomial (NB) loss class
param: theta (float): Theta parameter for NB distribution
param: masking (bool): Flag for masking NaN values
param: scope (str): Scope for loss
param: scale_factor (float): Scaling factor for loss
param: debug (bool): Debug flag
"""
class NB(object):
    def __init__(self, theta=None, masking=False, scope='nbinom_loss/',
                 scale_factor=1.0, debug=False):

        # for numerical stability
        self.eps = 1e-10
        self.scale_factor = scale_factor
        self.debug = debug
        self.scope = scope
        self.masking = masking
        self.theta = theta

    def loss(self, y_true, y_pred, mean=True):
        scale_factor = self.scale_factor
        eps = self.eps

        y_true = torch.tensor(y_true, dtype = torch.float32)
        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor

        if self.masking:
            nelem = _nelem(y_true)
            y_true = _nan2zero(y_true)

            # Clip theta
        theta = torch.minimum(self.theta,torch.tensor(1e6))

        t1 = torch.lgamma(theta+eps) + torch.lgamma(y_true+1.0) - torch.lgamma(y_true+theta+eps)
        t2 = (theta+y_true) * torch.log(1.0 + (y_pred/(theta+eps))) + (y_true * (torch.log(theta+eps) - torch.log(y_pred+eps)))


        final = t1 + t2

        final = _nan2inf(final)

        if mean:
            if self.masking:
                final = torch.divide(torch.sum(final), nelem)
            else:
                final = torch.sum(final)


        return final



"""
Defines the Zero-Inflated Negative Binomial (ZINB) loss class according the formula in the paper
param: pi (torch.Tensor): Probability tensor for zero inflation.
param: ridge_lambda (float): Ridge lambda parameter.
param: scope (str): Scope for loss.
"""
class ZINB(NB):
    def __init__(self, pi, ridge_lambda=0.0, scope='zinb_loss/', **kwargs):
        super().__init__(scope=scope, **kwargs)
        self.pi = pi
        self.ridge_lambda = ridge_lambda

    def loss(self, y_true, y_pred, mean=True):
        scale_factor = self.scale_factor
        eps = self.eps


            # reuse existing NB neg.log.lik.
            # mean is always False here, because everything is calculated
            # element-wise. we take the mean only in the end
        nb_case = super().loss(y_true, y_pred, mean=False) - torch.log(1.0-self.pi+eps)

        y_true = torch.tensor(y_true, dtype = torch.float32)
        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor
        theta = torch.minimum(self.theta,torch.tensor(1e6))

        zero_nb = torch.pow(theta/(theta+y_pred+eps), theta)
        zero_case = -torch.log(self.pi + ((1.0-self.pi)*zero_nb)+eps)
        result = torch.where(torch.less(y_true, 1e-8), zero_case, nb_case)
        ridge = self.ridge_lambda*torch.square(self.pi)
        result += ridge

        if mean:
            if self.masking:
                result = _reduce_mean(result)
            else:
                result = torch.sum(result)

        result = _nan2inf(result)

        return result

"""## (5) Training

### (5.1) Hyperparameter Analysis

Hyper-parameters we used and their values:

*   Learning Rate: lr=0.001 (10^-3)
*   k-value (For K-NN method): k=15
*   Distance measurement algorithm: 'cosine'

**Hyper-parameter analysis:**

For hyperparameter analysis, the k and distances measures are used as metrics for comparison. In this experiment, only the baseline method scTAG uses the K-NN method to construct cell graphs. The distance measures were set to ‘cityblock’, ‘cosine’, ‘euclidean’, ‘L2’, and ‘manhattan’ while the k values were set to 5, 10, 15, 20, and 25.

From the results, the ‘cosine’ distance measure had both the highest ARI and NMI for the Adam dataset. This means it captures the distance information/relationships between cells most effectively, which is explained due to the high sparsity of scRNA-seq data. Out of the k values, the ARI and NMI values are highest when k is set to 15. Additionally, scGCL always performs better than the baseline method scTAG across all distance measures and k values.

Table 1. Imputation results by scGCL with different distance measures
![picture](https://drive.google.com/uc?id=1Sx2FwHMp2MWKr4YJLoMveGPhmO-9ZcpR)

Table 2. Imputation results by scTAG with different distance measures
![picture](https://drive.google.com/uc?id=1k-v0YVeIG_slf9Y1YqATMvGxysHomtkK)


Table 3. Imputation results by scGCL with different k values
![picture](https://drive.google.com/uc?id=1jK3pvQCk_H9HhePlQ9GlaY-m6K7ydUIr)


Table 4. Imputation results by scTAG with different k values
![picture](https://drive.google.com/uc?id=1gxihue0XLR4e-mBvAoMupg8QSl0QzDni)


Average imputation results across all datasets
![picture](https://drive.google.com/uc?id=12GAOU96VDpt8Idiz5sSPfgwPP0Zfzcp8)

### (5.2) Computational requirements



1.   Num of epochs used for training: 300
2.   Avg. run time of one epoch: ~24 seconds per epoch
3.   Total runtime for 300 epochs: 1 hour 59 minutes 46 seconds
4.   Type of hardware used: GPU (on google colab)



For efficiency purposes, we have pre-trained the model and stored the checkpoint to use in evaluation. Executing the code below will train the model for 1 epoch for demonstration purposes.
"""

torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
random.seed(0)
np.random.seed(0)

"""
Main function to parse arguments and start model training.
"""
def main():
    args, unknown = parse_args()

    if args.embedder == 'AFGRL':
        embedder = AFGRL_ModelTrainer(args)

    embedder.train()
    embedder.writer.close()


"""
Function that performs the imputation and saves the result to a CSV file
param: file_path (str): Path to the file containing imputation data
return: file stored in the specified path that saves the result of the imputation
"""
def imputation(file_path):
    args, unknown = utils.parse_args()
    imputation_m = torch.load(file_path).detach().data.cpu().numpy()
    a = pd.DataFrame(imputation_m).T
    a.to_csv("/content/results/AFGRL-imputed-" + args.dataset + ".csv")


if __name__ == "__main__":
    main()

"""## (6) Evaluation

The two metrics used to evaluate the performance of scGCL are adjusted rand index (ARI) and normalized mutual information (NMI.) These numbers are compared against four “state-of-the-art” baseline imputation methods: GraphSCI, scTAG, AutoClass, and MAGIC. The clustering results of scGCL on the Adam dataset outperforms all four of these methods, at 0.91 ARI and 0.89 NMI, respectively (from original paper).

From the paper itself including metrics from the other 13 datasets, the average ARI and NMI of scGCL across all datasets are 0.82 and 0.80 with the second best values of 0.80 and 0.75.

![picture](https://drive.google.com/uc?id=1A_NayMELO5dLzEDAFJplTy9GWiwT7pmW)
Figure a. scGCL ARI results compared to other baseline methods


![picture](https://drive.google.com/uc?id=1ObVpExjWopnaCyVsjdI79WyZOdG07GLB)
Figure b. scGCLl NMI compared to other baseline methods



You can run our evaluation code below. This code uses the pre-trained model stored in the 'model_checkpoints' folder

Clustering NMI is the same as the NMI score.
Homogeneity score is the same as the ARI score.
"""

torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
random.seed(0)

# scGCL Model
# Revised freom Original version in AFGRL
# Ref:
# https://github.com/Namkyeong/AFGRL/tree/master/evaluate.py


"""
Evaluate node classification based on embeddings
prints out node classification results for validation and test sets
param: embeddings (tensor): Embeddings to evaluate
param: dataset (Dataset): Dataset containing node labels and masks
param: name (str): Name of the dataset
"""
def evaluate_node(embeddings, dataset, name):

    labels = dataset.y
    emb_dim, num_class = embeddings.shape[1], dataset.y.unique().shape[0]

    dev_accs, test_accs = [], []

    for i in range(20):

        train_mask = dataset.train_mask[i]
        dev_mask = dataset.val_mask[i]
        if name == "wikics":
            test_mask = dataset.test_mask
        else:
            test_mask = dataset.test_mask[i]

        classifier = LogisticRegression(emb_dim, num_class)
        optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)

        for _ in range(100):
            classifier.train()
            logits, loss = classifier(embeddings[train_mask], labels[train_mask])
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        dev_logits, _ = classifier(embeddings[dev_mask], labels[dev_mask])
        test_logits, _ = classifier(embeddings[test_mask], labels[test_mask])
        dev_preds = torch.argmax(dev_logits, dim=1)
        test_preds = torch.argmax(test_logits, dim=1)

        dev_acc = (torch.sum(dev_preds == labels[dev_mask]).float() /
                       labels[dev_mask].shape[0]).detach().cpu().numpy()
        test_acc = (torch.sum(test_preds == labels[test_mask]).float() /
                        labels[test_mask].shape[0]).detach().cpu().numpy()

        dev_accs.append(dev_acc * 100)
        test_accs.append(test_acc * 100)

    dev_accs = np.stack(dev_accs)
    test_accs = np.stack(test_accs)

    dev_acc, dev_std = dev_accs.mean(), dev_accs.std()
    test_acc, test_std = test_accs.mean(), test_accs.std()

    print('Evaluate node classification results')
    print('** Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(dev_acc, dev_std, test_acc, test_std))



"""
Evaluate clustering and return NMI and ARI scores
param: embeddings (tensor): Embeddings to evaluate.
param: dataset (Dataset): Dataset containing true labels.
print: clustering NMI score
print: clustering homogeniety(ARI) score
"""
def evaluate_clustering(embeddings, dataset):
    x, edge_index, y = dataset[0]
    embeddings = F.normalize(torch.from_numpy(embeddings), dim = -1, p = 2).detach().cpu().numpy()
    nb_class = len(y[1].unique())
    true_y = y[1].detach().cpu().numpy()

    estimator = KMeans(n_clusters = nb_class)

    NMI_list = []
    h_list = []

    for i in range(10):
        estimator.fit(embeddings)
        y_pred = estimator.predict(embeddings)

        h_score = metrics.homogeneity_score(true_y, y_pred)
        s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')
        NMI_list.append(s1)
        h_list.append(h_score)

    s1 = sum(NMI_list) / len(NMI_list)
    h_score = sum(h_list) / len(h_list)
    print('Evaluate clustering results')
    print('** Clustering NMI: {:.4f} | homogeneity score: {:.4f} **'.format(s1, h_score))

"""
Run similarity search based on embeddings
prints out cosine similarity scores
param: embeddings (tensor): Embeddings to perform similarity search
param: dataset (Dataset): Dataset containing true labels
"""
def run_similarity_search(embeddings, dataset):

    test_embs = embeddings.detach().cpu().numpy()
    test_lbls = dataset.y.detach().cpu().numpy()
    numRows = test_embs.shape[0]

    cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)
    st = []
    for N in [5, 10]:
        indices = np.argsort(cos_sim_array, axis=1)[:, -N:]
        tmp = np.tile(test_lbls, (numRows, 1))
        selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)
        original_label = np.repeat(test_lbls, N).reshape(numRows,N)
        st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))

    print('Evaluate similarity search results')
    print("** sim@5 : {} | sim@10 : {} **".format(st[0], st[1]))

if __name__ == "__main__":
    embedding = torch.load("/content/model_checkpoints/embeddings_adam_clustering_localtest.pt")
    dataset = torch.load("/content/data_testing/processed/data.pt")
    evaluate_clustering(embedding,dataset)

"""# Results

## (1) Analysis


These are the scores we get from running the trained model on the test dataset:

*   NMI: 0.84   
*   ARI (Homogeneity): 0.83

Our first Hypothesis was: "Hypothesis: We hypothesize that scGCL will exhibit better clustering performance and imputation performance than other existing methods."

From the table in the evaluation section, you can compare the NMI and ARI scores of the scGCL model and other existing models. However, from the evaluation scores, we can see that our hypothesis is not 100% true. Our trained model performs better than most existing methods but does not perform better than the existing scTAG method in both NMI and ARI scores. This deviates from the results presented by the original paper which reports much higher ARI and NMI scores. We believe this may have something to do with the randomness introduced into the model - the random values used in the original paper are different than ours, so we would have to experiment with different hyper-parameters to train a better model.

The original paper reports these scores on the test dataset:

*   NMI: 0.89   
*   ARI (Homogeneity): 0.91

As mentioned before, these scores are higher from the ones we get from our reproduced model. We think is is due to the randomness that is introduced into our model. The random number generator that we use in our environment (colab) is likely different from the one used in the original paper. This is likely the cause of the difference. If we had more time, we could run more iterations of the model while varying the hyper-parameters to try to increase the current performance.

## (2) Ablation Study

Although not implemented in our project, the paper also conducted ablation studies against the Adam dataset. Two key components were systematically removed to assess their impact on the system's performance:

The first scenario involved the removal of the ZINB-based encoder while maintaining the AFGRL and graph convolutional models. This allowed us to observe how the system performs without the contribution of the ZINB encoder, focusing solely on the effects of the AFGRL framework and the graph convolutional approach.

In the second scenario, the ZINB-based encoder was retained, but the graph convolution and AFGRL models were excluded. This setup aimed to isolate and evaluate the significance of the ZINB-based encoder by removing the influence of the other two components, offering insight into its standalone contribution to handling the datasets.

As shown by the values in the below Table, the method performs better with the ZINB distribution included, which captures the global probability distribution of scRNA-seq data. For the Adam dataset, having both the ZINB-based encoder and AFGRL model produced the best results.


![picture](https://drive.google.com/uc?id=1psEUY5DRJA5H8FS9paVNAudAeCqoZX36)
Figure C. Ablation Study Metrics

# Discussion

**Was the paper reproducible?**

We found that the paper was reproducible, although with a few minor revisions. We needed to update a few deprecated functions to the new versions, as well as edit some code to make it work in a colab notebook. Also, we could not get the same performance as the original paper with the listed hyper-parameters. We theorize that this is because of the random noise added into the model. Our (Colab's) random number generator is most likely different from the random number generator used in the original paper. Overall, we think the paper was fairly reporoducible, with minor revisions needed.

**What was easy and what wasn't**

Easy: Tasks, including data preprocessing, clustering, training, etc. were intuitively split up and easy to dissect. Each one of these tasks were in their own .py file in the original repository

Difficult: Parts of code were deprecated and many portions of code were not necessary. Not all datasets are readily or easily accessible either. Figuring out how the raw dataset is structured also was difficult due to the lack of documentation. Some annotations in the code were simply incorrect as they were copied from online tutorials.

**Suggestions to make reproduction better**

Better annotated code and more readily available data would help greatly. Also, updating the code to replace any deprecated functions with the updated version.

**Plans for the next phase**

*   Experiment with hyperparameters to increase performance
*   Add more detailed annotations
*   Clean up any unnecessary code
*   Transfer code to github repo
*   Create data download instructions
*   Video report
*   Fill in any missing write up requirements

# References

1. Project Github Repository: https://github.com/juehoujhou4/CS598DLH-FinalProject

2.  Zehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, Bo Wang, scGCL: an imputation method for scRNA-seq data based on graph contrastive learning, Bioinformatics, Volume 39, Issue 3, March 2023, btad098, https://doi.org/10.1093/bioinformatics/btad098

3. Original Github Repository: https://github.com/zehaoxiong123/scGCL/tree/main
"""