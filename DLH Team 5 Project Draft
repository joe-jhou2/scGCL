{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb","timestamp":1710432025769},{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **scGCL: an imputation method for scRNA-seq data based on Graph Contrastive Learning**\n","\n","### **Background**\n","The sparse nature of single-cell RNA sequencing (scRNA-seq) data arises from the biological and technical challenges encountered when measuring gene expression at the individual cell level. This sparsity indicates that numerous genes go undetected in the examined cells within the dataset. Several factors contribute to this phenomenon, including the complexities of capturing and analyzing the minute amounts of RNA present in single cells, such as:\n","\n","1. Low RNA content- Low content makes it challenging to detect all mRNA molecules present; some cells naturally contain more mRNA molecules than others, which have more chance to detect the genes.\n","2. Low gene expression or gene expression variance- Natural differences in gene expression levels between cells, even within the same cell type, can contribute to the observed sparsity. Some genes are only expressed in specific cell states or conditions, leading to a large number of zeros in the data matrix where a particular gene is not active in most cells.\n","3. Technical variance- The process of isolating single cells, reverse transcribing RNA into cDNA, and amplifying the cDNA before sequencing introduces technical variability. Some mRNA molecules may be lost or degraded during these steps, resulting in incomplete detection of the transcriptome.\n","4. Dropout events- The most important factor contributing to sparsity, dropout events occur when mRNA molecules present in the cell are not detected, leading to a zero count for genes that are actually expressed but missed during sequencing. This is often due to inefficiencies in reverse transcription, amplification, or sequencing steps. Addressing dropout events through imputation in scRNA-seq data can significantly streamline the analysis process by filling in gaps where gene expression values are missing or undetected.\n","\n","The importance of addressing this sparsity problem lies in several key areas:\n","1. Enhanced Data Quality: Sparsity in scRNA-seq data means that many gene expression readings are missing or near zero, leading to a dataset filled with a lot of noise and few signals. By effectively imputing these missing values, the quality of the data can be significantly enhanced, leading to more reliable and interpretable results.\n","2. Improved Biological Insights: The main goal of scRNA-seq analysis is to uncover the complex mechanisms of cellular processes and heterogeneity among cells in different states or environments. Addressing the sparsity issue allows for a more accurate identification of gene expression patterns, cell types, and developmental states, thereby facilitating deeper biological insights and discoveries.\n","3. Enabling Advanced Analysis: Many downstream analyses, such as clustering, trajectory inference, and differential expression analysis, require robust datasets without extensive missing values. By solving the sparsity problem, these analyses can be performed more effectively, leading to more nuanced understanding of cellular behavior and interactions.\n","4. Increased Comparability and Integration: As the field moves towards large-scale studies involving multiple datasets, the ability to accurately impute missing data becomes crucial for integrating and comparing datasets from different sources. This comparability is essential for drawing broader conclusions and for the reproducibility of findings across studies.\n","\n","The difficulty of addressing the sparsity in scRNA-seq data include:\n","1. Lack of Ground Truth: In many cases, there is no \"ground truth\" for what the imputed values should be. This makes it difficult to train models and evaluate their performance objectively. The best that can be done is to rely on biological validation or downstream analysis outcomes, which can be time-consuming and not always definitive.\n","2. Data complexity: The high dimensionality of the data, coupled with the sparsity of gene expression (many genes are not expressed in many cells), makes it difficult to accurately impute missing values without introducing bias or losing critical information.\n","3. Noise: scRNA-seq data is also plagued by technical noise and dropout events, where genes are expressed but not detected due to limitations in sequencing depth or efficiency. Differentiating between true biological zeros and technical dropouts is a significant challenge.\n","\n","The ideal approach we desired:\n","1. The approach not only diminishes noise in the data but also enhances the precision of clustering and classification efforts.\n","2. It bolsters the accuracy of differential expression analyses, and aids in the integration of data from various sources, thereby simplifying the overall complexity of handling scRNA-seq datasets.\n","\n","### **Paper proposed**\n","The paper proposes a novel method, scGCL (single-cell Graph Contrastive Learning), specifically designed for imputing missing values in single-cell RNA sequencing (scRNA-seq) data. This method innovatively combines graph contrastive learning with the Zero-inflated Negative Binomial (ZINB) distribution model to estimate dropout events accurately. By employing contrastive learning within a graph theory framework, scGCL is adept at capturing the complex relationships between cells, thereby enhancing the prediction and reconstruction of missing gene expression values.\n","\n","\n","The innovation of the scGCL method lies in its unique approach to leveraging the strengths of graph contrastive learning, tailored specifically to the graph domain of scRNA-seq data. This allows for a sophisticated encapsulation of both global and local semantic information within the data. Furthermore, scGCL introduces a strategic selection of positive samples that significantly improve the representation of target nodes. This is complemented by the utilization of an autoencoder framework based on the ZINB distribution, specifically designed to model the global probability distribution of gene expression data effectively. This nuanced approach provides a robust solution to dropout imputation challenges, setting it apart from existing methods.\n","\n","\n","The contribution of scGCL to the research regime is substantial, addressing critical challenges outlined in the background of sparse data, technical noise, and the need for advanced computational strategies in scRNA-seq data analysis. By providing a more accurate and efficient method for imputing missing values, scGCL enhances the quality of scRNA-seq data analysis, leading to deeper biological insights and facilitating advancements in personalized medicine and genomics. This approach not only solves a significant problem in the field but also pushes the boundaries of what is possible with self-supervised learning in genomics, highlighting the importance of this paper to the ongoing research and development within the domain."],"metadata":{"id":"lgIJyogv6pcv"}},{"cell_type":"markdown","source":["**Mount Notebook to Google Drive**\n","\n","Run the below cell to mount the notebook to google drive"],"metadata":{"id":"dlv6knX04FiY"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"sfk8Zrul_E8V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Scope of Reproducibility:\n","\n","Hypotheses to be tested:\n","\n","1. Hypothesis 1: We hypothesize that scGCL will exhibit better clustering performance and imputation performance than other existing methods.\n","2. Hypothesis 2: We hypothesize that scGCL can enhance differential gene expression analysis in disease datasets, such as Alzheimer’s disease or Huntington’s disease.\n"],"metadata":{"id":"uygL9tTPSVHB"}},{"cell_type":"markdown","source":["# Methodology\n","\n","## Environment\n","\n","The below two cells download and import the necessary packages for the model. The following is a list of versions for each necessary package that we used to recreate the original code. This code has only been tested in the Google colab environment.\n","\n","\n","\n","*   Python: 3.10\n","*   numpy:  1.25.2\n","*   pandas:  2.0.3\n","*   sklearn:  1.2.2\n","*   h5py 3.9.0\n","*   scipy:  1.11.4\n","*   torch:  2.2.1+cu121\n","*   faiss 1.8.0\n","*   argparse:  1.1\n","*   anndata: 0.10.7\n","*   scanpy: 1.10.1\n","*   torch_geometric: 2.5.2\n","*   tensorboardX: 2.6.2.2\n","*   matplotlib: 3.7.1\n","\n","\n","Please run the below two cells to download and import the necessary packages.\n","\n","\n"],"metadata":{"id":"xWAHJ_1CdtaA"}},{"cell_type":"code","source":["!pip install scanpy\n","!pip install torch_geometric\n","!pip install tensorboardX\n","!pip install faiss-cpu"],"metadata":{"id":"bdA8lWvyPCkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import packages you need\n","from google.colab import drive\n","import numpy as np\n","import pandas as pd\n","import random\n","import math\n","from anndata import AnnData\n","\n","import sklearn\n","from sklearn.neighbors import kneighbors_graph\n","from sklearn.decomposition import PCA\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.cluster import KMeans\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn import metrics\n","from sklearn.metrics import normalized_mutual_info_score, pairwise, adjusted_rand_score,silhouette_score\n","\n","import os\n","import os.path as osp\n","import csv\n","import cmath as cm\n","import h5py\n","from matplotlib import rcParams\n","import matplotlib.pyplot as plt\n","\n","import scanpy as sc\n","import scipy\n","import scipy.stats\n","from scipy import sparse as sp\n","from scipy.optimize import linear_sum_assignment\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.autograd import Variable\n","\n","from torch_geometric.data import InMemoryDataset, download_url, Data\n","from torch_geometric.io import read_npz\n","from torch_geometric.utils import remove_self_loops, to_undirected, dropout_adj\n","from torch_geometric.nn import GCNConv\n","from torch_geometric.loader import DataLoader\n","import torch_geometric.transforms as T\n","from torch_geometric.utils.loop import add_self_loops\n","\n","import sys\n","from tensorboardX import SummaryWriter\n","import os\n","import copy\n","import faiss\n","import argparse\n","from datetime import datetime"],"metadata":{"id":"yu61Jp1xrnKk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## **Data**\n","\n","*   Source of the data: The paper actually runs the experiment on 14 different datasets. However, we will run our project on a single dataset which the paper has called \"Adam\" because it comes from this paper ([Adam et al. (2017)](https://journals.biologists.com/dev/article/144/19/3625/48196/Psychrophilic-proteases-dramatically-reduce-single)). The dataset is a set of RNA sequences that is taken from kidney cells of mice. This dataset contains data for 3,660 cells, 23,797 genes and 8 cell types. The dataset is available to download [here](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE94333).\n","  \n","*   Data Process: The expression matrix of the scRNA-seq data is taken as the input raw data. To reduce noise in the scRNA-seq data, the paper pre-process the raw gene expression profiles using the following pre-processing methods:\n","\n","  1.   Data filtering and quality control are the first steps in scRNA-seq data pre-processing. Therefore, we only keep genes with non-zero expression in more than 1% of cells and cells with non-zero expression in more than 1% of genes.\n","  2.   Since the data in the count matrix are discrete and affected by the size factor, we normalize it by the size factor then transform discrete values through the log function. Finally, we select the top t highly variable genes based on the normalized discrete values computed by the scanpy package.\n","  \n","    Generally, we select 2048 highly variable genes for training and use a consistent pre-processing method before running all baseline methods.\n","\n","\n","\n","\n","*   Statistics: Run cell below to see statistics\n","*   Illustration: See below for plots of our data\n","\n","\n","\n"],"metadata":{"id":"XzVUQS0CHry0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZScZNbROw-N"},"outputs":[],"source":["from scipy.sparse import csr_matrix\n","\n","# dir and function to load raw data\n","raw_data_dir = '/content/drive/MyDrive/CS598 DLH Project/test_csv/data.h5'\n","\n","\n","f = h5py.File(raw_data_dir, 'r')\n","\n","data = np.array(f[\"exprs/data\"])\n","indices = np.array(f[\"exprs/indices\"])\n","indptr = np.array(f[\"exprs/indptr\"])\n","\n","matrix = csr_matrix((data, indices, indptr), shape=list(f[\"exprs/shape\"]))\n","\n","# list out organism\n","organism = str(list(f[\"obs/organism\"])[0])[2:-1] # string slicing just to get rid of extra characters\n","print(\"Organism that provided cell samples: \", organism)\n","\n","# list out lifestage it was taken\n","lifestage = str(list(f[\"obs/lifestage\"])[0])[2:-1]\n","print(\"Lifestage of when cell sample was taken: \", lifestage)\n","\n","# list out organ taken sample from\n","organ = str(list(f[\"obs/organ\"])[0])[2:-1]\n","print(\"Organ that cell sample was taken from: \", organ)\n","\n","# num of unique cell types\n","cell_types = list(set(list(f[\"obs/cell_type1\"])))\n","cell_types = [str(cell)[2:-1] for cell in cell_types]\n","\n","print(\"Num of unique cell types: \", len(cell_types))\n","# list out unique cell types\n","print(\"List of unique cell types: \", cell_types)\n","\n","# num of cell samples\n","print(\"Num of cell samples (Rows): \", matrix.shape[0])\n","# num of genes\n","print(\"Num of genes (Columns): \", matrix.shape[1])\n","\n","# print out sample row (A single cell sample)\n","print(\"This is an example row from the expression matrix:\")\n","print(matrix.toarray()[0])"]},{"cell_type":"code","source":["# graph function.py -- preprocessing pipeline for graph-based data analysis\n","\n","# this function constructs an adjacency matrix and its normalized count data\n","# Optionally applies PCA to reduce dimensionality before computing\n","# the k-nearest neighbors graph.\n","def get_adj(count, k=15, pca=50, mode=\"connectivity\"):\n","    if pca:\n","        countp = dopca(count, dim=pca)\n","    else:\n","        countp = count\n","    A = kneighbors_graph(countp, k, mode=mode, metric=\"cosine\", include_self=True)\n","    adj = A.toarray()\n","    adj_n = norm_adj(adj)\n","    return adj, adj_n\n","\n","# This function computes a diagonal matrix where each diagonal element\n","# is the degree of the corresponding node in the graph\n","def degree_power(A, k):\n","    degrees = np.power(np.array(A.sum(1)), k).flatten()\n","    degrees[np.isinf(degrees)] = 0.\n","    if sp.issparse(A):\n","        D = sp.diags(degrees)\n","    else:\n","        D = np.diag(degrees)\n","    return D\n","\n","# This function normalizes the adjacency matrix using the degree matrix.\n","def norm_adj(A):\n","    normalized_D = degree_power(A, -0.5)\n","    output = normalized_D.dot(A).dot(normalized_D)\n","    return output\n","\n","# This function applies PCA to the data to reduce its dimensionality\n","def dopca(X, dim=10):\n","    pcaten = PCA(n_components=dim)\n","    X_10 = pcaten.fit_transform(X)\n","    return X_10"],"metadata":{"id":"1P1vo3v0MVHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#data preprocess\n","'''\n","def read_cell_to_image(data_path,label_path,class_num):\n","    print(\"DATA_PREPROCESS READ CELL TO IMAGE\")\n","    data = pd.read_csv(data_path, header=None, sep=\",\")\n","    label = pd.read_csv(label_path, header=0, sep=\",\")\n","    arr1 = np.array(data)\n","    gene_name = np.array(arr1[1:, 0])\n","    cell_name = np.array(arr1[0, 1:])\n","    label = np.array(label)[:, 1]\n","    data_array = []\n","    data_for_count = []\n","    cell_num = np.shape(cell_name)[0]\n","    for i in range(cell_num):\n","        gene_list_for_count = np.array(arr1[1:, i + 1].astype('double'))\n","        gene_list_all = np.sum(gene_list_for_count)\n","        gene_list_median = np.median(gene_list_for_count)\n","        gene_list_for_count = gene_list_for_count*(gene_list_median/gene_list_all)\n","        data_for_count.append(gene_list_all/gene_list_median)\n","        gene_list_for_count = np.log2(gene_list_for_count+1)\n","        gene_list = gene_list_for_count.tolist()\n","        gene_len =  len(gene_list)\n","        figure_size = int(gene_len**0.5)\n","        if figure_size*figure_size == gene_len:\n","            data = np.array(gene_list).reshape(figure_size,figure_size,1).astype('double')\n","        else:\n","            for j in range((figure_size+1)**2-gene_len):\n","                gene_list.append(0)\n","            data =  np.array(gene_list).reshape(figure_size+1, figure_size+1, 1).astype('double')\n","        data_array.append(data)\n","    data_array = np.array(data_array)\n","    data_label = []\n","    for i in range(len(label)):\n","        x = np.zeros(class_num)\n","        x[int(label[i][5]) - 1] = 1\n","        data_label.append(x)\n","    data_label = np.array(data_label)\n","    return data_array,data_label,gene_name,cell_name,data_for_count\n","\n","\n","def read_cell(data_path,label_path,class_num):\n","    print(\"DATA_PREPROCESS READ CELL\")\n","    data = pd.read_csv(data_path, header=None, sep=\",\")\n","    label = pd.read_csv(label_path, header=0, sep=\",\")\n","    arr1 = np.array(data)\n","    gene_name = np.array(arr1[1:, 0])\n","    cell_name = np.array(arr1[0, 1:])\n","    label = np.array(label)[:, 1]\n","    data_array = []\n","    cell_num = np.shape(cell_name)[0]\n","    for i in range(cell_num):\n","        gene_list_for_count = np.array(arr1[1:, i + 1].astype('float64'))\n","        gene_list_all = np.sum(gene_list_for_count)\n","        gene_list_median = np.median(gene_list_for_count)\n","        gene_list_for_count = gene_list_for_count * (gene_list_median / gene_list_all)\n","        gene_list_for_count = np.log2(gene_list_for_count + 1)\n","        gene_list = gene_list_for_count.tolist()\n","        gene_len = len(gene_list)\n","        data_array.append(np.array(gene_list))\n","\n","    data_array = np.array(data_array)\n","    data_label = []\n","    for i in range(len(label)):\n","        x = np.zeros(class_num)\n","        x[int(label[i][5]) - 1] = 1\n","        data_label.append(x)\n","    data_label = np.array(data_label)\n","    print(data_array)\n","    return data_array, data_label,gene_name,cell_name\n","'''\n","\n","def normalize_for_AF(filename,gene_num,raw, sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS NORMALIZE FOR AF\")\n","    with h5py.File(filename, \"r\") as f:\n","        obs = np.array(f[\"obs_names\"][...])\n","        var = np.array(f[\"var_names\"][...])\n","        cell_name = np.array(f[\"obs\"][\"cell_type1\"])\n","        cell_type, cell_label = np.unique(cell_name, return_inverse=True)\n","        class_num = np.array(cell_label).max() + 1\n","        data_label = []\n","        data_array = []\n","        for i in range(cell_label.shape[0]):\n","            x = np.zeros(class_num)\n","            x[cell_label[i]] = 1\n","            data_label.append(x)\n","        data_label = np.array(data_label)\n","        cell_type = np.array(cell_type)\n","        if not skip_exprs:\n","            exprs_handle = f[\"exprs\"]\n","            if isinstance(exprs_handle, h5py.Group):\n","                mat = sp.csr_matrix((exprs_handle[\"data\"][...], exprs_handle[\"indices\"][...],\n","                                         exprs_handle[\"indptr\"][...]), shape=exprs_handle[\"shape\"][...])\n","            else:\n","                mat = exprs_handle[...].astype(np.float32)\n","                if sparsify:\n","                    mat = sp.csr_matrix(mat)\n","        else:\n","            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))\n","        X = np.array(mat.toarray())\n","        np.int = int\n","        X = np.ceil(X).astype(np.int)\n","        adata = sc.AnnData(X)\n","        adata.obs['Group'] = cell_label\n","        adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=raw, logtrans_input=True)\n","        count = adata.X\n","        # if raw == False:\n","        #     a = pd.DataFrame(count).T\n","        #     a.to_csv(\"./results/raw.csv\")\n","        return count,adata.obs['Group']\n","\n","def nomalize_for_AD(file_name,label_path,gene_num):\n","    print(\"DATA_PREPROCESS NORMALIZE FOR AD\")\n","    data = pd.read_csv(file_name, header=None, sep=\",\")\n","    label = pd.read_csv(label_path, header=0, sep=\",\")\n","    # data_label = []\n","    label = np.array(label)[:, 2]\n","    cell_type, cell_label = np.unique(label, return_inverse=True)\n","    data_label = []\n","    for i in range(len(cell_label)):\n","        data_label.append(cell_type[cell_label[i]])\n","    data_label = np.array(data_label)\n","    print(data_label)\n","    arr1 = np.array(data)\n","    gene_name = np.array(arr1[1:, 0])\n","    cell_name = np.array(arr1[0, 1:])\n","    X = arr1[1:, 1:].T\n","    adata = sc.AnnData(X)\n","    # print(cell_type.shape)\n","    adata.obs['Group'] = data_label\n","    adata.obs['cell_name'] = cell_name\n","    adata.var['Gene'] = gene_name\n","    adata = normalize(adata, copy=True, highly_genes=gene_num, size_factors=True, normalize_input=False,\n","                      logtrans_input=True)\n","    count = adata.X\n","    return count, cell_label,adata.obs['size_factors'],adata.var['Gene']\n","\n","'''\n","def create_adata_AD(file_name,file_label,original_file,funtion_name,file_res,gene_list):\n","    print(\"DATA_PREPROCESS CREATE ADATA AD\")\n","    data = pd.read_csv(file_name, header=None, sep=\",\")\n","    label_ = pd.read_csv(file_label, header=0, sep=\",\")\n","    label = np.array(label_)[:, 2]\n","    batch_label = np.array(label_)[:, 1]\n","\n","    cell_type, cell_label = np.unique(label, return_inverse=True)\n","    class_num = np.array(cell_label).max() + 1\n","    data_label = []\n","    for i in range(len(cell_label)):\n","        data_label.append(cell_type[cell_label[i]])\n","    data_label = np.array(data_label)\n","    X = np.array(data)[1:, 1:].T\n","    count_for = torch.tensor(X)\n","    count_for = F.normalize(count_for, dim=-1, p=2).detach().cpu().numpy()\n","    adata = sc.AnnData(count_for)\n","    adata.obs[funtion_name] = data_label\n","    _, _, _, gene =  nomalize_for_AD(original_file,file_label,2048)\n","    a = pd.DataFrame(gene)\n","    a.to_csv(gene_list)\n","    data_o = pd.read_csv(gene_list, header=0, sep=\",\")\n","    adata.var_names = np.array(data_o)[:,1]\n","    print(adata.var_names)\n","    adata.obs[funtion_name] = label\n","    adata.obs[\"batches\"] = batch_label\n","\n","    return adata, cell_type\n","'''\n","\n","def acc(y_true, y_pred):\n","    print(\"DATA_PREPROCESS ACC\")\n","    \"\"\"\n","    Calculate clustering accuracy. Require scikit-learn installed\n","    # Arguments\n","        y: true labels, numpy.array with shape `(n_samples,)`\n","        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n","    # Return\n","        accuracy, in [0,1]\n","    \"\"\"\n","    y_true = y_true.astype(np.int64)\n","    assert y_pred.size == y_true.size\n","    D = max(y_pred.max(), y_true.max()) + 1\n","    w = np.zeros((D, D), dtype=np.int64)\n","    for i in range(y_pred.size):\n","        w[y_pred[i], y_true[i]] += 1\n","    ind = linear_sum_assignment(w.max() - w)\n","    ind = np.array(ind).T\n","    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n","\n","'''\n","def dopca(X, dim=10):\n","    print(\"DATA_PREPROCESS DOPCA\")\n","    pcaten = PCA(n_components=dim)\n","    X_10 = pcaten.fit_transform(X)\n","    return X_10\n","'''\n","\n","def normalize(adata, copy=True, highly_genes = None, filter_min_counts=True, size_factors=True, normalize_input=True, logtrans_input=True):\n","    print(\"DATA_PREPROCESS NORMALIZE\")\n","    if isinstance(adata, sc.AnnData):\n","        if copy:\n","            adata = adata.copy()\n","    elif isinstance(adata, str):\n","        adata = sc.read(adata)\n","    else:\n","        raise NotImplementedError\n","    norm_error = 'Make sure that the dataset (adata.X) contains unnormalized count data.'\n","    assert 'n_count' not in adata.obs, norm_error\n","    if adata.X.size < 50e6: # check if adata.X is integer only if array is small\n","        if sp.issparse(adata.X):\n","            assert (adata.X.astype(int) != adata.X).nnz == 0, norm_error\n","        else:\n","            assert np.all(adata.X.astype(int) == adata.X), norm_error\n","\n","    if filter_min_counts:\n","        sc.pp.filter_genes(adata, min_counts=1)\n","        sc.pp.filter_cells(adata, min_counts=1)\n","    if size_factors or normalize_input or logtrans_input:\n","        adata.raw = adata.copy()\n","    else:\n","        adata.raw = adata\n","    if size_factors:\n","        sc.pp.normalize_per_cell(adata)\n","        adata.obs['size_factors'] = adata.obs.n_counts / np.median(adata.obs.n_counts)\n","\n","    else:\n","        adata.obs['size_factors'] = 1.0\n","    if logtrans_input:\n","        sc.pp.log1p(adata)\n","\n","    if highly_genes != None:\n","        sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5, n_top_genes = highly_genes, subset=True)\n","    if normalize_input:\n","        sc.pp.scale(adata)\n","    return adata\n","\n","'''\n","def read_cell_for_h5(filename,gene_num, sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS READ CELL FOR H5\")\n","    with h5py.File(filename, \"r\") as f:\n","        obs = np.array(f[\"obs_names\"][...])\n","        var = np.array(f[\"var_names\"][...])\n","        cell_name = np.array(f[\"obs\"][\"cell_type1\"])\n","        cell_type, cell_label = np.unique(cell_name, return_inverse=True)\n","        class_num = np.array(cell_label).max()+1\n","        data_label = []\n","        data_array = []\n","        for i in range(cell_label.shape[0]):\n","            x = np.zeros(class_num)\n","            x[cell_label[i]] = 1\n","            data_label.append(x)\n","        data_label = np.array(data_label)\n","        cell_type = np.array(cell_type)\n","        if not skip_exprs:\n","            exprs_handle = f[\"exprs\"]\n","            if isinstance(exprs_handle, h5py.Group):\n","                mat = sp.csr_matrix((exprs_handle[\"data\"][...], exprs_handle[\"indices\"][...],\n","                                               exprs_handle[\"indptr\"][...]), shape = exprs_handle[\"shape\"][...])\n","            else:\n","                mat = exprs_handle[...].astype(np.float32)\n","                if sparsify:\n","                    mat = sp.csr_matrix(mat)\n","        else:\n","            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))\n","        X = np.array(mat.toarray())\n","        print(X.shape)\n","        indicator = np.where(X > 0, 1, 0)\n","        sum_gene = np.sum(indicator, axis=0).flatten()\n","\n","        data = X[:, sum_gene > 10]\n","\n","        var_gene = np.var(data, axis=0)\n","        index = np.argsort(var_gene)[-gene_num:]\n","        data = data[:, index]\n","        # print(data.shape)\n","        scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n","        scaler = scaler.fit(data)\n","        data = scaler.fit_transform(data)\n","        print(X)\n","    return data,data_label,cell_label,cell_type,cell_type.shape[0],obs,var\n","'''\n","\n","'''\n","def read_cell_for_h5_to_csv(filename,rate,gene_num, output_file,sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS READ CELL FOR H5 TO CSV\")\n","    with h5py.File(filename, \"r\") as f:\n","        obs = np.array(f[\"obs_names\"][...])\n","        var = np.array(f[\"var_names\"][...])\n","        cell_name = np.array(f[\"obs\"][\"cell_type1\"])\n","        cell_type, cell_label = np.unique(cell_name, return_inverse=True)\n","        class_num = np.array(cell_label).max() + 1\n","        data_label = []\n","        data_count_array=[]\n","        data_array = []\n","        semi_label_index = []\n","        np.random.seed(1)\n","        semi_label = np.random.permutation(cell_name.shape[0])\n","        semi_label_index = int((1 - rate) * cell_name.shape[0])\n","        semi_label_train = semi_label[:semi_label_index]\n","        semi_label_real = cell_label[semi_label_train]\n","        weight_label_train = semi_label[semi_label_index+1:]\n","        cell_label[semi_label_train] = class_num\n","        class_weight = 'balanced'\n","        weight = compute_class_weight(class_weight, np.array(range(class_num)), cell_label[weight_label_train])\n","        for i in range(cell_label.shape[0]):\n","            x = np.zeros(class_num+1)\n","            x[cell_label[i]] = 1\n","            data_label.append(x)\n","        cell_type = np.array(cell_type)\n","        if not skip_exprs:\n","            exprs_handle = f[\"exprs\"]\n","            if isinstance(exprs_handle, h5py.Group):\n","                mat = sp.csr_matrix((exprs_handle[\"data\"][...], exprs_handle[\"indices\"][...],\n","                                               exprs_handle[\"indptr\"][...]), shape = exprs_handle[\"shape\"][...])\n","            else:\n","                mat = exprs_handle[...].astype(np.float32)\n","                if sparsify:\n","                    mat = sp.csr_matrix(mat)\n","        else:\n","            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))\n","        X = np.array(mat.toarray())\n","        print(X.shape)\n","        indicator = np.where(X > 0, 1, 0)\n","        sum_gene = np.sum(indicator, axis=0).flatten()\n","\n","        data = X[:, sum_gene>10]\n","\n","\n","        var_gene = np.var(data, axis=0)\n","        index = np.argsort(var_gene)[-gene_num:]\n","        data = data[:, index]\n","\n","        # data = scaler.fit_transform(data)\n","        for i in range(cell_label.shape[0]):\n","            gene_list_for_count = np.array(data[i,0:].astype('double'))\n","            # 把单细胞表达数据转化为图片\n","            data_array.append(gene_list_for_count)\n","\n","        a = pd.DataFrame(data_array).T\n","        a.to_csv(output_file)\n","'''\n","\n","'''\n","def read_cell_for_h5_to_image(filename,rate,gene_num, sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS READ CELL FOR H5 TO IMAGE\")\n","    with h5py.File(filename, \"r\") as f:\n","        obs = np.array(f[\"obs_names\"][...])\n","        var = np.array(f[\"var_names\"][...])\n","        cell_name = np.array(f[\"obs\"][\"cell_type1\"])\n","        cell_type, cell_label = np.unique(cell_name, return_inverse=True)\n","        class_num = np.array(cell_label).max() + 1\n","        data_label = []\n","        data_count_array=[]\n","        data_array = []\n","        semi_label_index = []\n","        np.random.seed(0)\n","        semi_label = np.random.permutation(cell_name.shape[0])\n","        # rate表示取多少标签进行训练\n","        semi_label_index = int((1 - rate) * cell_name.shape[0])\n","        semi_label_train = semi_label[:semi_label_index]\n","        #为了测试准确率用\n","        semi_label_real = cell_label[semi_label_train]\n","        weight_label_train = semi_label[semi_label_index+1:]\n","        cell_label[semi_label_train] = class_num\n","        class_weight = 'balanced'\n","        weight = compute_class_weight(class_weight, np.array(range(class_num)), cell_label[weight_label_train])\n","        for i in range(cell_label.shape[0]):\n","            x = np.zeros(class_num+1)\n","            x[cell_label[i]] = 1\n","            data_label.append(x)\n","        cell_type = np.array(cell_type)\n","        if not skip_exprs:\n","            exprs_handle = f[\"exprs\"]\n","            if isinstance(exprs_handle, h5py.Group):\n","                mat = sp.csr_matrix((exprs_handle[\"data\"][...], exprs_handle[\"indices\"][...],\n","                                               exprs_handle[\"indptr\"][...]), shape = exprs_handle[\"shape\"][...])\n","            else:\n","                mat = exprs_handle[...].astype(np.float32)\n","                if sparsify:\n","                    mat = sp.csr_matrix(mat)\n","        else:\n","            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))\n","        X = np.array(mat.toarray())\n","        print(X.shape)\n","        indicator = np.where(X > 0, 1, 0)\n","        sum_gene = np.sum(indicator, axis=0).flatten()\n","\n","        data = X[:, sum_gene>10]\n","\n","\n","        var_gene = np.var(data, axis=0)\n","        index = np.argsort(var_gene)[-gene_num:]\n","        data = data[:, index]\n","        # print(data.shape)\n","        scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n","        scaler = scaler.fit(data)\n","        data = scaler.fit_transform(data)\n","        for i in range(cell_label.shape[0]):\n","            gene_list_for_count = np.array(data[i,0:].astype('double'))\n","            # scRNAseq to image\n","            gene_list_max = np.max(gene_list_for_count)\n","            data_count_array.append(gene_list_max)\n","\n","            gene_list = gene_list_for_count.tolist()\n","            gene_len = len(gene_list)\n","            figure_size = int(gene_len ** 0.5)\n","            if figure_size * figure_size == gene_len:\n","                data_train = np.array(gene_list).reshape(figure_size, figure_size, 1).astype('double')\n","                figure_size_real = figure_size\n","            else:\n","                for j in range((figure_size + 1) ** 2 - gene_len):\n","                    gene_list.append(0)\n","                data_train = np.array(gene_list).reshape(figure_size + 1, figure_size + 1, 1).astype('double')\n","                figure_size_real = figure_size + 1\n","            data_array.append(data_train)\n","\n","        test_data = np.array(data_array)[semi_label_train]\n","    return data_array,data_label,cell_type,cell_type.shape[0],obs,var,figure_size_real,data_count_array,weight,scaler,semi_label_real,test_data\n","'''\n","\n","'''\n","def read_cell_for_h5_pre_clustering(filename,rate,gene_num,cluster_num, sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS READ CELL FOR H5 PRE CLUSTERING\")\n","    with h5py.File(filename, \"r\") as f:\n","        obs = np.array(f[\"obs_names\"][...])\n","        var = np.array(f[\"var_names\"][...])\n","        data_count_array = []\n","        data_array = []\n","        if not skip_exprs:\n","            exprs_handle = f[\"exprs\"]\n","            if isinstance(exprs_handle, h5py.Group):\n","                mat = sp.csr_matrix((exprs_handle[\"data\"][...], exprs_handle[\"indices\"][...],\n","                                         exprs_handle[\"indptr\"][...]), shape=exprs_handle[\"shape\"][...])\n","            else:\n","                mat = exprs_handle[...].astype(np.float32)\n","                if sparsify:\n","                    mat = sp.csr_matrix(mat)\n","        else:\n","            mat = sp.csr_matrix((obs.shape[0], var.shape[0]))\n","        X = np.array(mat.toarray())\n","        print(X.shape)\n","        indicator = np.where(X > 0, 1, 0)\n","        sum_gene = np.sum(indicator, axis=0).flatten()\n","\n","        data = X[:, sum_gene > 10]\n","\n","        var_gene = np.var(data, axis=0)\n","        index = np.argsort(var_gene)[-gene_num:]\n","        data = data[:, index]\n","        # print(data.shape)\n","        scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n","        scaler = scaler.fit(data)\n","        data = scaler.fit_transform(data)\n","        for i in range(data.shape[0]):\n","            gene_list_for_count = np.array(data[i, 0:].astype('double'))\n","            gene_list_max = np.max(gene_list_for_count)\n","            data_count_array.append(gene_list_max)\n","\n","            gene_list = gene_list_for_count.tolist()\n","            gene_len = len(gene_list)\n","            figure_size = int(gene_len ** 0.5)\n","            if figure_size * figure_size == gene_len:\n","                data_train = np.array(gene_list).reshape(figure_size, figure_size, 1).astype('double')\n","                figure_size_real = figure_size\n","            else:\n","                for j in range((figure_size + 1) ** 2 - gene_len):\n","                    gene_list.append(0)\n","                data_train = np.array(gene_list).reshape(figure_size + 1, figure_size + 1, 1).astype('double')\n","                figure_size_real = figure_size + 1\n","            data_array.append(data_train)\n","\n","        class_num = cluster_num\n","        data_label = []\n","        label_1 = cluster.k_means(data, cluster_num)\n","        semi_label_index = []\n","        np.random.seed(6)\n","        semi_label = np.random.permutation(label_1.shape[0])\n","        semi_label_index = int((1 - rate) * label_1.shape[0])\n","        semi_label_train = semi_label[:semi_label_index]\n","        semi_label_real = label_1[semi_label_train]\n","        weight_label_train = semi_label[semi_label_index + 1:]\n","        label_1[semi_label_train] = class_num\n","        class_weight = 'balanced'\n","        weight = compute_class_weight(class_weight, np.array(range(class_num)), label_1[weight_label_train])\n","\n","        for i in range(label_1.shape[0]):\n","            x = np.zeros(class_num + 1)\n","            x[label_1[i]] = 1\n","            data_label.append(x)\n","        cell_type = np.array(range(class_num))\n","        test_data = np.array(data_array)[semi_label_train]\n","    return data_array, data_label, cell_type, cell_type.shape[0], obs, var, figure_size_real, data_count_array, weight, scaler, semi_label_real, test_data\n","'''\n","\n","'''\n","def read_cell_for_h5_imputed(array_file,filename, sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS READ CELL FOR H5 IMPUTED\")\n","    with h5py.File(filename, \"r\") as f:\n","        obs = np.array(f[\"obs_names\"][...])\n","        var = np.array(f[\"var_names\"][...])\n","        cell_name = np.array(f[\"obs\"][\"cell_type1\"])\n","        print(np.array(f[\"obs\"]))\n","        batch_name = np.array(f[\"obs\"][\"organ\"])\n","        batch_type, batch_label = np.unique(batch_name, return_inverse=True)\n","        cell_type, cell_label = np.unique(cell_name, return_inverse=True)\n","        class_num = np.array(cell_label).max()+1\n","        data_label = []\n","        data_array = []\n","        for i in range(cell_label.shape[0]):\n","            x = np.zeros(class_num)\n","            x[cell_label[i]] = 1\n","            data_label.append(x)\n","        data_label = np.array(data_label)\n","        cell_type = np.array(cell_type)\n","        data = pd.read_csv(array_file, header=None, sep=\",\")\n","        arr1 = np.array(data)\n","        data_array = []\n","        cell_num = np.shape(cell_name)[0]\n","        print(cell_num)\n","\n","        for i in range(cell_num):\n","            gene_list_for_count = np.array(arr1[1:, i + 1].astype('float64'))\n","            gene_list = gene_list_for_count.tolist()\n","            gene_len = len(gene_list)\n","            data_array.append(np.array(gene_list))\n","        data_array = np.array(data_array)\n","    return data_array,data_label,cell_type,cell_type.shape[0],obs,var,batch_type,batch_label\n","'''\n","\n","'''\n","def read_cell_for_interpretable_imputed(data_path,label_path,class_num,data_set,gene_num,type, sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS READ CELL FOR INTERPRETABLE IMPUTED\")\n","    data = pd.read_csv(data_path, header=0, sep=\",\")\n","    if data_set == \"AD\" or data_set == \"HP\" or data_set == \"lung_cancer\" or data_set == \"AD_cluster\":\n","        label = pd.read_csv(label_path, header=0, sep=\",\")\n","    X = arr1[:, 1:].T\n","    indicator = np.where(X > 0, 1, 0)\n","    sum_gene = np.sum(indicator, axis=0).flatten()\n","    #find gene!!\n","    data = X[:, sum_gene > 10]\n","\n","    var_gene = np.var(data, axis=0)\n","    index = np.argsort(var_gene)[-gene_num:]\n","    data = data[:, index]\n","    scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n","    scaler = scaler.fit(data)\n","    data = scaler.fit_transform(data)\n","    gene_name = np.array(arr1[index, 0])\n","    cell_name = np.array(arr1[0, 1:])\n","    if data_set == \"AD\" or data_set == \"HP\"or data_set == \"lung_cancer\":\n","        label = np.array(label)[:, 1]\n","    if data_set == \"AD\":\n","        label2number = {\"AD\": 1, \"ct\": 2}\n","    elif data_set == \"AD_cluster\":\n","        label = np.array(label)[:, 2]\n","        label2number = {'OPC': 1, 'astro': 2, 'doublet': 3, 'endo': 4, 'mg': 5, 'neuron': 6, 'oligo': 7, 'unID': 8}\n","    elif data_set == \"HP\":\n","        label2number = {\"\"\"disease: Type 2 Diabetic\"\"\": 1, \"\"\"disease: Non-Diabetic\"\"\": 2}\n","    data_array = []\n","\n","    cell_num = np.shape(cell_name)[0]\n","    for i in range(cell_num):\n","        if type==\"interpretable\":\n","            gene_list_for_count = np.array(data[i, 0:].astype('double'))\n","        elif type == \"normal\":\n","            gene_list_for_count = np.array(arr1[0:, i + 1].astype('double'))\n","        gene_list = gene_list_for_count.tolist()\n","        data_array.append(np.array(gene_list))\n","    data_array = np.array(data_array)\n","    data_label = []\n","    for i in range(len(label)):\n","        x = np.zeros(class_num)\n","        x[label2number[label[i]] - 1] = 1\n","        data_label.append(x)\n","    data_label = np.array(data_label)\n","    return data_array, data_label, gene_name, cell_name,label,scaler\n","'''\n","\n","'''\n","def read_interpretable_for_train(data_path,label_path,class_num,data_set,rate,gene_num,sparsify = False, skip_exprs = False):\n","    print(\"DATA_PREPROCESS READ INTERPRETABLE FOR TRAIN\")\n","    data = pd.read_csv(data_path, header=0, sep=\",\")\n","    label = pd.read_csv(label_path, header=0, sep=\",\")\n","    data_label = []\n","    label2number={}\n","    if data_set == \"AD\":\n","        label = np.array(label)[:, 1]\n","        label2number = {\"AD\": 1, \"ct\": 2}\n","    elif data_set == \"AD_cluster\":\n","        label = np.array(label)[:, 2]\n","        label2number = {'OPC':1, 'astro':2, 'doublet':3, 'endo':4, 'mg':5, 'neuron':6, 'oligo':7, 'unID':8}\n","    label_u = np.unique(label,return_inverse=True)\n","    print(label_u)\n","    for i in range(len(label)):\n","        x = np.zeros(class_num+1)\n","        x[label2number[label[i]] - 1] = 1\n","        data_label.append(x)\n","    data_label = np.array(data_label)\n","\n","    data_label_index = np.argmax(data_label,1)\n","    arr1 = np.array(data)\n","\n","    X = arr1[:,1:].T\n","    indicator = np.where(X > 0, 1, 0)\n","    sum_gene = np.sum(indicator, axis=0).flatten()\n","\n","    data = X[:, sum_gene > 10]\n","\n","    var_gene = np.var(data, axis=0)\n","    index = np.argsort(var_gene)[-gene_num:]\n","    data = data[:, index]\n","    # print(data.shape)\n","    scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1))\n","    scaler = scaler.fit(data)\n","    data = scaler.fit_transform(data)\n","    gene_name = np.array(arr1[index, 0])\n","    cell_name = np.array(arr1[0, 1:])\n","    print(data.shape)\n","    data_array = []\n","    data_for_count = []\n","    np.random.seed(2)\n","    semi_label = np.random.permutation(cell_name.shape[0])\n","    semi_label_index = int((1 - rate) * cell_name.shape[0])\n","    semi_label_train = semi_label[:semi_label_index]\n","    semi_label_real = data_label_index[semi_label_train]\n","    weight_label_train = semi_label[semi_label_index + 1:]\n","    if data_set == \"AD_cluster\":\n","        data_label[semi_label_train] = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1])\n","    else:\n","        data_label[semi_label_train] = np.array([0,0,1])\n","    class_weight = 'balanced'\n","    weight = compute_class_weight(class_weight, np.array(range(class_num)), data_label_index[weight_label_train])\n","    cell_num = np.shape(cell_name)[0]\n","    for i in range(cell_num):\n","        gene_list_for_count = np.array(data[i, 0:].astype('double'))\n","        gene_list = gene_list_for_count.tolist()\n","        gene_len = len(gene_list)\n","        figure_size = int(gene_len ** 0.5)\n","        if figure_size * figure_size == gene_len:\n","            data_x = np.array(gene_list).reshape(figure_size, figure_size, 1).astype('double')\n","            figure_size_real = figure_size\n","        else:\n","            for j in range((figure_size + 1) ** 2 - gene_len):\n","                gene_list.append(0)\n","            data_x = np.array(gene_list).reshape(figure_size + 1, figure_size + 1, 1).astype('double')\n","            figure_size_real = figure_size + 1\n","        data_array.append(data_x)\n","    data_array = np.array(data_array)\n","\n","    test_data = np.array(data_array)[semi_label_train]\n","    return data_array, data_label, gene_name, cell_name, figure_size_real,weight,scaler,semi_label_real,test_data\n","'''\n","\n","# Revised freom Original version in scVI\n","# Ref:\n","# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py\n","\n","\n","def impute_dropout(X, seed=1, rate=0.1):\n","    print(\"DATA_PREPROCESS IMPUTE DROPOUT\")\n","    \"\"\"\n","    X: original testing set\n","    ========\n","    returns:\n","    X_zero: copy of X with zeros\n","    i, j, ix: indices of where dropout is applied\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        X_zero = np.copy(X)\n","        # select non-zero subset\n","        i, j = np.nonzero(X_zero)\n","    # If the input is a sparse matrix\n","    else:\n","        X_zero = scipy.sparse.lil_matrix.copy(X)\n","        # select non-zero subset\n","        i, j = X_zero.nonzero()\n","\n","    np.random.seed(seed)\n","    # changes here:\n","    # choice number 1 : select 10 percent of the non zero values (so that distributions overlap enough)\n","    ix = np.random.choice(range(len(i)), int(\n","        np.floor(rate * len(i))), replace=False)\n","    # X_zero[i[ix], j[ix]] *= np.random.binomial(1, rate)\n","    X_zero[i[ix], j[ix]] = 0.0\n","\n","    # choice number 2, focus on a few but corrupt binomially\n","    #ix = np.random.choice(range(len(i)), int(slice_prop * np.floor(len(i))), replace=False)\n","    #X_zero[i[ix], j[ix]] = np.random.binomial(X_zero[i[ix], j[ix]].astype(np.int), rate)\n","    return X_zero, i, j, ix\n","\n","# IMPUTATION METRICS\n","# Revised freom Original version in scVI\n","# Ref:\n","# https://github.com/romain-lopez/scVI-reproducibility/blob/master/demo_code/benchmarking.py\n","\n","\n","def imputation_error(X_mean, X, X_zero, i, j, ix):\n","    print(\"DATA_PREPROCESS IMPUTATION ERROR\")\n","    \"\"\"\n","    X_mean: imputed dataset\n","    X: original dataset\n","    X_zero: zeros dataset, does not need\n","    i, j, ix: indices of where dropout was applied\n","    ========\n","    returns:\n","    median L1 distance between datasets at indices given\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        all_index = i[ix], j[ix]\n","        x, y = X_mean[all_index], X[all_index]\n","        result = np.abs(x - y)\n","    # If the input is a sparse matrix\n","    else:\n","        all_index = i[ix], j[ix]\n","        x = X_mean[all_index[0], all_index[1]]\n","        y = X[all_index[0], all_index[1]]\n","        yuse = scipy.sparse.lil_matrix.todense(y)\n","        yuse = np.asarray(yuse).reshape(-1)\n","        result = np.abs(x - yuse)\n","    # return np.median(np.abs(x - yuse))\n","    return np.mean(result), np.median(result), np.min(result), np.max(result)\n","\n","def imputation_cosine(X_mean, X, X_zero, i, j, ix):\n","    print(\"DATA_PREPROCESS IMPUTATION COSINE\")\n","    \"\"\"\n","    X_mean: imputed dataset\n","    X: original dataset\n","    X_zero: zeros dataset, does not need\n","    i, j, ix: indices of where dropout was applied\n","    ========\n","    returns:\n","    cosine similarity between datasets at indices given\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        all_index = i[ix], j[ix]\n","        x, y = X_mean[all_index], X[all_index]\n","        x = x.reshape(1, -1)\n","        y = y.reshape(1, -1)\n","\n","        print(x)\n","        print(y)\n","        result = cosine_similarity(x, y)\n","    # If the input is a sparse matrix\n","    else:\n","        all_index = i[ix], j[ix]\n","        x = X_mean[all_index[0], all_index[1]]\n","        y = X[all_index[0], all_index[1]]\n","        yuse = scipy.sparse.lil_matrix.todense(y)\n","        yuse = np.asarray(yuse).reshape(-1)\n","        x = x.reshape(1, -1)\n","        yuse = yuse.reshape(1, -1)\n","        result = cosine_similarity(x, yuse)\n","    # return np.median(np.abs(x - yuse))\n","    return result[0][0]\n","\n","'''\n","def imputation_cosine_log(X_mean, X, X_zero, i, j, ix):\n","    print(\"DATA_PREPROCESS IMPUTATION COSINE LOG\")\n","    \"\"\"\n","    X_mean: imputed dataset\n","    X: original dataset\n","    X_zero: zeros dataset, does not need\n","    i, j, ix: indices of where dropout was applied\n","    ========\n","    returns:\n","    cosine similarity between datasets at indices given\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        all_index = i[ix], j[ix]\n","        x, y = X_mean[all_index], X[all_index]\n","        x = x.reshape(1, -1)\n","        y = y.reshape(1, -1)\n","        result = cosine_similarity(x, np.log(y+1))\n","    # If the input is a sparse matrix\n","    else:\n","        all_index = i[ix], j[ix]\n","        x = X_mean[all_index[0], all_index[1]]\n","        y = X[all_index[0], all_index[1]]\n","        yuse = scipy.sparse.lil_matrix.todense(y)\n","        yuse = np.asarray(yuse).reshape(-1)\n","        x = x.reshape(1, -1)\n","        yuse = yuse.reshape(1, -1)\n","        result = cosine_similarity(x, np.log(yuse+1))\n","    # return np.median(np.abs(x - yuse))\n","    return result[0][0]\n","'''\n","\n","'''\n","def imputation_error_log(X_mean, X, X_zero, i, j, ix):\n","    print(\"DATA_PREPROCESS IMPUTATION ERROR LOG\")\n","    \"\"\"\n","    X_mean: imputed dataset\n","    X: original dataset\n","    X_zero: zeros dataset, does not need\n","    i, j, ix: indices of where dropout was applied\n","    ========\n","    returns:\n","    median L1 distance between datasets at indices given\n","    \"\"\"\n","\n","    # If the input is a dense matrix\n","    if isinstance(X, np.ndarray):\n","        all_index = i[ix], j[ix]\n","        x, y = X_mean[all_index], X[all_index]\n","        result = np.abs(x - np.log(y+1))\n","    # If the input is a sparse matrix\n","    else:\n","        all_index = i[ix], j[ix]\n","        x = X_mean[all_index[0], all_index[1]]\n","        y = X[all_index[0], all_index[1]]\n","        yuse = scipy.sparse.lil_matrix.todense(y)\n","        yuse = np.asarray(yuse).reshape(-1)\n","        result = np.abs(x - np.log(yuse+1))\n","    # return np.median(np.abs(x - yuse))\n","    return np.mean(result), np.median(result), np.min(result), np.max(result)\n","'''\n","\n","'''\n","def imputation_impute_matrix(imputed_file,original_file,rate):\n","    print(\"DATA_PREPROCESS IMPUTATION IMPUTE MATRIX\")\n","    data_imputed = pd.read_csv(imputed_file, header=None, sep=\",\")\n","    data_original = pd.read_csv(original_file, header=None, sep=\",\")\n","    X_imputed = np.array(data_imputed)[1:, 1:].T\n","    X_original = np.array(data_original)[1:, 1:].T\n","    X_zero, i, j, ix = impute_dropout(X_original,1,rate)\n","    mean,median,min,max = imputation_error(X_imputed,X_original,X_zero,i,j,ix)\n","    cosine_sim = imputation_cosine(X_imputed,X_original,X_zero,i,j,ix)\n","    print(median)\n","    print(cosine_sim)\n","'''"],"metadata":{"id":"JGzYFVPoMtb6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Singlecell(InMemoryDataset):\n","\n","    def __init__(self, root, name, filepath, transform=None, pre_transform=None):\n","        self.name = name.lower()\n","        self.filepath = filepath\n","        self.labelpath = \"./test_csv/Alzheimer/GSE138852_covariates.csv\"\n","        # self.labelpath = \"./test_csv/Zeisel/label.csv\"\n","        super(Singlecell, self).__init__(root, transform, pre_transform)\n","        self.data, self.slices = torch.load(self.processed_paths[0])\n","\n","    @property\n","    def raw_dir(self):\n","        return osp.join(self.root, self.name.capitalize(), 'raw')\n","\n","    @property\n","    def processed_dir(self):\n","        return osp.join(self.root, self.name.capitalize(), 'processed')\n","\n","    @property\n","    def raw_file_names(self):\n","        return 'amazon_electronics_{}.npz'.format(self.name.lower())\n","\n","    @property\n","    def processed_file_names(self):\n","        return 'data.pt'\n","\n","    def download(self):\n","\n","        pass\n","\n","    def process(self):\n","\n","        raw = False\n","        data, data_label = normalize_for_AF(self.filepath, 2048,raw);\n","\n","        x = torch.tensor(np.array(data),dtype=torch.float32)\n","        y = torch.tensor(data_label, dtype=torch.long)\n","\n","        adj, adj_n = get_adj(data)\n","        adj = sp.coo_matrix(adj_n)\n","        edge_index = torch.tensor([adj.row, adj.col], dtype=torch.long)\n","        edge_index, _ = remove_self_loops(edge_index)\n","        edge_index = to_undirected(edge_index, x.size(0))  # Internal coalesce.\n","        data = Data(x=x, edge_index=edge_index, y=y)\n","        data = data if self.pre_transform is None else self.pre_transform(data)\n","        data, slices = self.collate([data])\n","        print(self.processed_paths[0])\n","        torch.save((data, slices), self.processed_paths[0])\n","\n","    def __repr__(self):\n","        print(\"SINGLECELL REPR\")\n","        return '{}{}()'.format(self.__class__.__name__, self.name.capitalize())"],"metadata":{"id":"XE6NZLDIMPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#utils.py from source code\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","\n","def currentTime():\n","    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","\n","'''\n","def to_sparse(x):\n","    \"\"\" converts dense tensor x to sparse format \"\"\"\n","    x_typename = torch.typename(x).split('.')[-1]\n","    sparse_tensortype = getattr(torch.sparse, x_typename)\n","\n","    indices = torch.nonzero(x)\n","    if len(indices.shape) == 0:  # if all elements are zeros\n","        return sparse_tensortype(*x.shape)\n","    indices = indices.t()\n","    values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n","    return sparse_tensortype(indices, values, x.size())\n","'''\n","\n","def parse_args():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--embedder\", type=str, default=\"AFGRL\")\n","    parser.add_argument(\"--dataset\", type=str, default=\"adam\", help=\"Name of the dataset. Supported names are: wikics, cs, computers, photo, and physics\")\n","\n","    parser.add_argument('--checkpoint_dir', type=str, default = '/content/drive/MyDrive/CS598 DLH Project/model_checkpoints', help='directory to save checkpoint')\n","    parser.add_argument(\"--root\", type=str, default=\"data\")\n","    parser.add_argument(\"--task\", type=str, default=\"clustering\", help=\"Downstream task. Supported tasks are: node, clustering, similarity\")\n","\n","    parser.add_argument(\"--layers\", nargs='?', default='[2048]', help=\"The number of units of each layer of the GNN. Default is [256]\")\n","    parser.add_argument(\"--pred_hid\", type=int, default=2048, help=\"The number of hidden units of layer of the predictor. Default is 512\")\n","\n","    parser.add_argument(\"--topk\", type=int, default=4, help=\"The number of neighbors to search\")\n","    parser.add_argument(\"--clus_num_iters\", type=int, default=20)\n","    parser.add_argument(\"--num_centroids\", type=int, default=9, help=\"The number of centroids for K-means Clustering\")\n","    parser.add_argument(\"--num_kmeans\", type=int, default=5, help=\"The number of K-means Clustering for being robust to randomness\")\n","    parser.add_argument(\"--eval_freq\", type=float, default=5, help=\"The frequency of model evaluation\")\n","    parser.add_argument(\"--mad\", type=float, default=0.9, help=\"Moving Average Decay for Teacher Network\")\n","    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n","    parser.add_argument(\"--es\", type=int, default=300, help=\"Early Stopping Criterion\")\n","    parser.add_argument(\"--device\", type=int, default=0)\n","    parser.add_argument(\"--epochs\", type=int, default=1)\n","    parser.add_argument(\"--dropout\", type=float, default=0.0)\n","    parser.add_argument(\"--aug_params\", \"-p\", nargs=\"+\", default=[0.3, 0.4, 0.3, 0.2],help=\"Hyperparameters for augmentation (p_f1, p_f2, p_e1, p_e2). Default is [0.2, 0.1, 0.2, 0.3]\")\n","    return parser.parse_known_args()\n","\n","class Augmentation:\n","\n","    def __init__(self, p_f1=0.2, p_f2=0.1, p_e1=0.2, p_e2=0.3):\n","        \"\"\"\n","        two simple graph augmentation functions --> \"Node feature masking\" and \"Edge masking\"\n","        Random binary node feature mask following Bernoulli distribution with parameter p_f\n","        Random binary edge mask following Bernoulli distribution with parameter p_e\n","        \"\"\"\n","        self.p_f1 = p_f1\n","        self.p_f2 = p_f2\n","        self.p_e1 = p_e1\n","        self.p_e2 = p_e2\n","        self.method = \"BGRL\"\n","\n","    def _feature_masking(self, data, device):\n","        feat_mask1 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f1\n","        feat_mask2 = torch.FloatTensor(data.x.shape[1]).uniform_() > self.p_f2\n","        feat_mask1, feat_mask2 = feat_mask1.to(device), feat_mask2.to(device)\n","        x1, x2 = data.x.clone(), data.x.clone()\n","        x1, x2 = x1 * feat_mask1, x2 * feat_mask2\n","\n","        edge_index1, edge_attr1 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e1)\n","        edge_index2, edge_attr2 = dropout_adj(data.edge_index, data.edge_attr, p=self.p_e2)\n","\n","        new_data1, new_data2 = data.clone(), data.clone()\n","        new_data1.x, new_data2.x = x1, x2\n","        new_data1.edge_index, new_data2.edge_index = edge_index1, edge_index2\n","        new_data1.edge_attr, new_data2.edge_attr = edge_attr1, edge_attr2\n","\n","        return new_data1, new_data2\n","\n","    def __call__(self, data):\n","        return self._feature_masking(data)\n","\n","def decide_config(root, dataset):\n","    \"\"\"\n","    Create a configuration to download datasets\n","    :param root: A path to a root directory where data will be stored\n","    :param dataset: The name of the dataset to be downloaded\n","    :return: A modified root dir, the name of the dataset class, and parameters associated to the class\n","    \"\"\"\n","    dataset = dataset.lower()\n","    if dataset == \"adam\":\n","        dataset = \"Adam\"\n","        root = osp.join(root, \"pyg\")\n","        filepath = '/content/drive/MyDrive/CS598 DLH Project/test_csv/data.h5' #modify this\n","        params = {\"kwargs\": {\"root\": root, \"name\": dataset, \"filepath\": filepath},\n","                  \"name\": dataset, \"class\": Singlecell, \"src\": \"pyg\"}\n","    else:\n","        raise Exception(\n","            f\"Unknown dataset name {dataset}, name has to be one of the following 'cora', 'citeseer', 'pubmed', 'photo', 'computers', 'cs', 'physics'\")\n","    return params\n","\n","\n","def create_dirs(dirs):\n","    for dir_tree in dirs:\n","        sub_dirs = dir_tree.split(\"/\")\n","        path = \"\"\n","        for sub_dir in sub_dirs:\n","            path = osp.join(path, sub_dir)\n","            os.makedirs(path, exist_ok=True)\n","\n","def tensor2var(x, grad=False):\n","    if torch.cuda.is_available():\n","        x = x.cuda()\n","    return Variable(x, requires_grad=grad)\n","\n","def create_masks(data):\n","    \"\"\"\n","    Splits data into training, validation, and test splits in a stratified manner if\n","    it is not already splitted. Each split is associated with a mask vector, which\n","    specifies the indices for that split. The data will be modified in-place\n","    :param data: Data object\n","    :return: The modified data\n","    \"\"\"\n","    if not hasattr(data, \"val_mask\"):\n","\n","        data.train_mask = data.dev_mask = data.test_mask = None\n","\n","        for i in range(20):\n","            labels = data.y.numpy()\n","            dev_size = int(labels.shape[0] * 0.1)\n","            test_size = int(labels.shape[0] * 0.8)\n","\n","            perm = np.random.permutation(labels.shape[0])\n","            test_index = perm[:test_size]\n","            dev_index = perm[test_size:test_size + dev_size]\n","\n","            data_index = np.arange(labels.shape[0])\n","            test_mask = torch.tensor(np.in1d(data_index, test_index), dtype=torch.bool)\n","            dev_mask = torch.tensor(np.in1d(data_index, dev_index), dtype=torch.bool)\n","            train_mask = ~(dev_mask + test_mask)\n","            test_mask = test_mask.reshape(1, -1)\n","            dev_mask = dev_mask.reshape(1, -1)\n","            train_mask = train_mask.reshape(1, -1)\n","\n","            if 'train_mask' not in data:\n","                data.train_mask = train_mask\n","                data.val_mask = dev_mask\n","                data.test_mask = test_mask\n","            else:\n","                data.train_mask = torch.cat((data.train_mask, train_mask), dim=0)\n","                data.val_mask = torch.cat((data.val_mask, dev_mask), dim=0)\n","                data.test_mask = torch.cat((data.test_mask, test_mask), dim=0)\n","\n","    else:  # in the case of WikiCS\n","        data.train_mask = data.train_mask.T\n","        data.val_mask = data.val_mask.T\n","\n","    return data\n","\n","\n","class EMA:\n","    def __init__(self, beta, epochs):\n","        super().__init__()\n","        self.beta = beta\n","        self.step = 0\n","        self.total_steps = epochs\n","\n","    def update_average(self, old, new):\n","        if old is None:\n","            return new\n","\n","        beta = 1 - (1 - self.beta) * (np.cos(np.pi * self.step / self.total_steps) + 1) / 2.0\n","        self.step += 1\n","        return old * beta + (1 - beta) * new\n","\n","\n","def init_weights(m):\n","    if type(m) == nn.Linear:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","\n","\n","def loss_fn(x, y):\n","    x = F.normalize(x, dim=-1, p=2)\n","    y = F.normalize(y, dim=-1, p=2)\n","\n","    return 2 - 2 * (x * y).sum(dim=-1)\n","\n","\n","def l2_normalize(x):\n","    return x / torch.sqrt(torch.sum(x**2, dim=1).unsqueeze(1))\n","\n","\n","def update_moving_average(ema_updater, ma_model, current_model):\n","    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n","        old_weight, up_weight = ma_params.data, current_params.data\n","        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n","\n","\n","def set_requires_grad(model, val):\n","    for p in model.parameters():\n","        p.requires_grad = val\n","\n","\n","def enumerateConfig(args):\n","    args_names = []\n","    args_vals = []\n","    for arg in vars(args):\n","        args_names.append(arg)\n","        args_vals.append(getattr(args, arg))\n","\n","    return args_names, args_vals\n","\n","\n","def config2string(args):\n","    args_names, args_vals = enumerateConfig(args)\n","    st = ''\n","    for name, val in zip(args_names, args_vals):\n","        if val == False:\n","            continue\n","        if name in ['dataset']:\n","            st_ = \"{}_{}_\".format(name, val)\n","            st += st_\n","\n","    return st[:-1]\n","\n","\n","def printConfig(args):\n","    args_names, args_vals = enumerateConfig(args)\n","    print(args_names)\n","    print(args_vals)\n","\n","\n","def repeat_1d_tensor(t, num_reps):\n","    return t.unsqueeze(1).expand(-1, num_reps)\n","\n","'''\n","def fill_ones(x):\n","    n_data = x.shape[0]\n","    x = torch.sparse_coo_tensor(x._indices(), torch.ones(x._nnz()).to(x.device), [n_data, n_data])\n","\n","    return x\n","'''\n"],"metadata":{"id":"Yi_REaQFNTGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#data.py from source code\n","\n","np.random.seed(0)\n","\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","\n","def download_pyg_data(config):\n","    \"\"\"\n","    Downloads a dataset from the PyTorch Geometric library\n","\n","    :param config: A dict containing info on the dataset to be downloaded\n","    :return: A tuple containing (root directory, dataset name, data directory)\n","    \"\"\"\n","    leaf_dir = config[\"kwargs\"][\"root\"].split(\"/\")[-1].strip()\n","    data_dir = osp.join(config[\"kwargs\"][\"root\"], \"\" if config[\"name\"] == leaf_dir else config[\"name\"])\n","    dst_path = osp.join(data_dir, \"raw\", \"data.pt\")\n","    if not osp.exists(dst_path):\n","        DatasetClass = config[\"class\"]\n","        if config[\"name\"] == \"WikiCS\":\n","            dataset = DatasetClass(data_dir, transform=T.NormalizeFeatures())\n","            size_factor = dataset.size_factors\n","        else :\n","            dataset = DatasetClass(**config[\"kwargs\"])\n","        create_masks(data=dataset.data)\n","        torch.save((dataset.data, dataset.slices), dst_path)\n","    return config[\"kwargs\"][\"root\"], config[\"name\"], data_dir\n","\n","\n","def download_data(root, dataset):\n","    \"\"\"\n","    Download data from different repositories. Currently only PyTorch Geometric is supported\n","\n","    :param root: The root directory of the dataset\n","    :param name: The name of the dataset\n","    :return:\n","    \"\"\"\n","    config = decide_config(root=root, dataset=dataset)\n","    if config[\"src\"] == \"pyg\":\n","        return download_pyg_data(config)\n","\n","\n","class Dataset(InMemoryDataset):\n","\n","    \"\"\"\n","    A PyTorch InMemoryDataset to build multi-view dataset through graph data augmentation\n","    \"\"\"\n","\n","    def __init__(self, root=\"data\", dataset='Adam', transform=None, pre_transform=None):\n","        self.root, self.dataset, self.data_dir = download_data(root=root, dataset=dataset)\n","        create_dirs(self.dirs)\n","        super().__init__(root=self.data_dir, transform=transform, pre_transform=pre_transform)\n","        self.process()\n","        path = osp.join(self.data_dir, \"processed\", self.processed_file_names[0])\n","        self.data, self.slices = torch.load(path)\n","\n","    def process_full_batch_data(self, data):\n","\n","        print(\"Processing full batch data\")\n","        nodes = torch.tensor(np.arange(data.num_nodes), dtype=torch.long)\n","\n","        edge_index, edge_attr = add_self_loops(data.edge_index, data.edge_attr)\n","\n","        data = Data(nodes=nodes, edge_index=data.edge_index, edge_attr=data.edge_attr, x=data.x, y=data.y,\n","                    train_mask=data.train_mask, val_mask=data.val_mask, test_mask=data.test_mask,\n","                    num_nodes=data.num_nodes, neighbor_index=edge_index, neighbor_attr=edge_attr)\n","\n","        return [data]\n","\n","    def process(self):\n","        \"\"\"\n","        Process either a full batch or cluster data.\n","\n","        :return:\n","        \"\"\"\n","        processed_path = osp.join(self.processed_dir, self.processed_file_names[0])\n","        if not osp.exists(processed_path):\n","            path = osp.join(self.raw_dir, self.raw_file_names[0])\n","            data, _ = torch.load(path)\n","            edge_attr = data.edge_attr\n","            edge_attr = torch.ones(data.edge_index.shape[1]) if edge_attr is None else edge_attr\n","            data.edge_attr = edge_attr\n","\n","\n","            data_list = self.process_full_batch_data(data)\n","\n","            data, slices = self.collate(data_list)\n","            torch.save((data, slices), processed_path)\n","\n","\n","    @property\n","    def raw_file_names(self):\n","        return [\"data.pt\"]\n","\n","    @property\n","    def processed_file_names(self):\n","        return [f'byg.data.pt']\n","\n","    @property\n","    def raw_dir(self):\n","        return osp.join(self.data_dir, \"raw\")\n","\n","    @property\n","    def processed_dir(self):\n","        return osp.join(self.data_dir, \"processed\")\n","\n","    @property\n","    def model_dir(self):\n","        return osp.join(self.data_dir, \"model\")\n","\n","    @property\n","    def result_dir(self):\n","        return osp.join(self.data_dir, \"results\")\n","\n","    @property\n","    def dirs(self):\n","        return [self.raw_dir, self.processed_dir, self.model_dir, self.result_dir]\n","\n","    def download(self):\n","        pass"],"metadata":{"id":"V2Bgey9kLm9n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##   Model\n","The model includes the model definitation which usually is a class, model training, and other necessary parts.\n","  * Model architecture: layer number/size/type, activation function, etc\n","  * Training objectives: loss function, optimizer, weight of each loss term, etc\n","  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n","  * The code of model should have classes of the model, functions of model training, model validation, etc.\n","  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."],"metadata":{"id":"3muyDPFPbozY"}},{"cell_type":"code","source":["#logreg.py\n","\n","# To fix the random seed\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","\n","random.seed(0)\n","np.random.seed(0)\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, num_dim, num_class):\n","        super().__init__()\n","        self.linear = nn.Linear(num_dim, num_class)\n","        torch.nn.init.xavier_uniform_(self.linear.weight.data)\n","        self.linear.bias.data.fill_(0.0)\n","        self.cross_entropy = nn.CrossEntropyLoss()\n","\n","    def forward(self, x, y):\n","        logits = self.linear(x)\n","        loss = self.cross_entropy(logits, y)\n","        return logits, loss"],"metadata":{"id":"FmZ5fq8tQDK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#embedder.py\n","# To fix the random seed\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","\n","np.random.seed(0)\n","random.seed(0)\n","\n","# scGCL Model\n","# Revised freom Original version in AFGRL\n","# Ref:\n","# https://github.com/Namkyeong/AFGRL/tree/master/embedder.py\n","\n","class embedder:\n","    def __init__(self, args):\n","        self.args = args\n","        self.hidden_layers = eval(args.layers)\n","        printConfig(args)\n","\n","    def infer_embeddings(self, epoch):\n","        print(\"EMBEDDER INFER EMBEDDINGS\")\n","        self._model.train(False)\n","        self._embeddings = self._labels = None\n","        self._train_mask = self._dev_mask = self._test_mask = None\n","\n","        for bc, batch_data in enumerate(self._loader):\n","            # augmentation = utils.Augmentation(float(self._args.aug_params[0]), float(self._args.aug_params[1]),\n","            #                                   float(self._args.aug_params[2]), float(self._args.aug_params[3]))\n","\n","            batch_data.to(self._device)\n","            # view1, view2 = augmentation._feature_masking(batch_data, self._device)\n","\n","            emb, loss = self._model(x = batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,\n","                                                                           neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],\n","                                                                           edge_weight=batch_data.edge_attr, epoch=epoch)\n","            emb = emb.detach()\n","            y = batch_data.y.detach()\n","            if self._embeddings is None:\n","                self._embeddings, self._labels = emb, y\n","            else:\n","                self._embeddings = torch.cat([self._embeddings, emb])\n","                self._labels = torch.cat([self._labels, y])\n","\n","\n","    def evaluate(self, task, epoch, sillog):\n","        print(\"EMBEDDER EVALUATE\")\n","        if task == \"node\":\n","            self.evaluate_node(epoch)\n","        elif task == \"clustering\":\n","            self.evaluate_clustering(epoch,sillog)\n","        elif task == \"similarity\":\n","            self.run_similarity_search(epoch)\n","\n","\n","    def evaluate_node(self, epoch):\n","        print(\"EVALUATE NODE\")\n","        emb_dim, num_class = self._embeddings.shape[1], self._labels.unique().shape[0]\n","\n","        dev_accs, test_accs = [], []\n","\n","        for i in range(20):\n","\n","            self._train_mask = self._dataset[0].train_mask[i]\n","            self._dev_mask = self._dataset[0].val_mask[i]\n","            if self._args.dataset == \"wikics\":\n","                self._test_mask = self._dataset[0].test_mask\n","            else:\n","                self._test_mask = self._dataset[0].test_mask[i]\n","\n","            classifier = LogisticRegression(emb_dim, num_class).to(self._device)\n","            optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)\n","\n","            for _ in range(100):\n","                classifier.train()\n","                logits, loss = classifier(self._embeddings[self._train_mask], self._labels[self._train_mask])\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","            dev_logits, _ = classifier(self._embeddings[self._dev_mask], self._labels[self._dev_mask])\n","            test_logits, _ = classifier(self._embeddings[self._test_mask], self._labels[self._test_mask])\n","            dev_preds = torch.argmax(dev_logits, dim=1)\n","            test_preds = torch.argmax(test_logits, dim=1)\n","\n","            dev_acc = (torch.sum(dev_preds == self._labels[self._dev_mask]).float() /\n","                       self._labels[self._dev_mask].shape[0]).detach().cpu().numpy()\n","            test_acc = (torch.sum(test_preds == self._labels[self._test_mask]).float() /\n","                        self._labels[self._test_mask].shape[0]).detach().cpu().numpy()\n","\n","            dev_accs.append(dev_acc * 100)\n","            test_accs.append(test_acc * 100)\n","\n","        dev_accs = np.stack(dev_accs)\n","        test_accs = np.stack(test_accs)\n","\n","        dev_acc, dev_std = dev_accs.mean(), dev_accs.std()\n","        test_acc, test_std = test_accs.mean(), test_accs.std()\n","\n","        print('** [{}] [Epoch: {}] Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(self.args.embedder, epoch, dev_acc, dev_std, test_acc, test_std))\n","\n","        if dev_acc > self.best_dev_acc:\n","            self.best_dev_acc = dev_acc\n","            self.best_test_acc = test_acc\n","            self.best_dev_std = dev_std\n","            self.best_test_std = test_std\n","            self.best_epoch = epoch\n","\n","        self.best_dev_accs.append(self.best_dev_acc)\n","        self.st_best = '** [Best epoch: {}] Best val | Best test: {:.4f} ({:.4f}) / {:.4f} ({:.4f})**\\n'.format(\n","            self.best_epoch, self.best_dev_acc, self.best_dev_std, self.best_test_acc, self.best_test_std)\n","        print(self.st_best)\n","\n","\n","    def evaluate_clustering(self, epoch,sillog):\n","        print(\"EMBEDDER EVALUATE CLUSTERING\")\n","        embeddings = F.normalize(self._embeddings, dim = -1, p = 2).detach().cpu().numpy()\n","\n","        nb_class = len(self._dataset[0].y.unique())\n","        true_y = self._dataset[0].y.detach().cpu().numpy()\n","\n","        estimator = KMeans(n_clusters = nb_class)\n","\n","        NMI_list = []\n","\n","        for i in range(10):\n","            estimator.fit(embeddings)\n","            y_pred = estimator.predict(embeddings)\n","            s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')\n","            NMI_list.append(s1)\n","        estimator.fit(embeddings)\n","        y_pred = estimator.predict(embeddings)\n","        silhid = metrics.silhouette_score(self._embeddings.detach().cpu().numpy(), y_pred, metric='euclidean')\n","        s1 = sum(NMI_list) / len(NMI_list)\n","        sillog.append(silhid)\n","        arr_sil = np.array(sillog)\n","\n","        print('** [{}] [Current Epoch {}] this epoch NMI values: {:.4f} ** and this epoch sil values: {}'.format(self.args.embedder, epoch, s1,silhid))\n","\n","        if math.floor(silhid*100) >= math.floor(self.best_dev_acc*100):\n","\n","            self.best_dev_acc = round(silhid, 2)\n","            self.best_embeddings = embeddings\n","            self.best_test_acc = s1\n","            print(\"~~~~~~~~~~~~~~~~~~\")\n","            print(silhid)\n","            if self._args.checkpoint_dir is not '':\n","                print('Saving checkpoint...')\n","                torch.save(embeddings, os.path.join(self._args.checkpoint_dir,\n","                                                    'embeddings_{}_{}.pt'.format(self._args.dataset, self._args.task)))\n","                # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)\n","                a = pd.DataFrame(self.best_embeddings).T\n","                a.to_csv(\"/content/drive/MyDrive/CS598 DLH Project/results/scGCL-Tosches_turtle-euclidean.csv\")\n","            print(\"save\")\n","            print(\"~~~~~~~~~~~~~~~~~~\")\n","\n","\n","    def run_similarity_search(self, epoch):\n","        print(\"EMBEDDER RUN SIMILARITY SEARCH\")\n","        test_embs = self._embeddings.detach().cpu().numpy()\n","        test_lbls = self._dataset[0].y.detach().cpu().numpy()\n","        numRows = test_embs.shape[0]\n","\n","        cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)\n","        st = []\n","        for N in [5, 10]:\n","            indices = np.argsort(cos_sim_array, axis=1)[:, -N:]\n","            tmp = np.tile(test_lbls, (numRows, 1))\n","            selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)\n","            original_label = np.repeat(test_lbls, N).reshape(numRows,N)\n","            st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))\n","\n","        print(\"** [{}] [Current Epoch {}] sim@5 : {} | sim@10 : {} **\".format(self.args.embedder, epoch, st[0], st[1]))\n","\n","        if st[0] > self.best_dev_acc:\n","            self.best_dev_acc = st[0]\n","            self.best_test_acc = st[1]\n","            self.best_epoch = epoch\n","\n","        self.best_dev_accs.append(self.best_dev_acc)\n","        self.st_best = '** [Best epoch: {}] Best @5 : {} | Best @10: {} **\\n'.format(self.best_epoch, self.best_dev_acc, self.best_test_acc)\n","        print(self.st_best)\n","\n","        return st\n","\n","\n","class Encoder(nn.Module):\n","\n","    def __init__(self, layer_config, dropout=None, project=False, **kwargs):\n","        super().__init__()\n","        self.stacked_gnn = nn.ModuleList([GCNConv(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])\n","        # self.stacked_en = nn.ModuleList(\n","        #     [nn.Linear(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])\n","        self.stacked_bns = nn.ModuleList([nn.BatchNorm1d(layer_config[i], momentum=0.01) for i in range(1, len(layer_config))])\n","        self.stacked_prelus = nn.ModuleList([nn.ReLU() for _ in range(1, len(layer_config))])\n","\n","    def forward(self, x, edge_index, edge_weight=None):\n","        print(\"EMBEDDER ENCODER FORWARD\")\n","        for i, gnn in enumerate(self.stacked_gnn):\n","            x = gnn(x, edge_index, edge_weight=edge_weight)\n","            #x = gnn(x)\n","            x = self.stacked_bns[i](x)\n","            x = self.stacked_prelus[i](x)\n","\n","        return x"],"metadata":{"id":"rQFonakSKhzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#AFGRL.py from source code\n","\n","np.random.seed(0)\n","\n","# To fix the random seed\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","# scGCL Model\n","# Revised freom Original version in AFGRL\n","# Ref:\n","# https://github.com/Namkyeong/AFGRL/tree/master/models/AFGRL.py\n","\n","class AFGRL_ModelTrainer(embedder):\n","\n","    def __init__(self, args):\n","        embedder.__init__(self, args)\n","        self._args = args\n","        self._init()\n","        self.config_str = config2string(args)\n","        print(\"\\n[Config] {}\\n\".format(self.config_str))\n","        self.writer = SummaryWriter(log_dir=\"runs/{}\".format(self.config_str))\n","\n","    def _init(self):\n","        args = self._args\n","        self._task = args.task\n","        print(\"Downstream Task : {}\".format(self._task))\n","        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)\n","        # self._device = f'cuda:{args.device}' if torch.cuda.is_available() else \"cpu\"\n","        self._device = \"cpu\"\n","        # torch.cuda.set_device(self._device)\n","        self._dataset = Dataset(root=args.root, dataset=args.dataset)\n","        self._loader = DataLoader(dataset=self._dataset)\n","        layers = [self._dataset.data.x.shape[1]] + self.hidden_layers\n","        self._model = AFGRL(layers, args).to(self._device)\n","        self._optimizer = optim.AdamW(params=self._model.parameters(), lr=args.lr, weight_decay= 1e-5)\n","\n","    def train(self):\n","\n","        self.best_test_acc, self.best_dev_acc, self.best_test_std, self.best_dev_std, self.best_epoch = 0, 0, 0, 0, 0\n","        self.best_dev_accs = []\n","        self.best_embeddings = None\n","        sillog = []\n","        # get Random Initial accuracy\n","        self.infer_embeddings(0)\n","        print(\"initial accuracy \")\n","        self.evaluate(self._task, 0, sillog)\n","\n","        f_final = open(\"/content/drive/MyDrive/CS598 DLH Project/results/{}.txt\".format(self._args.embedder), \"a\")\n","\n","        # Start Model Training\n","        print(\"Training Start!\")\n","        self._model.train()\n","        for epoch in range(self._args.epochs):\n","            for bc, batch_data in enumerate(self._loader):\n","                batch_data.to(self._device)\n","                # view1, view2 = augmentation._feature_masking(batch_data, self._device)\n","\n","                emb, loss = self._model(x=batch_data.x, y=batch_data.y, edge_index=batch_data.edge_index,\n","                                     neighbor=[batch_data.neighbor_index, batch_data.neighbor_attr],\n","                                     edge_weight=batch_data.edge_attr, epoch=epoch)\n","\n","                self._optimizer.zero_grad()\n","                loss.backward()\n","                self._optimizer.step()\n","                self._model.update_moving_average()\n","\n","                st = '[{}][Epoch {}/{}] Loss: {:.4f}'.format(currentTime(), epoch, self._args.epochs, loss.item())\n","                print(st)\n","\n","            if (epoch) % 5 == 0:\n","                self.infer_embeddings(epoch)\n","                # self.evaluate(self._task, epoch)\n","                self.evaluate(self._task, epoch, sillog)\n","\n","\n","\n","        print(\"\\nTraining Done!\")\n","        self.st_best = '** [last epoch: {}] last NMI: {:.4f} **\\n'.format(self._args.epochs, self.best_test_acc)\n","        print(\"[Final] {}\".format(self.st_best))\n","        print('Saving checkpoint...')\n","        torch.save(self.best_embeddings, os.path.join(self._args.checkpoint_dir,\n","                                            'embeddings_{}_{}.pt'.format(self._args.dataset,\n","                                                                         self._args.task)))\n","        # zzz = np.concatenate((true_y.reshape(3660, 1), y_pred.reshape(3660, 1)), axis=1)\n","        a = pd.DataFrame(self.best_embeddings).T\n","        a.to_csv(\"/content/drive/MyDrive/CS598 DLH Project/results/scGCL-Tosches_turtle-euclidean.csv\")\n","        f_final.write(\"{} -> {}\\n\".format(self.config_str, self.st_best))\n","\n","\n","class AFGRL(nn.Module):\n","    def __init__(self, layer_config, args, **kwargs):\n","        super().__init__()\n","        dec_dim = [512, 256]\n","        self.student_encoder = Encoder(layer_config=layer_config, dropout=args.dropout, **kwargs)\n","        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n","        set_requires_grad(self.teacher_encoder, False)\n","        self.teacher_ema_updater = EMA(args.mad, args.epochs)\n","        self.neighbor = Neighbor(args)\n","        rep_dim = layer_config[-1]\n","        rep_dim_o = layer_config[0]\n","        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, args.pred_hid), nn.BatchNorm1d(args.pred_hid), nn.ReLU(), nn.Linear(args.pred_hid, rep_dim), nn.ReLU())\n","        self.ZINB_Encoder = nn.Sequential(nn.Linear(rep_dim, dec_dim[0]), nn.ReLU(),\n","                                          nn.Linear(dec_dim[0], dec_dim[1]), nn.ReLU())\n","        self.pi_Encoder =  nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o),nn.Sigmoid())\n","        self.disp_Encoder = nn.Sequential(nn.Linear(dec_dim[1], rep_dim_o), nn.Softplus())\n","        self.mean_Encoder = nn.Linear(dec_dim[1], rep_dim_o)\n","        self.student_predictor.apply(init_weights)\n","        self.relu = nn.ReLU()\n","        self.topk = args.topk\n","        # self._device = args.device\n","        self._device = \"cpu\"\n","    def clip_by_tensor(self,t, t_min, t_max):\n","        \"\"\"\n","        clip_by_tensor\n","        :param t: tensor\n","        :param t_min: min\n","        :param t_max: max\n","        :return: cliped tensor\n","        \"\"\"\n","        t = torch.tensor(t,dtype = torch.float32)\n","        t_min = torch.tensor(t_min,dtype = torch.float32)\n","        t_max = torch.tensor(t_max,dtype = torch.float32)\n","\n","        result = torch.tensor((t >= t_min),dtype = torch.float32) * t + torch.tensor((t < t_min),dtype = torch.float32) * t_min\n","        result = torch.tensor((result <= t_max),dtype = torch.float32) * result + torch.tensor((result > t_max),dtype = torch.float32) * t_max\n","        return result\n","\n","    def reset_moving_average(self):\n","        del self.teacher_encoder\n","        self.teacher_encoder = None\n","\n","    def update_moving_average(self):\n","        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n","        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n","\n","    def forward(self, x, y, edge_index, neighbor, edge_weight=None, epoch=None):\n","        student = self.student_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n","        # student_ = self.student_encoder(x=x2, edge_index=edge_index_2, edge_weight=edge_weight_2)\n","        pred = self.student_predictor(student)\n","        # pred_ = self.student_predictor(student_)\n","        z = self.ZINB_Encoder(student)\n","        pi = self.pi_Encoder(z)\n","        disp = self.disp_Encoder(z)\n","        disp = self.clip_by_tensor(disp,1e-4,1e4)\n","        mean = self.mean_Encoder(z)\n","        mean = self.clip_by_tensor(torch.exp(mean),1e-5,1e6)\n","        modify = 0\n","        with torch.no_grad():\n","            teacher = self.teacher_encoder(x=x, edge_index=edge_index, edge_weight=edge_weight)\n","        if edge_weight == None:\n","            adj = torch.sparse.FloatTensor(neighbor[0], torch.ones_like(neighbor[0][0]), [x.shape[0], x.shape[0]])\n","        else:\n","            adj = torch.sparse.FloatTensor(neighbor[0], neighbor[1], [x.shape[0], x.shape[0]])\n","        #\n","        ind, k = self.neighbor(adj, F.normalize(student, dim=-1, p=2), F.normalize(teacher, dim=-1, p=2), self.topk, epoch)\n","        zinb = ZINB(pi, theta=disp, ridge_lambda=0, debug=False)\n","        zinb_loss = zinb.loss(x, mean, mean=True)\n","\n","        loss1 = loss_fn(pred[ind[0]], teacher[ind[1]].detach())\n","        loss2 = loss_fn(pred[ind[1]], teacher[ind[0]].detach())\n","        recon_loss = torch.nn.MSELoss(reduction='mean')\n","        recon_loss_ = recon_loss(x,student)\n","        # adj_recon_ = recon_loss(adj.to_dense(),adj_recon)\n","        loss_reforce = (loss1 + loss2)\n","        if modify == 0:\n","            loss = zinb_loss + loss_reforce + recon_loss_\n","        elif modify == 1:\n","            loss = loss_reforce + recon_loss_\n","        elif modify == 2:\n","            loss = zinb_loss\n","        return student, loss.mean()\n","\n","\n","\n","class Neighbor(nn.Module):\n","    def __init__(self, args):\n","        super(Neighbor, self).__init__()\n","        # self.device = args.device\n","        self.device = \"cpu\"\n","        self.num_centroids = args.num_centroids\n","        self.num_kmeans = args.num_kmeans\n","        self.clus_num_iters = args.clus_num_iters\n","\n","    def __get_close_nei_in_back(self, indices, each_k_idx, cluster_labels, back_nei_idxs, k):\n","        # get which neighbors are close in the background set\n","        batch_labels = cluster_labels[each_k_idx][indices]\n","        top_cluster_labels = cluster_labels[each_k_idx][back_nei_idxs]\n","        batch_labels = repeat_1d_tensor(batch_labels, k)\n","\n","        curr_close_nei = torch.eq(batch_labels, top_cluster_labels)\n","        return curr_close_nei\n","\n","    def forward(self, adj, student, teacher, top_k, epoch):\n","        n_data, d = student.shape\n","        similarity = torch.matmul(student, torch.transpose(teacher, 1, 0).detach())\n","        similarity += torch.eye(n_data, device=self.device) * 10\n","\n","        _, I_knn = similarity.topk(k=top_k, dim=1, largest=True, sorted=True)\n","        tmp = torch.LongTensor(np.arange(n_data)).unsqueeze(-1).to(self.device)\n","\n","        knn_neighbor = self.create_sparse(I_knn)\n","        locality = knn_neighbor * adj\n","\n","        ncentroids = self.num_centroids\n","        niter = self.clus_num_iters\n","\n","        pred_labels = []\n","        # d_means = []\n","        for seed in range(self.num_kmeans):\n","            kmeans = faiss.Kmeans(d, ncentroids, niter=niter, gpu=False, seed=seed + 1234)\n","            kmeans.train(teacher.cpu().numpy())\n","            _, I_kmeans = kmeans.index.search(teacher.cpu().numpy(), 1)\n","\n","            clust_labels = I_kmeans[:,0]\n","            # d_means.append(D_kmeans)\n","            pred_labels.append(clust_labels)\n","        pred_labels = np.stack(pred_labels, axis=0)\n","        cluster_labels = torch.from_numpy(pred_labels).float()\n","\n","        all_close_nei_in_back = None\n","        with torch.no_grad():\n","            for each_k_idx in range(self.num_kmeans):\n","                curr_close_nei = self.__get_close_nei_in_back(tmp.squeeze(-1), each_k_idx, cluster_labels, I_knn, I_knn.shape[1])\n","\n","                if all_close_nei_in_back is None:\n","                    all_close_nei_in_back = curr_close_nei\n","                else:\n","                    all_close_nei_in_back = all_close_nei_in_back | curr_close_nei\n","\n","        all_close_nei_in_back = all_close_nei_in_back.to(self.device)\n","\n","        globality = self.create_sparse_revised(I_knn, all_close_nei_in_back)\n","\n","        pos_ = locality + globality\n","\n","        return pos_.coalesce()._indices(), I_knn.shape[1]\n","\n","    def create_sparse(self, I):\n","\n","        similar = I.reshape(-1).tolist()\n","        index = np.repeat(range(I.shape[0]), I.shape[1])\n","\n","        assert len(similar) == len(index)\n","        indices = torch.tensor([index, similar],dtype=torch.int32).to(self.device)\n","        result = torch.sparse_coo_tensor(indices, torch.ones_like(I.reshape(-1)), [I.shape[0], I.shape[0]])\n","\n","        return result\n","\n","    def create_sparse_revised(self, I, all_close_nei_in_back):\n","        n_data, k = I.shape[0], I.shape[1]\n","\n","        index = []\n","        similar = []\n","        for j in range(I.shape[0]):\n","            for i in range(k):\n","                index.append(int(j))\n","                similar.append(I[j][i].item())\n","\n","        index = torch.masked_select(torch.LongTensor(index).to(self.device), all_close_nei_in_back.reshape(-1))\n","        similar = torch.masked_select(torch.LongTensor(similar).to(self.device), all_close_nei_in_back.reshape(-1))\n","\n","        assert len(similar) == len(index)\n","        indices = torch.tensor([index.cpu().numpy().tolist(), similar.cpu().numpy().tolist()]).to(self.device)\n","        result = torch.sparse_coo_tensor(indices, torch.ones(len(index)).to(self.device), [n_data, n_data])\n","\n","        return result"],"metadata":{"id":"KI8a96_bORbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ZINB.py\n","\n","def _nan2zero(x):\n","    print(\"ZINB NAN2ZERO\")\n","    return torch.where(torch.isnan(x), torch.zeros_like(x), x)\n","\n","def _nan2inf(x):\n","    print(\"ZINB NAN2INF\")\n","    return torch.where(torch.isnan(x), torch.zeros_like(x)+np.inf, x)\n","\n","def _nelem(x):\n","    print(\"ZINB NELEM\")\n","    nelem = torch.sum(torch.tensor(~torch.isnan(x),dtype = torch.float32))\n","    return torch.tensor(torch.where(torch.equal(nelem, 0.), 1., nelem), dtype = x.dtype)\n","\n","\n","def _reduce_mean(x):\n","    print(\"ZINB REDUCE MEAN\")\n","    nelem = _nelem(x)\n","    x = _nan2zero(x)\n","    return torch.divide(torch.sum(x), nelem)\n","\n","\n","def mse_loss(y_true, y_pred):\n","    print(\"ZINB MSE LOSS\")\n","    ret = torch.square(y_pred - y_true)\n","\n","    return _reduce_mean(ret)\n","\n","\n","def poisson_loss(y_true, y_pred):\n","    print(\"ZINB POISSON LOSS\")\n","    y_pred = torch.tensor(y_pred, dtype = torch.float32)\n","    y_true = torch.tensor(y_true, dtype = torch.float32)\n","    nelem = _nelem(y_true)\n","    y_true = _nan2zero(y_true)\n","    ret = y_pred - y_true*torch.log(y_pred+1e-10) + torch.lgamma(y_true+1.0)\n","\n","    return torch.divide(torch.sum(ret), nelem)\n","\n","\n","class NB(object):\n","    def __init__(self, theta=None, masking=False, scope='nbinom_loss/',\n","                 scale_factor=1.0, debug=False):\n","\n","        # for numerical stability\n","        self.eps = 1e-10\n","        self.scale_factor = scale_factor\n","        self.debug = debug\n","        self.scope = scope\n","        self.masking = masking\n","        self.theta = theta\n","\n","    def loss(self, y_true, y_pred, mean=True):\n","        print(\"ZINB NB LOSS\")\n","        scale_factor = self.scale_factor\n","        eps = self.eps\n","\n","        y_true = torch.tensor(y_true, dtype = torch.float32)\n","        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n","\n","        if self.masking:\n","            nelem = _nelem(y_true)\n","            y_true = _nan2zero(y_true)\n","\n","            # Clip theta\n","        theta = torch.minimum(self.theta,torch.tensor(1e6))\n","\n","        t1 = torch.lgamma(theta+eps) + torch.lgamma(y_true+1.0) - torch.lgamma(y_true+theta+eps)\n","        t2 = (theta+y_true) * torch.log(1.0 + (y_pred/(theta+eps))) + (y_true * (torch.log(theta+eps) - torch.log(y_pred+eps)))\n","\n","\n","        final = t1 + t2\n","\n","        final = _nan2inf(final)\n","\n","        if mean:\n","            if self.masking:\n","                final = torch.divide(torch.sum(final), nelem)\n","            else:\n","                final = torch.sum(final)\n","\n","\n","        return final\n","\n","class ZINB(NB):\n","    def __init__(self, pi, ridge_lambda=0.0, scope='zinb_loss/', **kwargs):\n","        super().__init__(scope=scope, **kwargs)\n","        self.pi = pi\n","        self.ridge_lambda = ridge_lambda\n","\n","    def loss(self, y_true, y_pred, mean=True):\n","        print(\"ZINB ZINB LOSS\")\n","        scale_factor = self.scale_factor\n","        eps = self.eps\n","\n","\n","            # reuse existing NB neg.log.lik.\n","            # mean is always False here, because everything is calculated\n","            # element-wise. we take the mean only in the end\n","        nb_case = super().loss(y_true, y_pred, mean=False) - torch.log(1.0-self.pi+eps)\n","\n","        y_true = torch.tensor(y_true, dtype = torch.float32)\n","        y_pred = torch.tensor(y_pred, dtype = torch.float32) * scale_factor\n","        theta = torch.minimum(self.theta,torch.tensor(1e6))\n","\n","        zero_nb = torch.pow(theta/(theta+y_pred+eps), theta)\n","        zero_case = -torch.log(self.pi + ((1.0-self.pi)*zero_nb)+eps)\n","        result = torch.where(torch.less(y_true, 1e-8), zero_case, nb_case)\n","        ridge = self.ridge_lambda*torch.square(self.pi)\n","        result += ridge\n","\n","        if mean:\n","            if self.masking:\n","                result = _reduce_mean(result)\n","            else:\n","                result = torch.sum(result)\n","\n","        result = _nan2inf(result)\n","\n","        return result"],"metadata":{"id":"77f15u4oKZO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#main.py from source code\n","\n","# To fix the random seed\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","\n","random.seed(0)\n","np.random.seed(0)\n","\n","def main():\n","    args, unknown = parse_args()\n","\n","    if args.embedder == 'AFGRL':\n","        embedder = AFGRL_ModelTrainer(args)\n","\n","    embedder.train()\n","    embedder.writer.close()\n","\n","def imputation(file_path):\n","    args, unknown = utils.parse_args()\n","    imputation_m = torch.load(file_path).detach().data.cpu().numpy()\n","    a = pd.DataFrame(imputation_m).T\n","    a.to_csv(\"/content/drive/MyDrive/CS598 DLH Project/results/AFGRL-imputed-\" + args.dataset + \".csv\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"BapPqIue-WzR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training\n","\n","\n","\n","*   Hyper Params\n","*   Computational requirements\n","*   Training Code\n","\n","\n","\n","\n"],"metadata":{"id":"zj0-yaAFHJPE"}},{"cell_type":"markdown","source":["## Evaluation\n","\n","\n","\n","*  Metrics\n","*  Metrics Descriptions\n","*  Evaluation Code\n","\n","\n","\n"],"metadata":{"id":"ccyuM29GHLna"}},{"cell_type":"markdown","source":["# Results\n","In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n","\n","Please test and report results for all experiments that you run with:\n","\n","*   specific numbers (accuracy, AUC, RMSE, etc)\n","*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"],"metadata":{"id":"gX6bCcZNuxmz"}},{"cell_type":"code","source":["# metrics to evaluate my model\n","\n","# plot figures to better show the results\n","\n","# it is better to save the numbers and figures for your presentation."],"metadata":{"id":"LjW9bCkouv8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#evaluate.py from source code\n","\n","# To fix the random seed\n","torch.manual_seed(0)\n","torch.cuda.manual_seed_all(0)\n","random.seed(0)\n","\n","# scGCL Model\n","# Revised freom Original version in AFGRL\n","# Ref:\n","# https://github.com/Namkyeong/AFGRL/tree/master/evaluate.py\n","\n","def evaluate_node(embeddings, dataset, name):\n","\n","    labels = dataset.y\n","    emb_dim, num_class = embeddings.shape[1], dataset.y.unique().shape[0]\n","\n","    dev_accs, test_accs = [], []\n","\n","    for i in range(20):\n","\n","        train_mask = dataset.train_mask[i]\n","        dev_mask = dataset.val_mask[i]\n","        if name == \"wikics\":\n","            test_mask = dataset.test_mask\n","        else:\n","            test_mask = dataset.test_mask[i]\n","\n","        classifier = LogisticRegression(emb_dim, num_class)\n","        optimizer = torch.optim.Adam(classifier.parameters(), lr=0.01, weight_decay=1e-5)\n","\n","        for _ in range(100):\n","            classifier.train()\n","            logits, loss = classifier(embeddings[train_mask], labels[train_mask])\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","        dev_logits, _ = classifier(embeddings[dev_mask], labels[dev_mask])\n","        test_logits, _ = classifier(embeddings[test_mask], labels[test_mask])\n","        dev_preds = torch.argmax(dev_logits, dim=1)\n","        test_preds = torch.argmax(test_logits, dim=1)\n","\n","        dev_acc = (torch.sum(dev_preds == labels[dev_mask]).float() /\n","                       labels[dev_mask].shape[0]).detach().cpu().numpy()\n","        test_acc = (torch.sum(test_preds == labels[test_mask]).float() /\n","                        labels[test_mask].shape[0]).detach().cpu().numpy()\n","\n","        dev_accs.append(dev_acc * 100)\n","        test_accs.append(test_acc * 100)\n","\n","    dev_accs = np.stack(dev_accs)\n","    test_accs = np.stack(test_accs)\n","\n","    dev_acc, dev_std = dev_accs.mean(), dev_accs.std()\n","    test_acc, test_std = test_accs.mean(), test_accs.std()\n","\n","    print('Evaluate node classification results')\n","    print('** Val: {:.4f} ({:.4f}) | Test: {:.4f} ({:.4f}) **'.format(dev_acc, dev_std, test_acc, test_std))\n","\n","\n","def evaluate_clustering(embeddings, dataset):\n","    x, edge_index, y = dataset[0]\n","    #print(y[1])\n","    embeddings = F.normalize(torch.from_numpy(embeddings), dim = -1, p = 2).detach().cpu().numpy()\n","    nb_class = len(y[1].unique())\n","    true_y = y[1].detach().cpu().numpy()\n","\n","    estimator = KMeans(n_clusters = nb_class)\n","\n","    NMI_list = []\n","    h_list = []\n","\n","    for i in range(10):\n","        estimator.fit(embeddings)\n","        y_pred = estimator.predict(embeddings)\n","\n","        h_score = metrics.homogeneity_score(true_y, y_pred)\n","        s1 = normalized_mutual_info_score(true_y, y_pred, average_method='arithmetic')\n","        NMI_list.append(s1)\n","        h_list.append(h_score)\n","\n","    s1 = sum(NMI_list) / len(NMI_list)\n","    h_score = sum(h_list) / len(h_list)\n","    print('Evaluate clustering results')\n","    print('** Clustering NMI: {:.4f} | homogeneity score: {:.4f} **'.format(s1, h_score))\n","\n","\n","def run_similarity_search(embeddings, dataset):\n","\n","    test_embs = embeddings.detach().cpu().numpy()\n","    test_lbls = dataset.y.detach().cpu().numpy()\n","    numRows = test_embs.shape[0]\n","\n","    cos_sim_array = pairwise.cosine_similarity(test_embs) - np.eye(numRows)\n","    st = []\n","    for N in [5, 10]:\n","        indices = np.argsort(cos_sim_array, axis=1)[:, -N:]\n","        tmp = np.tile(test_lbls, (numRows, 1))\n","        selected_label = tmp[np.repeat(np.arange(numRows), N), indices.ravel()].reshape(numRows, N)\n","        original_label = np.repeat(test_lbls, N).reshape(numRows,N)\n","        st.append(np.round(np.mean(np.sum((selected_label == original_label), 1) / N),4))\n","\n","    print('Evaluate similarity search results')\n","    print(\"** sim@5 : {} | sim@10 : {} **\".format(st[0], st[1]))"],"metadata":{"id":"6DI4V0UK-m31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    embedding = torch.load(\"/content/drive/MyDrive/CS598 DLH Project/model_checkpoints/embeddings_adam_clustering.pt\")\n","    dataset = torch.load(\"/content/drive/MyDrive/CS598 DLH Project/data/processed/data.pt\")\n","    evaluate_clustering(embedding,dataset)"],"metadata":{"id":"lvdSPgdXHVnx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model comparison"],"metadata":{"id":"8EAWAy_LwHlV"}},{"cell_type":"code","source":["# compare you model with others\n","# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"],"metadata":{"id":"uOdhGrbwwG71"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["From paper:\n","\n","To validate the performance of scGCL on scRNA-seq data imputation, we evaluate scGCL and other state-of-the-art baseline methods on multiple downstream analysis tasks such as clustering performance, recovering gene expression levels and pseudo-time analysis. scGCL demonstrates its effectiveness on scRNA-seq data imputation through extensive experiments. Furthermore, we verify the influence of different hyperparameters on the clustering results of scGCL, and the importance of each part on scGCL through ablation experiments.\n","\n","\n","Clustering analysis of scRNA-seq data is an essential analysis task, which affects the discrimination of cell types and subtypes. To evaluate the Clustering analysis of scGCL, we compare it with four state-of-the-art baseline methods. Among these four baseline methods, GraphSCI and scTAG are based on graph convolution, AutoClass is based on pre-clustering, and MAGIC is the traditional scRNA-seq imputation method. Two traditional Clustering analysis criteria, adjusted rand index (ARI) and normalized mutual information (NMI) are used to evaluate the Clustering performance of scGCL.\n","\n","The Clustering results of scGCL slightly outperforms on the average all baseline methods on most datasets. The average ARI and NMI of scGCL across all 14 datasets are 0.82 and 0.80 with the second best values of 0.80 and 0.75. At the same time, we find that state-of-the-art graph convolutional embedding methods have certain drawbacks through UMAP visualization. scTAG measures the similarity between cells and cluster centers, making cells with ambiguous clusters move closer to the wrong cell clusters.  GraphSCI applies association information between genes, which destroys the original properties of scRNA-seq data. Therefore, although GraphSCI outperforms the raw data in terms of clustering effect, its UMAP visualization is affected by the gene graph, and it is difficult to show the original biological characteristics. scGCL not only effectively distinguishes different types of cells but preserves the biological properties of scRNA-seq data (Fig. 3c). The number of cells detected by scGCL on different cell clusters is the closest to the number of cells of the real cell type, which indicates that scGCL maintains the characteristics of the cell type. In summary, scGCL can effectively eliminate the dropout events of scRNA-seq data and facilitate downstream analysis.\n","\n"],"metadata":{"id":"4GjEyLBIOXZw"}},{"cell_type":"markdown","source":["# Discussion\n","\n","In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n","  * Make assessment that the paper is reproducible or not.\n","  * Explain why it is not reproducible if your results are kind negative.\n","  * Describe “What was easy” and “What was difficult” during the reproduction.\n","  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n","  * What will you do in next phase.\n","\n"],"metadata":{"id":"qH75TNU71eRH"}},{"cell_type":"markdown","source":["# References\n","\n","1.  Zehao Xiong, Jiawei Luo, Wanwan Shi, Ying Liu, Zhongyuan Xu, Bo Wang, scGCL: an imputation method for scRNA-seq data based on graph contrastive learning, Bioinformatics, Volume 39, Issue 3, March 2023, btad098, https://doi.org/10.1093/bioinformatics/btad098\n","\n"],"metadata":{"id":"SHMI2chl9omn"}}]}